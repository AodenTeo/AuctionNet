{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:51.785873Z",
     "start_time": "2024-11-12T23:37:51.783696Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from scratch import ArtDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from model import create_vocab_csv, text_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b232412f332d13c",
   "metadata": {},
   "source": [
    "## Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5feafcb1d70f0103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:29:19.684963Z",
     "start_time": "2024-11-12T23:29:13.939927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# initialize the dataset and the dataloader\n",
    "dataset = ArtDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22762236c99383",
   "metadata": {},
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "Train: 95%\n",
    "Validation: 2.5%\n",
    "Test: 2.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c1cd3c142596b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:14.087736Z",
     "start_time": "2024-11-12T23:36:14.084854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(0.025 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d334a62a30ae9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:25.806319Z",
     "start_time": "2024-11-12T23:36:25.803823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316406 8326 8327\n"
     ]
    }
   ],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66344851359cb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:55.298758Z",
     "start_time": "2024-11-12T23:37:55.283177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da68da70bb17489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:05.395086Z",
     "start_time": "2024-11-12T23:38:05.392195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316406\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc63acd350c92bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:56.288337Z",
     "start_time": "2024-11-12T23:38:56.284944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for each split\n",
    "batch = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442959e-8212-45c4-bb00-7dba3d009d6a",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "We define the model to consist of an embedding layer (to process the text input), and 2 hidden fully-connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5afb8366-ecd0-4924-868d-75e51b20fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Set path to GloVe file and embedding dimension\n",
    "glove_path = \"glove.6B.50d.txt\"  # Update this to your GloVe file path\n",
    "embedding_dim = 50\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b2b35c-82cd-44f4-8244-2f9de0d7f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36373b2-41e0-4331-8670-481ea4efb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_neighbors(word, glove_embeddings, k=5):\n",
    "    if word not in glove_embeddings:\n",
    "        print(f\"{word} not found in GloVe embeddings.\")\n",
    "        return []\n",
    "\n",
    "    # Get the embedding vector of the target word\n",
    "    target_vector = glove_embeddings[word]\n",
    "    \n",
    "    # Calculate similarity between the target word and every other word in the embeddings\n",
    "    similarities = {}\n",
    "    for other_word, other_vector in glove_embeddings.items():\n",
    "        if other_word != word:\n",
    "            similarity = cosine_similarity(target_vector, other_vector)\n",
    "            similarities[other_word] = similarity\n",
    "\n",
    "    # Sort words by similarity and get the top k\n",
    "    nearest_neighbors = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d704e2f-7951-4ff7-a90c-dcf845fba0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('contemporary', np.float32(0.8596234)), ('works', np.float32(0.83878714)), ('arts', np.float32(0.8278873)), ('museum', np.float32(0.82194173)), ('collection', np.float32(0.8105687))]\n"
     ]
    }
   ],
   "source": [
    "print(find_k_nearest_neighbors(\"art\", glove_embeddings, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1192718-ead1-4e97-af2d-5de22b0a41f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.2013035)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_embeddings[\"arbus\"], glove_embeddings[\"rare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc95cd91-0076-472f-bb3b-03cc1c7e6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtPricePredictor(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        super(ArtPricePredictor, self).__init__()\n",
    "        \n",
    "        # Fully connected layers for combined features\n",
    "        self.fc1 = nn.Linear(features_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Final output for price prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1d0b49f7927db",
   "metadata": {},
   "source": [
    "## Instantiate the Art Price Predictor Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3937adf300be14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artist Vocab Size: 142937\n",
      " Title Vocab Size: 96962\n",
      " Number of numerical features: 11\n"
     ]
    }
   ],
   "source": [
    "# Extract the dimensions of the inputs \n",
    "vocab_size_artist = dataset.artist_vocab_len + 1\n",
    "vocab_size_title = dataset.title_vocab_len + 1\n",
    "numerical_features_dim = dataset.numerics.shape[1]\n",
    "print(f' Artist Vocab Size: {vocab_size_artist}\\n Title Vocab Size: {vocab_size_title}\\n Number of numerical features: {numerical_features_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8a1cf26-6522-42f5-8870-7f95bdec7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model hyperparameters \n",
    "learning_rate =0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c4499f930c43f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtPricePredictor(\n",
      "  (fc1): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "x, y = dataset.__getitem__(0)\n",
    "model = ArtPricePredictor(x.size()[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0dc9ae1-ccde-4a60-a56c-886b786110e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define percentage error loss\n",
    "# We use percentage error so that errors on large prices are treated more leniently than errors on small prices \n",
    "class MAPE(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(MAPE, self).__init__()\n",
    "        self.epsilon = epsilon  # Small constant to avoid division by zero\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Calculate MAPE\n",
    "        percentage_errors = torch.abs((targets - predictions) / (targets + self.epsilon))\n",
    "        mape = 100.0 * torch.mean(percentage_errors)\n",
    "        return mape\n",
    "\n",
    "criterion = MAPE() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.90, 0.999))  # ADAM Optimization (first beta controls momentum)\n",
    "\n",
    "# APPLY LEARNING RATE DECAY \n",
    "# Reduces the learning rate by a factor of gamma every step_size epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57a91a76-1ba1-440d-bd13-8220ec298ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtPricePredictor(\n",
      "  (fc1): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a38f452c-9536-4f0e-a58f-11c45a9825d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss  tensor(1481.3180, grad_fn=<MulBackward0>)\n",
      "Model Prediction:  tensor([[0.1241],\n",
      "        [0.1237]], grad_fn=<AddmmBackward0>)\n",
      "Actual Label:  tensor([[-0.0083],\n",
      "        [-0.0097]])\n"
     ]
    }
   ],
   "source": [
    "# make a prediction and extract the actual value\n",
    "prediction = model(dataset.x[0:2])\n",
    "label = dataset.price[0:2]\n",
    "\n",
    "# evaluate the loss\n",
    "print(\"Loss \", criterion(prediction, label))\n",
    "print(\"Model Prediction: \", prediction)\n",
    "print(\"Actual Label: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730f645-16b7-44e1-a43d-a654bfdd2b02",
   "metadata": {},
   "source": [
    "## Train the model using Mini-Batch Gradient Descent with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9a54ca6-f74a-4c3e-a5ad-02a3ac48d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_losses = []  # Store each batch's loss\n",
    "    step_num = 0\n",
    "\n",
    "    # loops over all mini-batches in the dataloader \n",
    "    for x, price in dataloader:\n",
    "        \n",
    "        # Forward pass\n",
    "        price_pred = model(x)\n",
    "        loss = criterion(price_pred, price)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        batch_losses.append(loss.item())\n",
    "        if (step_num % 100) == 0: \n",
    "            print(f'Loss at step {step_num}: {loss.item()}')\n",
    "\n",
    "        # increment the step count \n",
    "        step_num += 1\n",
    "    \n",
    "    return batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3329a7a-c67b-4f9d-bbdd-364b3a805689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one epoch...\n",
      "Loss at step 0: 6723.677734375\n",
      "Loss at step 100: 3818.71826171875\n",
      "Loss at step 200: 2377.814208984375\n",
      "Loss at step 300: 1246.66455078125\n",
      "Loss at step 400: 807.837646484375\n",
      "Loss at step 500: 919.9601440429688\n",
      "Loss at step 600: 347.30743408203125\n",
      "Loss at step 700: 266.62493896484375\n",
      "Loss at step 800: 25694.98046875\n",
      "Loss at step 900: 1278.3765869140625\n",
      "Loss at step 1000: 591.1775512695312\n",
      "Loss at step 1100: 657.5821533203125\n",
      "Loss at step 1200: 228.69334411621094\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One example training epopppch \n",
    "print(\"Training one epoch...\")\n",
    "losses = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses, label=\"Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29eb082c-cbea-4f43-8f7d-457c33fe331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the model's total percentage error on a given dataset \n",
    "def evaluate_percent(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_percentage_error = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, price in dataloader:\n",
    "            # Calculate the model's prediction on the given artist, title and numerics\n",
    "            outputs = model(x)\n",
    "            \n",
    "            # Calculate absolute percentage error\n",
    "            abs_percentage_error = torch.abs((outputs - price) / price) * 100\n",
    "            \n",
    "            # Accumulate the sum of percentage errors\n",
    "            total_percentage_error += abs_percentage_error.sum().item()\n",
    "            \n",
    "            # Count the number of samples\n",
    "            num_samples += price.size(0)\n",
    "    \n",
    "    # Calculate the mean percentage error\n",
    "    mean_percentage_error = total_percentage_error / num_samples\n",
    "    return mean_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b45ff947-455f-4e77-8cb2-4c72a9810124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 131, in <module>\n",
      "    dataset = ArtDataset()\n",
      "              ^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 94, in __init__\n",
      "    avg_cosine_similarity_with_museum(title, glove_embeddings) for title in title_col\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 56, in avg_cosine_similarity_with_museum\n",
      "    similarities.append(cosine_similarity(embedding, target_embedding))\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 37, in cosine_similarity\n",
      "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/numpy/linalg/_linalg.py\", line 2717, in norm\n",
      "    x = asarray(x)\n",
      "        ^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError of the model without training on the training set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate_percent(model,\u001b[38;5;250m \u001b[39mtrain_loader,\u001b[38;5;250m \u001b[39mcriterion)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError of the model without training on the validation set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate_percent(model,\u001b[38;5;250m \u001b[39mval_loader,\u001b[38;5;250m \u001b[39mcriterion)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m, in \u001b[0;36mevaluate_percent\u001b[0;34m(model, dataloader, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, price \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# Calculate the model's prediction on the given artist, title and numerics\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Calculate absolute percentage error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Error of the model without training on the training set: {evaluate_percent(model, train_loader, criterion)}%\")\n",
    "print(f\"Error of the model without training on the validation set: {evaluate_percent(model, val_loader, criterion)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42137855-9170-42de-a068-0e253cab3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average loss of the model \n",
    "def evaluate_loss(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, price in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(artist, title, numerics)\n",
    "            loss = criterion(outputs, price)\n",
    "            \n",
    "            # Accumulate the loss\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4bb2d50c-6c9e-447e-a90e-bc4f17b7cc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Price: $2830.89\n",
      "Mean Price: $31777.83\n",
      "Normalized to MEDIAN\n",
      "Dataset loaded successfully!\n",
      "Median Price: $2830.89\n",
      "Mean Price: $31777.83\n",
      "Normalized to MEDIAN\n",
      "Dataset loaded successfully!\n",
      "Mean Squared Error of the untrained model on Validation Set: 1807.3191605196655\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Squared Error of the untrained model on Validation Set: {evaluate_loss(model, val_loader, criterion)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09a37ef9-218a-4c1e-a67d-e74114349f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 272.2344055175781\n",
      "Loss at step 100: 443.10614013671875\n",
      "Loss at step 200: 379.9480895996094\n",
      "Loss at step 300: 204.9539794921875\n",
      "Loss at step 400: 312.6573181152344\n",
      "Loss at step 500: 805.221923828125\n",
      "Loss at step 600: 180.98406982421875\n",
      "Loss at step 700: 229.6699981689453\n",
      "Loss at step 800: 168.03521728515625\n",
      "Loss at step 900: 277.91998291015625\n",
      "Loss at step 1000: 172.78599548339844\n",
      "Loss at step 1100: 131.76138305664062\n",
      "Loss at step 1200: 133.61172485351562\n",
      "Epoch 1/1000, Avg Train Loss: 4958.084858570284\n",
      "Loss at step 0: 365.14691162109375\n",
      "Loss at step 100: 190.15350341796875\n",
      "Loss at step 200: 1188.224609375\n",
      "Loss at step 300: 196.10829162597656\n",
      "Loss at step 400: 140.8017578125\n",
      "Loss at step 500: 253.430908203125\n",
      "Loss at step 600: 159.80690002441406\n",
      "Loss at step 700: 202.0606231689453\n",
      "Loss at step 800: 219.68409729003906\n",
      "Loss at step 900: 207.0189208984375\n",
      "Loss at step 1000: 139.2301025390625\n",
      "Loss at step 1100: 253.45162963867188\n",
      "Loss at step 1200: 1253.46875\n",
      "Epoch 2/1000, Avg Train Loss: 3865.3486743791204\n",
      "Loss at step 0: 146.9954833984375\n",
      "Loss at step 100: 147.54241943359375\n",
      "Loss at step 200: 161.0314483642578\n",
      "Loss at step 300: 363666.03125\n",
      "Loss at step 400: 142.27621459960938\n",
      "Loss at step 500: 134.58387756347656\n",
      "Loss at step 600: 258.7806091308594\n",
      "Loss at step 700: 29621.9453125\n",
      "Loss at step 800: 259.5421447753906\n",
      "Loss at step 900: 288.7552185058594\n",
      "Loss at step 1000: 244.9909210205078\n",
      "Loss at step 1100: 158.65826416015625\n",
      "Loss at step 1200: 192.29931640625\n",
      "Epoch 3/1000, Avg Train Loss: 4007.620472281496\n",
      "Loss at step 0: 138.4916534423828\n",
      "Loss at step 100: 233.62554931640625\n",
      "Loss at step 200: 132.02256774902344\n",
      "Loss at step 300: 282.87542724609375\n",
      "Loss at step 400: 286.0627136230469\n",
      "Loss at step 500: 146.2257843017578\n",
      "Loss at step 600: 134.4356231689453\n",
      "Loss at step 700: 127.21663665771484\n",
      "Loss at step 800: 114.44700622558594\n",
      "Loss at step 900: 230.17227172851562\n",
      "Loss at step 1000: 155.12753295898438\n",
      "Loss at step 1100: 187.34254455566406\n",
      "Loss at step 1200: 174.69664001464844\n",
      "Epoch 4/1000, Avg Train Loss: 3263.739499027289\n",
      "Loss at step 0: 117.71422576904297\n",
      "Loss at step 100: 404.4962463378906\n",
      "Loss at step 200: 304.81658935546875\n",
      "Loss at step 300: 189.66732788085938\n",
      "Loss at step 400: 119.82542419433594\n",
      "Loss at step 500: 484.3857727050781\n",
      "Loss at step 600: 138.13922119140625\n",
      "Loss at step 700: 243.70828247070312\n",
      "Loss at step 800: 330.7916259765625\n",
      "Loss at step 900: 161.72950744628906\n",
      "Loss at step 1000: 178.04835510253906\n",
      "Loss at step 1100: 6056.19873046875\n",
      "Loss at step 1200: 120.01433563232422\n",
      "Epoch 5/1000, Avg Train Loss: 3388.094646836562\n",
      "Loss at step 0: 202.8308868408203\n",
      "Loss at step 100: 205.78518676757812\n",
      "Loss at step 200: 140.38421630859375\n",
      "Loss at step 300: 110.48029327392578\n",
      "Loss at step 400: 180.70364379882812\n",
      "Loss at step 500: 182.50833129882812\n",
      "Loss at step 600: 221.5888214111328\n",
      "Loss at step 700: 139.4155731201172\n",
      "Loss at step 800: 143.83172607421875\n",
      "Loss at step 900: 214.59939575195312\n",
      "Loss at step 1000: 106.89875793457031\n",
      "Loss at step 1100: 120.36308288574219\n",
      "Loss at step 1200: 112.59330749511719\n",
      "Epoch 6/1000, Avg Train Loss: 2523.8787656987756\n",
      "Loss at step 0: 144.35281372070312\n",
      "Loss at step 100: 248.09896850585938\n",
      "Loss at step 200: 2665.42236328125\n",
      "Loss at step 300: 106.40924072265625\n",
      "Loss at step 400: 239.52862548828125\n",
      "Loss at step 500: 164.60476684570312\n",
      "Loss at step 600: 281.71270751953125\n",
      "Loss at step 700: 114.4600830078125\n",
      "Loss at step 800: 410.5981750488281\n",
      "Loss at step 900: 300.1236572265625\n",
      "Loss at step 1000: 161.07559204101562\n",
      "Loss at step 1100: 251.10617065429688\n",
      "Loss at step 1200: 107.86161804199219\n",
      "Epoch 7/1000, Avg Train Loss: 3313.65373871936\n",
      "Loss at step 0: 202.07870483398438\n",
      "Loss at step 100: 152.00514221191406\n",
      "Loss at step 200: 120.7065200805664\n",
      "Loss at step 300: 139.98513793945312\n",
      "Loss at step 400: 280.4334716796875\n",
      "Loss at step 500: 114.58699798583984\n",
      "Loss at step 600: 124.75601196289062\n",
      "Loss at step 700: 1290.3173828125\n",
      "Loss at step 800: 123.69351196289062\n",
      "Loss at step 900: 390.24462890625\n",
      "Loss at step 1000: 131.6887664794922\n",
      "Loss at step 1100: 110.94795227050781\n",
      "Loss at step 1200: 125.47725677490234\n",
      "Epoch 8/1000, Avg Train Loss: 3289.7644766464973\n",
      "Loss at step 0: 139426.75\n",
      "Loss at step 100: 107.06939697265625\n",
      "Loss at step 200: 105.85314178466797\n",
      "Loss at step 300: 109.5346450805664\n",
      "Loss at step 400: 354.2672119140625\n",
      "Loss at step 500: 140.2299346923828\n",
      "Loss at step 600: 227.08863830566406\n",
      "Loss at step 700: 126.55308532714844\n",
      "Loss at step 800: 119.25196838378906\n",
      "Loss at step 900: 121.71435546875\n",
      "Loss at step 1000: 143.50804138183594\n",
      "Loss at step 1100: 342.04888916015625\n",
      "Loss at step 1200: 228.09646606445312\n",
      "Epoch 9/1000, Avg Train Loss: 2902.5408069771083\n",
      "Loss at step 0: 325.8515319824219\n",
      "Loss at step 100: 234.11602783203125\n",
      "Loss at step 200: 148.87060546875\n",
      "Loss at step 300: 522.583740234375\n",
      "Loss at step 400: 123.03697967529297\n",
      "Loss at step 500: 169.8323974609375\n",
      "Loss at step 600: 151.04835510253906\n",
      "Loss at step 700: 98.96554565429688\n",
      "Loss at step 800: 146.38853454589844\n",
      "Loss at step 900: 133.1420440673828\n",
      "Loss at step 1000: 98.12134552001953\n",
      "Loss at step 1100: 126.88982391357422\n",
      "Loss at step 1200: 109.91838073730469\n",
      "Epoch 10/1000, Avg Train Loss: 2752.0155478974375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 42, in <module>\n",
      "    glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 30, in load_glove_embeddings\n",
      "    values = line.split()\n",
      "             ^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m average \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, criterion, optimizer)\n\u001b[1;32m      8\u001b[0m     loss_plot\u001b[38;5;241m.\u001b[39mextend(batch_losses)  \u001b[38;5;66;03m# Store losses for each batch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Print average loss for the epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m step_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# loops over all mini-batches in the dataloader \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, price \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     price_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(price_pred, price)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Training Loop \n",
    "num_epochs = 1000\n",
    "loss_plot = []  # keep a vector to plot the loss as we go \n",
    "average = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    loss_plot.extend(batch_losses)  # Store losses for each batch\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg Train Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Update the learning rate \n",
    "    scheduler.step()\n",
    "\n",
    "    # if the average epoch loss goes below human error significantly, stop training \n",
    "    if avg_epoch_loss < 30: \n",
    "        break\n",
    "\n",
    "    if (epoch % 100) == 0: \n",
    "        torch.save(model.state_dict(), 'model_weights.txt')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(loss_plot, label=\"Batch Loss\")\n",
    "plt.plot(average, label=\"Average Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4213067-3809-46fa-91c3-442678fdcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(average, label=\"Average Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31cbe535-ac17-4ad2-b76f-6e0b7fec80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: Duane Hanson (1925-1996)\n",
      "Title: Man on a bench\n",
      "Real Price: $174755.39\n",
      "Model Prediction: $1827.33\n",
      "17231.5859375\n"
     ]
    }
   ],
   "source": [
    "def prediction_to_real_price(price_tensor): \n",
    "    return (price_tensor * dataset.price_std) + dataset.price_median\n",
    "\n",
    "# Sample the Training Set \n",
    "index = 200000\n",
    "x_test, price_test = dataset.__getitem__(index)\n",
    "artist_str, title_str = dataset.__getstring__(index)\n",
    "print(f\"Artist: {artist_str}\")\n",
    "print(f\"Title: {title_str}\")\n",
    "print(f\"Real Price: ${prediction_to_real_price(price_test).item():.2f}\")\n",
    "prediction = model(x_test.view(1, -1))\n",
    "print(f\"Model Prediction: ${prediction_to_real_price(prediction).item():.2f}\")\n",
    "with torch.no_grad(): \n",
    "    print(criterion(price_test, prediction).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cc8aeab-5ef6-4f0f-9fc8-67710cd44e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: A pink lustre 'Freemason's' jug\n",
      "Title: nan\n",
      "Real Price: $219.52\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'artist_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Price: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_to_real_price(price_test)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model(artist_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), title_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), numerics_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Prediction: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_to_real_price(prediction)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'artist_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Artist: {artist_str}\")\n",
    "print(f\"Title: {title_str}\")\n",
    "print(f\"Real Price: ${prediction_to_real_price(price_test).item():.2f}\")\n",
    "prediction = model(artist_test.view(1, -1), title_test.view(1, -1), numerics_test.view(1, -1))\n",
    "print(f\"Model Prediction: ${prediction_to_real_price(prediction).item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "35729b1f-12eb-4630-8629-bed3cb9e1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54.4121)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    print(criterion(price_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8ab652e-d671-456d-9a3e-364fb098e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Price Normalized:  tensor([-0.0092])\n",
      "Model Prediction Normalized:  tensor([[-2.1695e-08]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Price Normalized: \", price_test)\n",
    "print(\"Model Prediction Normalized: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5423a23-3678-4b67-9e99-aee0cb76d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight:\n",
      "tensor([[-0.2612, -0.3165, -0.0925, -0.1503, -0.1302,  0.0803,  0.1983, -0.0735,\n",
      "          0.0091,  0.3834, -0.1505,  0.0686,  0.0193],\n",
      "        [ 0.2101,  0.3574,  0.4466, -0.3402,  0.1273,  0.0058, -0.3784,  0.0053,\n",
      "         -0.5275,  0.3315, -0.1071, -0.1518, -0.0346],\n",
      "        [-0.0795, -0.2030,  0.3063, -0.0636, -0.1748,  0.0216,  0.0373, -0.1911,\n",
      "         -0.2639,  0.0804,  0.1417, -0.0013, -0.0812],\n",
      "        [ 0.1090,  0.1107, -0.2536,  0.3599, -0.1825,  0.2160,  0.1724, -0.4350,\n",
      "          0.2164, -0.4358,  0.1931,  0.3384,  0.0540],\n",
      "        [-0.3601, -0.1574, -0.1143, -0.0907, -0.2680,  0.1386,  0.2232, -0.1045,\n",
      "          0.1425, -0.0342,  0.0586, -0.0394, -0.0060],\n",
      "        [-0.1171, -0.5389,  0.1044,  0.1699, -0.1923, -0.3639, -0.0117, -0.0824,\n",
      "         -0.1775, -0.2600, -0.2471,  0.2647, -0.0533],\n",
      "        [ 0.1294,  0.0744, -0.0263,  0.4745, -0.2291,  0.1951,  0.3845,  0.1570,\n",
      "          0.2338, -0.2118,  0.5391,  0.0463,  0.4172],\n",
      "        [-0.2072, -0.0140, -0.0899,  0.0605, -0.2144, -0.1092,  0.0264, -0.0385,\n",
      "         -0.0209, -0.0822,  0.0858, -0.2819, -0.0158]])\n",
      "\n",
      "fc1.bias:\n",
      "tensor([-0.1312,  0.3242, -0.0517, -0.1531,  0.1452, -0.1008,  0.2891, -0.0679])\n",
      "\n",
      "fc2.weight:\n",
      "tensor([[ 0.1471, -0.3638, -0.0070, -0.0702, -0.3050, -0.1307, -0.2299, -0.0376],\n",
      "        [-0.0179, -0.2813, -0.0876,  0.0059, -0.2308,  0.0426, -0.0613, -0.3399],\n",
      "        [-0.5157, -0.1543,  0.0142, -0.4407,  0.4583,  0.2321, -0.2439, -0.0939],\n",
      "        [ 0.4911, -0.4961,  0.2836, -0.0364, -0.2880, -0.0405, -0.0236, -0.1612]])\n",
      "\n",
      "fc2.bias:\n",
      "tensor([ 0.2232, -0.0777, -0.2261, -0.2100])\n",
      "\n",
      "fc3.weight:\n",
      "tensor([[-0.2992, -0.0023, -0.0525,  0.0641]])\n",
      "\n",
      "fc3.bias:\n",
      "tensor([-2.1695e-08])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all weights and biases\n",
    "for name, param in model.state_dict().items():\n",
    "    print(f\"{name}:\\n{param}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dae16-82c1-4b11-92eb-23a6dbdaa020",
   "metadata": {},
   "source": [
    "Compute human error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2039366-5817-4d25-aafd-fb8294cc6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (MAPE): 39.08%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'clean_art.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the human predicted price (midpoint)\n",
    "data['Human Predicted Price'] = (data['Real LB Estimate USD'] + data['Real UB Estimate USD']) / 2\n",
    "\n",
    "# Calculate the absolute percentage error for each row\n",
    "data['Absolute Percentage Error'] = abs(data['Human Predicted Price'] - data['Real Price USD']) / data['Real Price USD']\n",
    "\n",
    "# Calculate the mean of the absolute percentage error (MAPE)\n",
    "mape = data['Absolute Percentage Error'].mean() * 100\n",
    "\n",
    "# Output the result\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93666450-0d6e-4efa-b20a-9172a66a829d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
