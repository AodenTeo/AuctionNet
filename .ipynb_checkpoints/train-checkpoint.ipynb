{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:51.785873Z",
     "start_time": "2024-11-12T23:37:51.783696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataload import ArtDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from stringproc import create_vocab_csv, text_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b232412f332d13c",
   "metadata": {},
   "source": [
    "## Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5feafcb1d70f0103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:29:19.684963Z",
     "start_time": "2024-11-12T23:29:13.939927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist Indexed Tokens: tensor([[  101,  3434,  3158,  ...,     0,     0,     0],\n",
      "        [  101,  2394,  2082,  ...,     0,     0,     0],\n",
      "        [  101,  9586,  1012,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  3744,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  8149,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  2726, 12154,  ...,     0,     0,     0]])\n",
      " Artist Segment IDs: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "Title Indexed Tokens: tensor([[  101,  1037,  3803,  ...,     0,     0,     0],\n",
      "        [  101, 14783,  3317,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  3193,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1996,  3752,  ...,     0,     0,     0],\n",
      "        [  101,  7095,  3417,  ...,     0,     0,     0],\n",
      "        [  101,  6533,  1997,  ...,  1037,  2911,   102]])\n",
      " Title Segment IDs: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# initialize the dataset and the dataloader\n",
    "dataset = ArtDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22762236c99383",
   "metadata": {},
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "Train: 95%\n",
    "Validation: 2.5%\n",
    "Test: 2.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c1cd3c142596b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:14.087736Z",
     "start_time": "2024-11-12T23:36:14.084854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(0.025 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d334a62a30ae9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:25.806319Z",
     "start_time": "2024-11-12T23:36:25.803823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316406 8326 8327\n"
     ]
    }
   ],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66344851359cb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:55.298758Z",
     "start_time": "2024-11-12T23:37:55.283177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da68da70bb17489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:05.395086Z",
     "start_time": "2024-11-12T23:38:05.392195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316406\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc63acd350c92bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:56.288337Z",
     "start_time": "2024-11-12T23:38:56.284944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Higher batch size seems to make the model train faster, but converge happens slower \n",
    "batch = 256  \n",
    "\n",
    "# Create dataloaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442959e-8212-45c4-bb00-7dba3d009d6a",
   "metadata": {},
   "source": [
    "## Load The GloVe Embeddings\n",
    "Below we load the GloVe embeddings, and define functions to search the vicinity of given target words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5afb8366-ecd0-4924-868d-75e51b20fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Set path to GloVe file and embedding dimension\n",
    "glove_path = \"Glove/glove.6B.50d.txt\"  # Update this to your GloVe file path\n",
    "embedding_dim = 50\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "74b2b35c-82cd-44f4-8244-2f9de0d7f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c36373b2-41e0-4331-8670-481ea4efb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_neighbors(word, glove_embeddings, k=5):\n",
    "    if word not in glove_embeddings:\n",
    "        print(f\"{word} not found in GloVe embeddings.\")\n",
    "        return []\n",
    "\n",
    "    # Get the embedding vector of the target word\n",
    "    target_vector = glove_embeddings[word]\n",
    "    \n",
    "    # Calculate similarity between the target word and every other word in the embeddings\n",
    "    similarities = {}\n",
    "    for other_word, other_vector in glove_embeddings.items():\n",
    "        if other_word != word:\n",
    "            similarity = cosine_similarity(target_vector, other_vector)\n",
    "            similarities[other_word] = similarity\n",
    "\n",
    "    # Sort words by similarity and get the top k\n",
    "    nearest_neighbors = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3d704e2f-7951-4ff7-a90c-dcf845fba0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('contemporary', np.float32(0.8596234)), ('works', np.float32(0.83878714)), ('arts', np.float32(0.8278873)), ('museum', np.float32(0.82194173)), ('collection', np.float32(0.8105687))]\n"
     ]
    }
   ],
   "source": [
    "print(find_k_nearest_neighbors(\"art\", glove_embeddings, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d1192718-ead1-4e97-af2d-5de22b0a41f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.2013035)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_embeddings[\"arbus\"], glove_embeddings[\"rare\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd7115-5619-4a2d-9500-ca2066af99d8",
   "metadata": {},
   "source": [
    "## Import BERT \n",
    "BERT allows us to extract context dependent embeddings of each word in a given sentence. For instance, in the phrases \"river bank\" and \"bank of America\" the word bank would end up having a drastically different embedding. \n",
    "\n",
    "#### INCLUDE CITATION\n",
    "Chris McCormick and Nick Ryan. (2019, May 14). *BERT Word Embeddings Tutorial*. Retrieved from http://www.mccormickml.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c6c9ffc-3faf-418a-aa1b-81f9ea00ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pretrained BERT Model\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "\n",
    "# if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27352cc8-4c91-469e-bd6c-f2f35c8d98ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "base_model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states\n",
    "                                  num_labels=13                            \n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f65e47b4-0d83-4d13-abf6-a956479628af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ArtDataset' object has no attribute 'price_classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertForSequenceClassification\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mprice_classifier)),  \u001b[38;5;66;03m# Number of classes\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ArtDataset' object has no attribute 'price_classifier'"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(set(dataset.price_classifier)),  # Number of classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe6964dc-686d-4608-9be1-df1dc8f3dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1037,  3940,  1997,  3756,  3384,  9226,  2275, 26450, 13767,\n",
      "          2015,  1010,  1037, 28653,  2275,  3614,  1006,  2962, 13366, 20132,\n",
      "          1007,  1010,  1037,  2962,  2275,  3614,  1010,  1037,  3940,   102]])\n",
      "torch.Size([1, 30])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([1, 30])\n"
     ]
    }
   ],
   "source": [
    "# function: prediction_to_real_price \n",
    "# ------------------------------------------------------\n",
    "# converts the normalized price tensor back to the original tensor \n",
    "# \n",
    "# @param price_tensor Tensor containing the price we would like to convert \n",
    "# to the original price \n",
    "# \n",
    "# @returns Original Price\n",
    "def prediction_to_real_price(price_tensor): \n",
    "    return (price_tensor * dataset.price_std) + dataset.price_median\n",
    "\n",
    "# Sample the Training Set \n",
    "index = 1004\n",
    "artist_tokens_tensor, artist_segids, price_tensor = dataset.__getitem__(index)\n",
    "artist_tokens_tensor = artist_tokens_tensor.view(1, -1)\n",
    "artist_segids = artist_segids.view(1, -1)\n",
    "print(artist_tokens_tensor)\n",
    "print(artist_tokens_tensor.shape)\n",
    "print(artist_segids)\n",
    "print(artist_segids.shape)\n",
    "#artist_str, title_str = dataset.__getstring__(index)\n",
    "#print(f\"Artist: {artist_str}\")\n",
    "#print(f\"Title: {title_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3fe2aec-da99-4130-96f2-3b9a135533e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Takes in a string and outputs a list of the indices of the \n",
    "# words inside, and the tokens \n",
    "def encode_text(text): \n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    return tokenized_text, indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "009ad2d5-2d51-47b3-9b9a-f7cd14399fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "artist        3,063\n",
      "name          2,171\n",
      "from          2,013\n",
      "christie     13,144\n",
      "auction      10,470\n",
      "lot           2,843\n",
      ":             1,024\n",
      "duane        27,319\n",
      "hanson       17,179\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "tokenized_text, indexed_tokens = encode_text(\"Artist Name from Christie Auction Lot: Duane Hanson\")\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e8643fcb-7575-4e2e-953b-9fb9fe637a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b930cf99-849f-4c66-8b5e-43f750eba099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor_test = torch.tensor([indexed_tokens])\n",
    "segments_tensors_test = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1eea227c-b286-4323-bb49-9cbcaa5b99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers.\n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = base_model(tokens_tensor_test, segments_tensors_test)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on\n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "    # becase we set `output_hidden_states = True`, the third item will be the\n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49722aec-de31-4bdf-b022-61826557dc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 14, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f81ec110-e078-4637-a144-2fc97daf965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 14, 768])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "352c7682-b1b6-4d5f-b359-d19757536c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 13 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last\n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e397e6e-00bf-4e97-b3e9-a827f0917f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 duane\n",
      "2 hanson\n",
      "3 and\n",
      "4 henry\n",
      "5 moore\n",
      "6 went\n",
      "7 to\n",
      "8 see\n",
      "9 a\n",
      "10 performance\n",
      "11 of\n",
      "12 hanson\n",
      "13 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ca651711-ded9-45fe-9b1e-70fe0735610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duane HANSON    tensor([-0.6000,  1.6732,  1.1900,  0.2904, -1.0233])\n",
      "HANSON (band)   tensor([ 0.5569, -0.2083, -0.1436,  1.1234,  1.2210])\n",
      "MOORE tensor([-0.7156,  1.3359,  2.1189,  1.0708,  0.1506])\n"
     ]
    }
   ],
   "source": [
    "print(\"duane HANSON   \", str(token_vecs_sum[2][:5]))\n",
    "print(\"HANSON (band)  \", str(token_vecs_sum[12][:5]))\n",
    "print(\"MOORE\", str(token_vecs_sum[5][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ab14464-5976-4af5-b7c0-6d6277b6ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  two sculptors:  0.74\n",
      "Vector similarity for *different* Hansons:  0.50\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word Hanson\n",
    "# in \"Duane Hanson\" vs the band \"Hanson\" (different meanings).\n",
    "diff_hanson = 1 - cosine(token_vecs_sum[2], token_vecs_sum[10])\n",
    "\n",
    "# Calculate the cosine similarity between the word Hanson\n",
    "# in \"Duane Hanson\" vs \"Henry Moore\" (another sculptor).\n",
    "sculptors = 1 - cosine(token_vecs_sum[2], token_vecs_sum[5])\n",
    "\n",
    "print('Vector similarity for  two sculptors:  %.2f' % sculptors)\n",
    "print('Vector similarity for *different* Hansons:  %.2f' % diff_hanson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940fd4c-b17a-4cee-a53b-981885d00f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ceeb72-cbba-4f2d-b4cd-59bbcb08809d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc8bf433-893c-4d1d-ba2a-e987166a486e",
   "metadata": {},
   "source": [
    "## Define the Models\n",
    "We define the model to have 6 fully-connected layers, with 16, 16, 16, 16, 16, 8, 1 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cc95cd91-0076-472f-bb3b-03cc1c7e6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtPricePredictor(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        super(ArtPricePredictor, self).__init__()\n",
    "        \n",
    "        # Fully connected layers for combined features\n",
    "        self.fc1 = nn.Linear(features_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Final output for price prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17180bea-0b82-468d-89b1-46e993873e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtPriceClassifier(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        super(ArtPricePredictor, self).__init__()\n",
    "        \n",
    "        # Fully connected layers for combined features\n",
    "        self.fc1 = nn.Linear(features_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Final output for price prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1d0b49f7927db",
   "metadata": {},
   "source": [
    "## Instantiate the Art Price Predictor Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3937adf300be14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artist Vocab Size: 142937\n",
      " Title Vocab Size: 96962\n",
      " Number of numerical features: 11\n"
     ]
    }
   ],
   "source": [
    "# Extract the dimensions of the inputs \n",
    "vocab_size_artist = dataset.artist_vocab_len + 1\n",
    "vocab_size_title = dataset.title_vocab_len + 1\n",
    "numerical_features_dim = dataset.numerics.shape[1]\n",
    "print(f' Artist Vocab Size: {vocab_size_artist}\\n Title Vocab Size: {vocab_size_title}\\n Number of numerical features: {numerical_features_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8a1cf26-6522-42f5-8870-7f95bdec7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model hyperparameters \n",
    "learning_rate =0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c4499f930c43f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtPricePredictor(\n",
      "  (fc1): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "x, y = dataset.__getitem__(0)\n",
    "model = ArtPricePredictor(x.size()[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dee4b9-c52c-450a-807f-01d581255626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class BERTSentenceRegressor(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"bert-base-uncased\"):\n",
    "        super(BERTSentenceRegressor, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name, output_hidden_states=True)\n",
    "        \n",
    "        # Regression head\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.regressor = nn.Linear(hidden_size, 1)  # Predict a single numerical value\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states  # Tuple of all hidden states\n",
    "        \n",
    "        # Average over the last four hidden layers\n",
    "        last_four_layers = hidden_states[-4:]  # Last four layers\n",
    "        stacked_layers = torch.stack(last_four_layers, dim=0)  # Shape: (4, batch_size, seq_len, hidden_size)\n",
    "        avg_last_four = torch.mean(stacked_layers, dim=0)  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Average over all tokens to get the sentence embedding\n",
    "        sentence_embeddings = torch.mean(avg_last_four, dim=1)  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Predict numerical value\n",
    "        prediction = self.regressor(sentence_embeddings)\n",
    "        return prediction.squeeze(-1)  # Shape: (batch_size,)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BERTSentenceRegressor(pretrained_model_name=\"bert-base-uncased\")\n",
    "\n",
    "# Example input text\n",
    "texts = [\"This is an example sentence.\", \"Another sentence for testing.\"]\n",
    "\n",
    "# Tokenize input\n",
    "tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = tokenized[\"input_ids\"]\n",
    "attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "# Forward pass\n",
    "predictions = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4343f25-4e3a-4ed3-8f6f-75a17f4ffdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"bert-base-uncased\", num_classes=2):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # Classification head\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass tokenized input through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "        \n",
    "        # Pass [CLS] token representation to the classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BERTClassifier(pretrained_model_name=\"bert-base-uncased\", num_classes=2)\n",
    "\n",
    "# Example input text\n",
    "texts = [\"Hello, how are you?\", \"I love PyTorch!\"]\n",
    "\n",
    "# Tokenize input\n",
    "tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = tokenized[\"input_ids\"]\n",
    "attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d0dc9ae1-ccde-4a60-a56c-886b786110e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define percentage error loss\n",
    "# We use percentage error so that errors on large prices are treated more leniently than errors on small prices \n",
    "class MAPE(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(MAPE, self).__init__()\n",
    "        self.epsilon = epsilon  # Small constant to avoid division by zero\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Calculate MAPE\n",
    "        percentage_errors = torch.abs((targets - predictions) / (targets + self.epsilon))\n",
    "        mape = 100.0 * torch.mean(percentage_errors)\n",
    "        return mape\n",
    "\n",
    "criterion = MAPE() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.90, 0.999))  # ADAM Optimization (first beta controls momentum)\n",
    "\n",
    "# APPLY LEARNING RATE DECAY \n",
    "# Reduces the learning rate by a factor of gamma every step_size epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "57a91a76-1ba1-440d-bd13-8220ec298ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtPricePredictor(\n",
      "  (fc1): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a38f452c-9536-4f0e-a58f-11c45a9825d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss  tensor(99.9681, grad_fn=<MulBackward0>)\n",
      "Model Prediction:  tensor([[ 1.7762e-07],\n",
      "        [-6.4557e-06]], grad_fn=<AddmmBackward0>)\n",
      "Actual Label:  tensor([[-0.0083],\n",
      "        [-0.0097]])\n"
     ]
    }
   ],
   "source": [
    "# make a prediction and extract the actual value\n",
    "prediction = model(dataset.x[0:2])\n",
    "label = dataset.price[0:2]\n",
    "\n",
    "# evaluate the loss\n",
    "print(\"Loss \", criterion(prediction, label))\n",
    "print(\"Model Prediction: \", prediction)\n",
    "print(\"Actual Label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c28a69e5-1a66-458e-881b-39c4932c4c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state dictionary\n",
    "state_dict = torch.load('model_weights.txt')\n",
    "\n",
    "# Load the weights into your model\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730f645-16b7-44e1-a43d-a654bfdd2b02",
   "metadata": {},
   "source": [
    "## Train the model using Mini-Batch Gradient Descent with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f9a54ca6-f74a-4c3e-a5ad-02a3ac48d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_losses = []  # Store each batch's loss\n",
    "    step_num = 0\n",
    "\n",
    "    # loops over all mini-batches in the dataloader \n",
    "    for x, price in dataloader:\n",
    "        \n",
    "        # Forward pass\n",
    "        price_pred = model(x)\n",
    "        loss = criterion(price_pred, price)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        batch_losses.append(loss.item())\n",
    "        if (step_num % 100) == 0: \n",
    "            print(f'Loss at step {step_num}: {loss.item()}')\n",
    "\n",
    "        # increment the step count \n",
    "        step_num += 1\n",
    "    \n",
    "    return batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e3329a7a-c67b-4f9d-bbdd-364b3a805689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one epoch...\n",
      "Loss at step 0: 100.01216888427734\n",
      "Loss at step 100: 104.21656036376953\n",
      "Loss at step 200: 98.885498046875\n",
      "Loss at step 300: 97.80712127685547\n",
      "Loss at step 400: 99.3489761352539\n",
      "Loss at step 500: 98.05636596679688\n",
      "Loss at step 600: 99.31829833984375\n",
      "Loss at step 700: 99.65673828125\n",
      "Loss at step 800: 99.76976013183594\n",
      "Loss at step 900: 98.59403228759766\n",
      "Loss at step 1000: 98.342041015625\n",
      "Loss at step 1100: 98.83724212646484\n",
      "Loss at step 1200: 98.95890045166016\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One example training epopppch \n",
    "print(\"Training one epoch...\")\n",
    "losses = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses, label=\"Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "29eb082c-cbea-4f43-8f7d-457c33fe331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the model's total percentage error on a given dataset \n",
    "def evaluate_percent(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_percentage_error = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, price in dataloader:\n",
    "            # Calculate the model's prediction on the given artist, title and numerics\n",
    "            outputs = model(x)\n",
    "            \n",
    "            # Calculate absolute percentage error\n",
    "            abs_percentage_error = torch.abs((outputs - price) / price) * 100\n",
    "            \n",
    "            # Accumulate the sum of percentage errors\n",
    "            total_percentage_error += abs_percentage_error.sum().item()\n",
    "            \n",
    "            # Count the number of samples\n",
    "            num_samples += price.size(0)\n",
    "    \n",
    "    # Calculate the mean percentage error\n",
    "    mean_percentage_error = total_percentage_error / num_samples\n",
    "    return mean_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b45ff947-455f-4e77-8cb2-4c72a9810124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of the model without training on the training set: inf%\n",
      "Error of the model without training on the validation set: 99.60106542194632%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Error of the model without training on the training set: {evaluate_percent(model, train_loader, criterion)}%\")\n",
    "print(f\"Error of the model without training on the validation set: {evaluate_percent(model, val_loader, criterion)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42137855-9170-42de-a068-0e253cab3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average loss of the model \n",
    "def evaluate_loss(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, price in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(artist, title, numerics)\n",
    "            loss = criterion(outputs, price)\n",
    "            \n",
    "            # Accumulate the loss\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4bb2d50c-6c9e-447e-a90e-bc4f17b7cc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Price: $2830.89\n",
      "Mean Price: $31777.83\n",
      "Normalized to MEDIAN\n",
      "Dataset loaded successfully!\n",
      "Median Price: $2830.89\n",
      "Mean Price: $31777.83\n",
      "Normalized to MEDIAN\n",
      "Dataset loaded successfully!\n",
      "Mean Squared Error of the untrained model on Validation Set: 1807.3191605196655\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Squared Error of the untrained model on Validation Set: {evaluate_loss(model, val_loader, criterion)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09a37ef9-218a-4c1e-a67d-e74114349f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 98.10074615478516\n",
      "Loss at step 100: 98.79345703125\n",
      "Loss at step 200: 98.26093292236328\n",
      "Loss at step 300: 98.58424377441406\n",
      "Loss at step 400: 100.46464538574219\n",
      "Loss at step 500: 98.22772979736328\n",
      "Loss at step 600: 99.21451568603516\n",
      "Loss at step 700: 98.0748519897461\n",
      "Loss at step 800: 100.44029998779297\n",
      "Loss at step 900: 102.77926635742188\n",
      "Loss at step 1000: 98.0472412109375\n",
      "Loss at step 1100: 100.29419708251953\n",
      "Loss at step 1200: 98.06096649169922\n",
      "Epoch 1/1000, Avg Train Loss: 99.5037512732941\n",
      "Loss at step 0: 99.05168151855469\n",
      "Loss at step 100: 98.28726196289062\n",
      "Loss at step 200: 99.45365905761719\n",
      "Loss at step 300: 98.72306823730469\n",
      "Loss at step 400: 99.27804565429688\n",
      "Loss at step 500: 100.13745880126953\n",
      "Loss at step 600: 99.2129135131836\n",
      "Loss at step 700: 98.7256088256836\n",
      "Loss at step 800: 98.78205871582031\n",
      "Loss at step 900: 106.23340606689453\n",
      "Loss at step 1000: 98.32015991210938\n",
      "Loss at step 1100: 99.21719360351562\n",
      "Loss at step 1200: 98.65901184082031\n",
      "Epoch 2/1000, Avg Train Loss: 99.46883629980982\n",
      "Loss at step 0: 114.23286437988281\n",
      "Loss at step 100: 102.439208984375\n",
      "Loss at step 200: 99.58062744140625\n",
      "Loss at step 300: 97.89729309082031\n",
      "Loss at step 400: 102.47976684570312\n",
      "Loss at step 500: 99.32899475097656\n",
      "Loss at step 600: 104.85771942138672\n",
      "Loss at step 700: 97.43453216552734\n",
      "Loss at step 800: 99.3169937133789\n",
      "Loss at step 900: 98.27519989013672\n",
      "Loss at step 1000: 100.54927062988281\n",
      "Loss at step 1100: 98.69076538085938\n",
      "Loss at step 1200: 98.28050231933594\n",
      "Epoch 3/1000, Avg Train Loss: 99.44522490393382\n",
      "Loss at step 0: 97.34456634521484\n",
      "Loss at step 100: 97.0862045288086\n",
      "Loss at step 200: 98.4619140625\n",
      "Loss at step 300: 98.35494995117188\n",
      "Loss at step 400: 98.3983154296875\n",
      "Loss at step 500: 98.56233215332031\n",
      "Loss at step 600: 99.3369140625\n",
      "Loss at step 700: 98.52046966552734\n",
      "Loss at step 800: 98.27034759521484\n",
      "Loss at step 900: 99.00274658203125\n",
      "Loss at step 1000: 98.60382843017578\n",
      "Loss at step 1100: 97.78440856933594\n",
      "Loss at step 1200: 99.61937713623047\n",
      "Epoch 4/1000, Avg Train Loss: 99.46256910404341\n",
      "Loss at step 0: 99.0131607055664\n",
      "Loss at step 100: 100.6212387084961\n",
      "Loss at step 200: 98.82461547851562\n",
      "Loss at step 300: 98.47289276123047\n",
      "Loss at step 400: 98.0979232788086\n",
      "Loss at step 500: 98.24420166015625\n",
      "Loss at step 600: 99.54971313476562\n",
      "Loss at step 700: 104.28426361083984\n",
      "Loss at step 800: 99.72852325439453\n",
      "Loss at step 900: 97.3121337890625\n",
      "Loss at step 1000: 98.90181732177734\n",
      "Loss at step 1100: 98.32402801513672\n",
      "Loss at step 1200: 99.46176147460938\n",
      "Epoch 5/1000, Avg Train Loss: 99.42835204732457\n",
      "Loss at step 0: 98.4942398071289\n",
      "Loss at step 100: 98.72637176513672\n",
      "Loss at step 200: 98.92090606689453\n",
      "Loss at step 300: 98.41749572753906\n",
      "Loss at step 400: 98.99203491210938\n",
      "Loss at step 500: 98.173583984375\n",
      "Loss at step 600: 97.7901611328125\n",
      "Loss at step 700: 97.32470703125\n",
      "Loss at step 800: 100.10142517089844\n",
      "Loss at step 900: 99.04168701171875\n",
      "Loss at step 1000: 98.42611694335938\n",
      "Loss at step 1100: 99.71131896972656\n",
      "Loss at step 1200: 97.94708251953125\n",
      "Epoch 6/1000, Avg Train Loss: 99.48612816819868\n",
      "Loss at step 0: 141.63526916503906\n",
      "Loss at step 100: 98.21344757080078\n",
      "Loss at step 200: 98.98770141601562\n",
      "Loss at step 300: 98.9250717163086\n",
      "Loss at step 400: 98.54882049560547\n",
      "Loss at step 500: 98.89657592773438\n",
      "Loss at step 600: 97.97994995117188\n",
      "Loss at step 700: 98.34391784667969\n",
      "Loss at step 800: 98.76106262207031\n",
      "Loss at step 900: 98.21250915527344\n",
      "Loss at step 1000: 99.11611938476562\n",
      "Loss at step 1100: 98.10933685302734\n",
      "Loss at step 1200: 100.84253692626953\n",
      "Epoch 7/1000, Avg Train Loss: 99.42292475777536\n",
      "Loss at step 0: 98.96146392822266\n",
      "Loss at step 100: 98.80977630615234\n",
      "Loss at step 200: 97.79325866699219\n",
      "Loss at step 300: 100.88274383544922\n",
      "Loss at step 400: 99.21597290039062\n",
      "Loss at step 500: 98.96644592285156\n",
      "Loss at step 600: 98.74678039550781\n",
      "Loss at step 700: 98.70135498046875\n",
      "Loss at step 800: 97.43553161621094\n",
      "Loss at step 900: 103.94763946533203\n",
      "Loss at step 1000: 98.6519775390625\n",
      "Loss at step 1100: 97.84947204589844\n",
      "Loss at step 1200: 98.12051391601562\n",
      "Epoch 8/1000, Avg Train Loss: 99.46286013211247\n",
      "Loss at step 0: 98.86344909667969\n",
      "Loss at step 100: 99.3711929321289\n",
      "Loss at step 200: 97.94136810302734\n",
      "Loss at step 300: 98.24544525146484\n",
      "Loss at step 400: 98.06736755371094\n",
      "Loss at step 500: 97.59100341796875\n",
      "Loss at step 600: 98.39726257324219\n",
      "Loss at step 700: 98.98347473144531\n",
      "Loss at step 800: 97.93890380859375\n",
      "Loss at step 900: 99.48346710205078\n",
      "Loss at step 1000: 99.52703857421875\n",
      "Loss at step 1100: 98.78726196289062\n",
      "Loss at step 1200: 98.54627227783203\n",
      "Epoch 9/1000, Avg Train Loss: 99.41444370430264\n",
      "Loss at step 0: 99.83121490478516\n",
      "Loss at step 100: 98.9925308227539\n",
      "Loss at step 200: 101.0064468383789\n",
      "Loss at step 300: 99.73914337158203\n",
      "Loss at step 400: 101.11251831054688\n",
      "Loss at step 500: 99.02278900146484\n",
      "Loss at step 600: 99.90385437011719\n",
      "Loss at step 700: 96.99967956542969\n",
      "Loss at step 800: 102.75259399414062\n",
      "Loss at step 900: 98.01383972167969\n",
      "Loss at step 1000: 99.04361724853516\n",
      "Loss at step 1100: 98.85601806640625\n",
      "Loss at step 1200: 99.3271713256836\n",
      "Epoch 10/1000, Avg Train Loss: 99.47610779796217\n",
      "Loss at step 0: 99.17875671386719\n",
      "Loss at step 100: 98.44140625\n",
      "Loss at step 200: 98.67911529541016\n",
      "Loss at step 300: 97.63858795166016\n",
      "Loss at step 400: 97.77931213378906\n",
      "Loss at step 500: 98.93869018554688\n",
      "Loss at step 600: 100.1363525390625\n",
      "Loss at step 700: 99.42503356933594\n",
      "Loss at step 800: 98.18350982666016\n",
      "Loss at step 900: 98.68586730957031\n",
      "Loss at step 1000: 111.53740692138672\n",
      "Loss at step 1100: 98.603759765625\n",
      "Loss at step 1200: 99.41483306884766\n",
      "Epoch 11/1000, Avg Train Loss: 99.47299106684318\n",
      "Loss at step 0: 100.1561279296875\n",
      "Loss at step 100: 97.95700073242188\n",
      "Loss at step 200: 97.53093719482422\n",
      "Loss at step 300: 98.58195495605469\n",
      "Loss at step 400: 99.18048858642578\n",
      "Loss at step 500: 99.42781829833984\n",
      "Loss at step 600: 97.45802307128906\n",
      "Loss at step 700: 101.16729736328125\n",
      "Loss at step 800: 97.48135375976562\n",
      "Loss at step 900: 99.23318481445312\n",
      "Loss at step 1000: 98.73458099365234\n",
      "Loss at step 1100: 98.7737808227539\n",
      "Loss at step 1200: 99.62895202636719\n",
      "Epoch 12/1000, Avg Train Loss: 99.53472802862765\n",
      "Loss at step 0: 97.47614288330078\n",
      "Loss at step 100: 97.55874633789062\n",
      "Loss at step 200: 99.28455352783203\n",
      "Loss at step 300: 98.33303833007812\n",
      "Loss at step 400: 98.42324829101562\n",
      "Loss at step 500: 100.0600814819336\n",
      "Loss at step 600: 98.93782043457031\n",
      "Loss at step 700: 101.9170150756836\n",
      "Loss at step 800: 98.66405487060547\n",
      "Loss at step 900: 97.26734161376953\n",
      "Loss at step 1000: 98.48590850830078\n",
      "Loss at step 1100: 98.60394287109375\n",
      "Loss at step 1200: 98.83189392089844\n",
      "Epoch 13/1000, Avg Train Loss: 99.48822324561455\n",
      "Loss at step 0: 99.94747924804688\n",
      "Loss at step 100: 98.09644317626953\n",
      "Loss at step 200: 99.15426635742188\n",
      "Loss at step 300: 98.78775787353516\n",
      "Loss at step 400: 97.59427642822266\n",
      "Loss at step 500: 101.1015396118164\n",
      "Loss at step 600: 97.65599060058594\n",
      "Loss at step 700: 100.18489074707031\n",
      "Loss at step 800: 99.46647644042969\n",
      "Loss at step 900: 100.24170684814453\n",
      "Loss at step 1000: 99.81397247314453\n",
      "Loss at step 1100: 108.75226593017578\n",
      "Loss at step 1200: 98.89600372314453\n",
      "Epoch 14/1000, Avg Train Loss: 99.48604854719538\n",
      "Loss at step 0: 98.45006561279297\n",
      "Loss at step 100: 99.45199584960938\n",
      "Loss at step 200: 99.67079162597656\n",
      "Loss at step 300: 98.43930053710938\n",
      "Loss at step 400: 99.7900390625\n",
      "Loss at step 500: 99.46700286865234\n",
      "Loss at step 600: 104.0704345703125\n",
      "Loss at step 700: 97.75191497802734\n",
      "Loss at step 800: 97.50115966796875\n",
      "Loss at step 900: 98.62761688232422\n",
      "Loss at step 1000: 99.64537048339844\n",
      "Loss at step 1100: 100.7491455078125\n",
      "Loss at step 1200: 99.52666473388672\n",
      "Epoch 15/1000, Avg Train Loss: 99.44165056963183\n",
      "Loss at step 0: 98.24034118652344\n",
      "Loss at step 100: 98.66942596435547\n",
      "Loss at step 200: 97.56651306152344\n",
      "Loss at step 300: 103.16036987304688\n",
      "Loss at step 400: 97.57614135742188\n",
      "Loss at step 500: 98.77432250976562\n",
      "Loss at step 600: 99.5000991821289\n",
      "Loss at step 700: 99.56340026855469\n",
      "Loss at step 800: 99.49274444580078\n",
      "Loss at step 900: 104.97257995605469\n",
      "Loss at step 1000: 99.04541778564453\n",
      "Loss at step 1100: 99.21266174316406\n",
      "Loss at step 1200: 97.40316772460938\n",
      "Epoch 16/1000, Avg Train Loss: 99.4985272922948\n",
      "Loss at step 0: 99.8647689819336\n",
      "Loss at step 100: 100.90351104736328\n",
      "Loss at step 200: 98.70329284667969\n",
      "Loss at step 300: 97.96736907958984\n",
      "Loss at step 400: 98.269775390625\n",
      "Loss at step 500: 98.4417495727539\n",
      "Loss at step 600: 98.77893829345703\n",
      "Loss at step 700: 98.84056091308594\n",
      "Loss at step 800: 99.86004638671875\n",
      "Loss at step 900: 100.01834869384766\n",
      "Loss at step 1000: 98.02464294433594\n",
      "Loss at step 1100: 98.030517578125\n",
      "Loss at step 1200: 99.0965347290039\n",
      "Epoch 17/1000, Avg Train Loss: 99.47293956843009\n",
      "Loss at step 0: 99.94055938720703\n",
      "Loss at step 100: 99.4766845703125\n",
      "Loss at step 200: 98.27134704589844\n",
      "Loss at step 300: 100.44068908691406\n",
      "Loss at step 400: 103.3320541381836\n",
      "Loss at step 500: 100.21896362304688\n",
      "Loss at step 600: 98.96259307861328\n",
      "Loss at step 700: 99.22425842285156\n",
      "Loss at step 800: 98.59566497802734\n",
      "Loss at step 900: 100.50862121582031\n",
      "Loss at step 1000: 101.04633331298828\n",
      "Loss at step 1100: 99.0023193359375\n",
      "Loss at step 1200: 97.4617919921875\n",
      "Epoch 18/1000, Avg Train Loss: 99.46285886671937\n",
      "Loss at step 0: 98.89740753173828\n",
      "Loss at step 100: 98.18154907226562\n",
      "Loss at step 200: 99.54273223876953\n",
      "Loss at step 300: 97.49855041503906\n",
      "Loss at step 400: 97.93003845214844\n",
      "Loss at step 500: 98.78839874267578\n",
      "Loss at step 600: 98.40486907958984\n",
      "Loss at step 700: 97.80555725097656\n",
      "Loss at step 800: 100.1824722290039\n",
      "Loss at step 900: 99.06143188476562\n",
      "Loss at step 1000: 102.3854751586914\n",
      "Loss at step 1100: 98.82329559326172\n",
      "Loss at step 1200: 100.38272094726562\n",
      "Epoch 19/1000, Avg Train Loss: 99.42706482773075\n",
      "Loss at step 0: 97.5101089477539\n",
      "Loss at step 100: 97.94361877441406\n",
      "Loss at step 200: 97.93327331542969\n",
      "Loss at step 300: 100.52076721191406\n",
      "Loss at step 400: 102.57169342041016\n",
      "Loss at step 500: 97.72631072998047\n",
      "Loss at step 600: 98.99435424804688\n",
      "Loss at step 700: 98.79563903808594\n",
      "Loss at step 800: 98.53274536132812\n",
      "Loss at step 900: 98.22349548339844\n",
      "Loss at step 1000: 97.6441879272461\n",
      "Loss at step 1100: 99.08625793457031\n",
      "Loss at step 1200: 98.18135833740234\n",
      "Epoch 20/1000, Avg Train Loss: 99.44122899003014\n",
      "Loss at step 0: 99.12835693359375\n",
      "Loss at step 100: 97.6943130493164\n",
      "Loss at step 200: 98.4764404296875\n",
      "Loss at step 300: 99.96211242675781\n",
      "Loss at step 400: 98.29846954345703\n",
      "Loss at step 500: 98.05007934570312\n",
      "Loss at step 600: 98.14898681640625\n",
      "Loss at step 700: 99.32328796386719\n",
      "Loss at step 800: 98.47427368164062\n",
      "Loss at step 900: 98.29568481445312\n",
      "Loss at step 1000: 97.48082733154297\n",
      "Loss at step 1100: 98.1176986694336\n",
      "Loss at step 1200: 98.99456787109375\n",
      "Epoch 21/1000, Avg Train Loss: 99.47912589014541\n",
      "Loss at step 0: 97.72442626953125\n",
      "Loss at step 100: 99.63103485107422\n",
      "Loss at step 200: 97.75177001953125\n",
      "Loss at step 300: 99.62834930419922\n",
      "Loss at step 400: 98.06027221679688\n",
      "Loss at step 500: 99.05145263671875\n",
      "Loss at step 600: 99.391845703125\n",
      "Loss at step 700: 98.08311462402344\n",
      "Loss at step 800: 98.52691650390625\n",
      "Loss at step 900: 99.5116958618164\n",
      "Loss at step 1000: 99.46150970458984\n",
      "Loss at step 1100: 98.76741027832031\n",
      "Loss at step 1200: 102.0557861328125\n",
      "Epoch 22/1000, Avg Train Loss: 99.46737664725788\n",
      "Loss at step 0: 97.6905746459961\n",
      "Loss at step 100: 98.77987670898438\n",
      "Loss at step 200: 99.02568817138672\n",
      "Loss at step 300: 97.87957763671875\n",
      "Loss at step 400: 108.91878509521484\n",
      "Loss at step 500: 102.11531066894531\n",
      "Loss at step 600: 98.46797943115234\n",
      "Loss at step 700: 97.4563217163086\n",
      "Loss at step 800: 99.24320220947266\n",
      "Loss at step 900: 99.574951171875\n",
      "Loss at step 1000: 101.2796630859375\n",
      "Loss at step 1100: 97.8232192993164\n",
      "Loss at step 1200: 98.71271514892578\n",
      "Epoch 23/1000, Avg Train Loss: 99.45325209484903\n",
      "Loss at step 0: 100.22180938720703\n",
      "Loss at step 100: 98.06661987304688\n",
      "Loss at step 200: 99.35404205322266\n",
      "Loss at step 300: 104.18243408203125\n",
      "Loss at step 400: 100.30464935302734\n",
      "Loss at step 500: 98.12364196777344\n",
      "Loss at step 600: 98.24897003173828\n",
      "Loss at step 700: 99.08444213867188\n",
      "Loss at step 800: 97.16275787353516\n",
      "Loss at step 900: 99.15946960449219\n",
      "Loss at step 1000: 105.49732208251953\n",
      "Loss at step 1100: 98.59799194335938\n",
      "Loss at step 1200: 98.46353149414062\n",
      "Epoch 24/1000, Avg Train Loss: 99.46115971537469\n",
      "Loss at step 0: 98.06212615966797\n",
      "Loss at step 100: 98.20603942871094\n",
      "Loss at step 200: 98.34022521972656\n",
      "Loss at step 300: 99.92964935302734\n",
      "Loss at step 400: 98.13383483886719\n",
      "Loss at step 500: 99.40569305419922\n",
      "Loss at step 600: 98.54119873046875\n",
      "Loss at step 700: 103.7475814819336\n",
      "Loss at step 800: 98.74527740478516\n",
      "Loss at step 900: 101.89757537841797\n",
      "Loss at step 1000: 98.66254425048828\n",
      "Loss at step 1100: 98.60515594482422\n",
      "Loss at step 1200: 101.12060546875\n",
      "Epoch 25/1000, Avg Train Loss: 99.45806300215736\n",
      "Loss at step 0: 102.4006118774414\n",
      "Loss at step 100: 99.43841552734375\n",
      "Loss at step 200: 98.27058410644531\n",
      "Loss at step 300: 98.54637145996094\n",
      "Loss at step 400: 100.29180145263672\n",
      "Loss at step 500: 98.84968566894531\n",
      "Loss at step 600: 97.17278289794922\n",
      "Loss at step 700: 97.87705993652344\n",
      "Loss at step 800: 99.25054168701172\n",
      "Loss at step 900: 98.30873107910156\n",
      "Loss at step 1000: 98.96046447753906\n",
      "Loss at step 1100: 98.96443176269531\n",
      "Loss at step 1200: 98.13478088378906\n",
      "Epoch 26/1000, Avg Train Loss: 99.4468757234345\n",
      "Loss at step 0: 98.54478454589844\n",
      "Loss at step 100: 102.07698059082031\n",
      "Loss at step 200: 98.61394500732422\n",
      "Loss at step 300: 101.77571868896484\n",
      "Loss at step 400: 100.00331115722656\n",
      "Loss at step 500: 99.82672119140625\n",
      "Loss at step 600: 99.57244110107422\n",
      "Loss at step 700: 98.10044860839844\n",
      "Loss at step 800: 98.47476196289062\n",
      "Loss at step 900: 99.31490325927734\n",
      "Loss at step 1000: 98.75655364990234\n",
      "Loss at step 1100: 97.9456787109375\n",
      "Loss at step 1200: 99.25175476074219\n",
      "Epoch 27/1000, Avg Train Loss: 99.433611280324\n",
      "Loss at step 0: 100.40066528320312\n",
      "Loss at step 100: 99.36402130126953\n",
      "Loss at step 200: 99.41181945800781\n",
      "Loss at step 300: 98.53585815429688\n",
      "Loss at step 400: 100.47721862792969\n",
      "Loss at step 500: 98.35809326171875\n",
      "Loss at step 600: 98.88029479980469\n",
      "Loss at step 700: 102.03802490234375\n",
      "Loss at step 800: 98.71421813964844\n",
      "Loss at step 900: 98.5200424194336\n",
      "Loss at step 1000: 101.55477905273438\n",
      "Loss at step 1100: 98.01553344726562\n",
      "Loss at step 1200: 100.23763275146484\n",
      "Epoch 28/1000, Avg Train Loss: 99.4386350452707\n",
      "Loss at step 0: 97.99359130859375\n",
      "Loss at step 100: 98.3389892578125\n",
      "Loss at step 200: 97.5726547241211\n",
      "Loss at step 300: 99.7350845336914\n",
      "Loss at step 400: 98.57538604736328\n",
      "Loss at step 500: 99.3022232055664\n",
      "Loss at step 600: 98.26890563964844\n",
      "Loss at step 700: 99.63731384277344\n",
      "Loss at step 800: 97.6798324584961\n",
      "Loss at step 900: 100.24545288085938\n",
      "Loss at step 1000: 99.64424133300781\n",
      "Loss at step 1100: 99.20396423339844\n",
      "Loss at step 1200: 98.48650360107422\n",
      "Epoch 29/1000, Avg Train Loss: 99.4700584658527\n",
      "Loss at step 0: 98.59691619873047\n",
      "Loss at step 100: 97.78205108642578\n",
      "Loss at step 200: 98.37027740478516\n",
      "Loss at step 300: 98.6024169921875\n",
      "Loss at step 400: 99.51134490966797\n",
      "Loss at step 500: 97.43673706054688\n",
      "Loss at step 600: 99.36622619628906\n",
      "Loss at step 700: 98.35298919677734\n",
      "Loss at step 800: 98.54590606689453\n",
      "Loss at step 900: 98.79840850830078\n",
      "Loss at step 1000: 98.19658660888672\n",
      "Loss at step 1100: 98.18494415283203\n",
      "Loss at step 1200: 100.33120727539062\n",
      "Epoch 30/1000, Avg Train Loss: 99.45563094747105\n",
      "Loss at step 0: 97.90569305419922\n",
      "Loss at step 100: 110.45106506347656\n",
      "Loss at step 200: 101.85569763183594\n",
      "Loss at step 300: 98.23934173583984\n",
      "Loss at step 400: 99.09261322021484\n",
      "Loss at step 500: 99.78021240234375\n",
      "Loss at step 600: 99.16265869140625\n",
      "Loss at step 700: 99.17475891113281\n",
      "Loss at step 800: 98.1874008178711\n",
      "Loss at step 900: 98.34503173828125\n",
      "Loss at step 1000: 100.36375427246094\n",
      "Loss at step 1100: 97.75794982910156\n",
      "Loss at step 1200: 97.94051361083984\n",
      "Epoch 31/1000, Avg Train Loss: 99.42674628199111\n",
      "Loss at step 0: 99.3837890625\n",
      "Loss at step 100: 98.03109741210938\n",
      "Loss at step 200: 99.39292907714844\n",
      "Loss at step 300: 99.16211700439453\n",
      "Loss at step 400: 102.11815643310547\n",
      "Loss at step 500: 98.31298828125\n",
      "Loss at step 600: 99.72704315185547\n",
      "Loss at step 700: 98.16100311279297\n",
      "Loss at step 800: 98.4188461303711\n",
      "Loss at step 900: 100.7970199584961\n",
      "Loss at step 1000: 101.93269348144531\n",
      "Loss at step 1100: 98.22516632080078\n",
      "Loss at step 1200: 98.59504699707031\n",
      "Epoch 32/1000, Avg Train Loss: 99.45822993528496\n",
      "Loss at step 0: 98.637939453125\n",
      "Loss at step 100: 101.59630584716797\n",
      "Loss at step 200: 98.32970428466797\n",
      "Loss at step 300: 98.6567611694336\n",
      "Loss at step 400: 98.75106811523438\n",
      "Loss at step 500: 97.89513397216797\n",
      "Loss at step 600: 97.72584533691406\n",
      "Loss at step 700: 98.53086853027344\n",
      "Loss at step 800: 101.87348175048828\n",
      "Loss at step 900: 98.8894271850586\n",
      "Loss at step 1000: 100.0133285522461\n",
      "Loss at step 1100: 102.04215240478516\n",
      "Loss at step 1200: 99.43020629882812\n",
      "Epoch 33/1000, Avg Train Loss: 99.4467975159679\n",
      "Loss at step 0: 98.61031341552734\n",
      "Loss at step 100: 99.65836334228516\n",
      "Loss at step 200: 98.17961120605469\n",
      "Loss at step 300: 99.26546478271484\n",
      "Loss at step 400: 98.98233032226562\n",
      "Loss at step 500: 98.5331802368164\n",
      "Loss at step 600: 99.38782501220703\n",
      "Loss at step 700: 98.4056625366211\n",
      "Loss at step 800: 99.20402526855469\n",
      "Loss at step 900: 97.33985900878906\n",
      "Loss at step 1000: 98.8814926147461\n",
      "Loss at step 1100: 100.825927734375\n",
      "Loss at step 1200: 97.660400390625\n",
      "Epoch 34/1000, Avg Train Loss: 99.42267317293532\n",
      "Loss at step 0: 97.98304748535156\n",
      "Loss at step 100: 98.60765838623047\n",
      "Loss at step 200: 98.97075653076172\n",
      "Loss at step 300: 103.15713500976562\n",
      "Loss at step 400: 101.5303955078125\n",
      "Loss at step 500: 99.63973999023438\n",
      "Loss at step 600: 97.78909301757812\n",
      "Loss at step 700: 100.1015625\n",
      "Loss at step 800: 98.71007537841797\n",
      "Loss at step 900: 98.14318084716797\n",
      "Loss at step 1000: 99.24371337890625\n",
      "Loss at step 1100: 100.51676177978516\n",
      "Loss at step 1200: 98.65686798095703\n",
      "Epoch 35/1000, Avg Train Loss: 99.45516899874295\n",
      "Loss at step 0: 98.16497802734375\n",
      "Loss at step 100: 114.2078857421875\n",
      "Loss at step 200: 98.94596099853516\n",
      "Loss at step 300: 97.61556243896484\n",
      "Loss at step 400: 98.11547088623047\n",
      "Loss at step 500: 97.88378143310547\n",
      "Loss at step 600: 98.15515899658203\n",
      "Loss at step 700: 98.06461334228516\n",
      "Loss at step 800: 97.76700592041016\n",
      "Loss at step 900: 98.26663970947266\n",
      "Loss at step 1000: 97.95858001708984\n",
      "Loss at step 1100: 98.52234649658203\n",
      "Loss at step 1200: 100.52610778808594\n",
      "Epoch 36/1000, Avg Train Loss: 99.4580771930781\n",
      "Loss at step 0: 98.39164733886719\n",
      "Loss at step 100: 98.65692901611328\n",
      "Loss at step 200: 99.0795669555664\n",
      "Loss at step 300: 99.07805633544922\n",
      "Loss at step 400: 98.76710510253906\n",
      "Loss at step 500: 107.67515563964844\n",
      "Loss at step 600: 99.27609252929688\n",
      "Loss at step 700: 100.40541076660156\n",
      "Loss at step 800: 98.45472717285156\n",
      "Loss at step 900: 97.21033477783203\n",
      "Loss at step 1000: 97.77973175048828\n",
      "Loss at step 1100: 99.0875473022461\n",
      "Loss at step 1200: 98.37265014648438\n",
      "Epoch 37/1000, Avg Train Loss: 99.49908500967673\n",
      "Loss at step 0: 100.46589660644531\n",
      "Loss at step 100: 98.75455474853516\n",
      "Loss at step 200: 98.55006408691406\n",
      "Loss at step 300: 97.86255645751953\n",
      "Loss at step 400: 99.36135864257812\n",
      "Loss at step 500: 98.76165008544922\n",
      "Loss at step 600: 103.60933685302734\n",
      "Loss at step 700: 98.6521987915039\n",
      "Loss at step 800: 99.02896118164062\n",
      "Loss at step 900: 99.81732940673828\n",
      "Loss at step 1000: 98.32710266113281\n",
      "Loss at step 1100: 101.20470428466797\n",
      "Loss at step 1200: 98.47252655029297\n",
      "Epoch 38/1000, Avg Train Loss: 99.43994431974046\n",
      "Loss at step 0: 99.40628051757812\n",
      "Loss at step 100: 99.32466888427734\n",
      "Loss at step 200: 100.7136001586914\n",
      "Loss at step 300: 97.93502044677734\n",
      "Loss at step 400: 98.8976058959961\n",
      "Loss at step 500: 98.8103256225586\n",
      "Loss at step 600: 99.94625091552734\n",
      "Loss at step 700: 99.1606674194336\n",
      "Loss at step 800: 99.53104400634766\n",
      "Loss at step 900: 100.71575927734375\n",
      "Loss at step 1000: 98.1390609741211\n",
      "Loss at step 1100: 98.45625305175781\n",
      "Loss at step 1200: 97.8105697631836\n",
      "Epoch 39/1000, Avg Train Loss: 99.47417013544866\n",
      "Loss at step 0: 98.91270446777344\n",
      "Loss at step 100: 98.68671417236328\n",
      "Loss at step 200: 98.78791809082031\n",
      "Loss at step 300: 102.44664001464844\n",
      "Loss at step 400: 97.86602020263672\n",
      "Loss at step 500: 98.18961334228516\n",
      "Loss at step 600: 98.86972045898438\n",
      "Loss at step 700: 100.00497436523438\n",
      "Loss at step 800: 100.5860824584961\n",
      "Loss at step 900: 97.9542465209961\n",
      "Loss at step 1000: 98.16124725341797\n",
      "Loss at step 1100: 99.24856567382812\n",
      "Loss at step 1200: 99.34829711914062\n",
      "Epoch 40/1000, Avg Train Loss: 99.44508047011293\n",
      "Loss at step 0: 99.19375610351562\n",
      "Loss at step 100: 110.71176147460938\n",
      "Loss at step 200: 98.7552261352539\n",
      "Loss at step 300: 99.35215759277344\n",
      "Loss at step 400: 97.55567169189453\n",
      "Loss at step 500: 101.13877868652344\n",
      "Loss at step 600: 98.4521484375\n",
      "Loss at step 700: 97.915771484375\n",
      "Loss at step 800: 98.27159881591797\n",
      "Loss at step 900: 97.86834716796875\n",
      "Loss at step 1000: 100.96926879882812\n",
      "Loss at step 1100: 98.26795959472656\n",
      "Loss at step 1200: 99.13955688476562\n",
      "Epoch 41/1000, Avg Train Loss: 99.50756920502795\n",
      "Loss at step 0: 98.6996078491211\n",
      "Loss at step 100: 102.30033111572266\n",
      "Loss at step 200: 101.54257202148438\n",
      "Loss at step 300: 126.94248962402344\n",
      "Loss at step 400: 100.76255798339844\n",
      "Loss at step 500: 97.80358123779297\n",
      "Loss at step 600: 102.35299682617188\n",
      "Loss at step 700: 98.258056640625\n",
      "Loss at step 800: 97.92403411865234\n",
      "Loss at step 900: 98.28996276855469\n",
      "Loss at step 1000: 98.6397705078125\n",
      "Loss at step 1100: 98.35067749023438\n",
      "Loss at step 1200: 99.86026763916016\n",
      "Epoch 42/1000, Avg Train Loss: 99.42712252965637\n",
      "Loss at step 0: 98.6053695678711\n",
      "Loss at step 100: 99.56912231445312\n",
      "Loss at step 200: 101.9044189453125\n",
      "Loss at step 300: 98.80606842041016\n",
      "Loss at step 400: 99.07303619384766\n",
      "Loss at step 500: 98.46794891357422\n",
      "Loss at step 600: 98.70370483398438\n",
      "Loss at step 700: 100.8494644165039\n",
      "Loss at step 800: 97.80410766601562\n",
      "Loss at step 900: 98.42881774902344\n",
      "Loss at step 1000: 98.61394500732422\n",
      "Loss at step 1100: 98.77910614013672\n",
      "Loss at step 1200: 97.83622741699219\n",
      "Epoch 43/1000, Avg Train Loss: 99.44345425479234\n",
      "Loss at step 0: 97.78385925292969\n",
      "Loss at step 100: 98.24397277832031\n",
      "Loss at step 200: 99.45404815673828\n",
      "Loss at step 300: 98.503173828125\n",
      "Loss at step 400: 98.12379455566406\n",
      "Loss at step 500: 99.30636596679688\n",
      "Loss at step 600: 99.23616790771484\n",
      "Loss at step 700: 99.63319396972656\n",
      "Loss at step 800: 98.39860534667969\n",
      "Loss at step 900: 98.66447448730469\n",
      "Loss at step 1000: 97.10774230957031\n",
      "Loss at step 1100: 98.3138656616211\n",
      "Loss at step 1200: 98.3145523071289\n",
      "Epoch 44/1000, Avg Train Loss: 99.44998848554\n",
      "Loss at step 0: 98.10530090332031\n",
      "Loss at step 100: 99.06636810302734\n",
      "Loss at step 200: 100.58454132080078\n",
      "Loss at step 300: 114.12024688720703\n",
      "Loss at step 400: 98.9437484741211\n",
      "Loss at step 500: 98.1600570678711\n",
      "Loss at step 600: 98.4561538696289\n",
      "Loss at step 700: 97.06376647949219\n",
      "Loss at step 800: 99.59782409667969\n",
      "Loss at step 900: 98.71459197998047\n",
      "Loss at step 1000: 99.07840728759766\n",
      "Loss at step 1100: 100.54173278808594\n",
      "Loss at step 1200: 98.38411712646484\n",
      "Epoch 45/1000, Avg Train Loss: 99.4256345631621\n",
      "Loss at step 0: 96.28178405761719\n",
      "Loss at step 100: 97.07140350341797\n",
      "Loss at step 200: 101.06942749023438\n",
      "Loss at step 300: 98.3062744140625\n",
      "Loss at step 400: 99.4752197265625\n",
      "Loss at step 500: 99.40460968017578\n",
      "Loss at step 600: 98.83746337890625\n",
      "Loss at step 700: 111.50115966796875\n",
      "Loss at step 800: 100.54440307617188\n",
      "Loss at step 900: 98.3917465209961\n",
      "Loss at step 1000: 99.28646850585938\n",
      "Loss at step 1100: 98.24394989013672\n",
      "Loss at step 1200: 98.3753662109375\n",
      "Epoch 46/1000, Avg Train Loss: 99.45581952807973\n",
      "Loss at step 0: 98.02220153808594\n",
      "Loss at step 100: 98.2580337524414\n",
      "Loss at step 200: 98.42300415039062\n",
      "Loss at step 300: 98.4532470703125\n",
      "Loss at step 400: 98.86590576171875\n",
      "Loss at step 500: 99.00960540771484\n",
      "Loss at step 600: 99.35287475585938\n",
      "Loss at step 700: 137.04510498046875\n",
      "Loss at step 800: 98.34537506103516\n",
      "Loss at step 900: 101.07453155517578\n",
      "Loss at step 1000: 99.68952178955078\n",
      "Loss at step 1100: 98.59769439697266\n",
      "Loss at step 1200: 98.30967712402344\n",
      "Epoch 47/1000, Avg Train Loss: 99.46898236321015\n",
      "Loss at step 0: 106.2591552734375\n",
      "Loss at step 100: 98.8019790649414\n",
      "Loss at step 200: 99.86600494384766\n",
      "Loss at step 300: 98.84996032714844\n",
      "Loss at step 400: 98.91280364990234\n",
      "Loss at step 500: 98.29605865478516\n",
      "Loss at step 600: 98.65702056884766\n",
      "Loss at step 700: 98.12916564941406\n",
      "Loss at step 800: 97.84873962402344\n",
      "Loss at step 900: 99.7775650024414\n",
      "Loss at step 1000: 98.3382568359375\n",
      "Loss at step 1100: 99.6768569946289\n",
      "Loss at step 1200: 98.25503540039062\n",
      "Epoch 48/1000, Avg Train Loss: 99.4057613545637\n",
      "Loss at step 0: 97.57398223876953\n",
      "Loss at step 100: 98.194091796875\n",
      "Loss at step 200: 100.55370330810547\n",
      "Loss at step 300: 100.33272552490234\n",
      "Loss at step 400: 99.02799987792969\n",
      "Loss at step 500: 101.51309967041016\n",
      "Loss at step 600: 97.76276397705078\n",
      "Loss at step 700: 99.5050277709961\n",
      "Loss at step 800: 99.36944580078125\n",
      "Loss at step 900: 97.64170837402344\n",
      "Loss at step 1000: 98.28885650634766\n",
      "Loss at step 1100: 98.23766326904297\n",
      "Loss at step 1200: 98.84506225585938\n",
      "Epoch 49/1000, Avg Train Loss: 99.44009870387204\n",
      "Loss at step 0: 99.27740478515625\n",
      "Loss at step 100: 100.0703353881836\n",
      "Loss at step 200: 98.02869415283203\n",
      "Loss at step 300: 98.0995864868164\n",
      "Loss at step 400: 99.7245101928711\n",
      "Loss at step 500: 98.02685546875\n",
      "Loss at step 600: 98.7535400390625\n",
      "Loss at step 700: 98.19772338867188\n",
      "Loss at step 800: 98.09851837158203\n",
      "Loss at step 900: 98.53304290771484\n",
      "Loss at step 1000: 98.137451171875\n",
      "Loss at step 1100: 99.3825454711914\n",
      "Loss at step 1200: 97.78775787353516\n",
      "Epoch 50/1000, Avg Train Loss: 99.46230198264507\n",
      "Loss at step 0: 97.73348999023438\n",
      "Loss at step 100: 99.0075454711914\n",
      "Loss at step 200: 97.74759674072266\n",
      "Loss at step 300: 97.9836196899414\n",
      "Loss at step 400: 99.75434112548828\n",
      "Loss at step 500: 97.8183822631836\n",
      "Loss at step 600: 97.87842559814453\n",
      "Loss at step 700: 98.88914489746094\n",
      "Loss at step 800: 99.17980194091797\n",
      "Loss at step 900: 104.00543212890625\n",
      "Loss at step 1000: 100.06494140625\n",
      "Loss at step 1100: 98.10294342041016\n",
      "Loss at step 1200: 100.63001251220703\n",
      "Epoch 51/1000, Avg Train Loss: 99.44914904227149\n",
      "Loss at step 0: 100.4892807006836\n",
      "Loss at step 100: 98.98336791992188\n",
      "Loss at step 200: 99.1624526977539\n",
      "Loss at step 300: 99.36771392822266\n",
      "Loss at step 400: 98.50675201416016\n",
      "Loss at step 500: 106.13587951660156\n",
      "Loss at step 600: 98.39366912841797\n",
      "Loss at step 700: 99.08538818359375\n",
      "Loss at step 800: 97.7205810546875\n",
      "Loss at step 900: 98.57138061523438\n",
      "Loss at step 1000: 100.92027282714844\n",
      "Loss at step 1100: 99.6214599609375\n",
      "Loss at step 1200: 99.10322570800781\n",
      "Epoch 52/1000, Avg Train Loss: 99.47545126881029\n",
      "Loss at step 0: 100.66930389404297\n",
      "Loss at step 100: 100.81670379638672\n",
      "Loss at step 200: 107.58322143554688\n",
      "Loss at step 300: 103.24639892578125\n",
      "Loss at step 400: 100.62296295166016\n",
      "Loss at step 500: 98.03826141357422\n",
      "Loss at step 600: 98.64543151855469\n",
      "Loss at step 700: 97.60105895996094\n",
      "Loss at step 800: 98.32669830322266\n",
      "Loss at step 900: 99.40608978271484\n",
      "Loss at step 1000: 99.68589782714844\n",
      "Loss at step 1100: 99.38969421386719\n",
      "Loss at step 1200: 97.97718811035156\n",
      "Epoch 53/1000, Avg Train Loss: 99.41638187297339\n",
      "Loss at step 0: 107.66014099121094\n",
      "Loss at step 100: 98.51178741455078\n",
      "Loss at step 200: 97.62940216064453\n",
      "Loss at step 300: 101.9118881225586\n",
      "Loss at step 400: 98.51993560791016\n",
      "Loss at step 500: 98.85765075683594\n",
      "Loss at step 600: 97.94084167480469\n",
      "Loss at step 700: 97.99211883544922\n",
      "Loss at step 800: 98.874267578125\n",
      "Loss at step 900: 110.64353942871094\n",
      "Loss at step 1000: 99.13665008544922\n",
      "Loss at step 1100: 101.55011749267578\n",
      "Loss at step 1200: 97.49334716796875\n",
      "Epoch 54/1000, Avg Train Loss: 99.43539313282396\n",
      "Loss at step 0: 99.3019790649414\n",
      "Loss at step 100: 99.64401245117188\n",
      "Loss at step 200: 99.62215423583984\n",
      "Loss at step 300: 99.10325622558594\n",
      "Loss at step 400: 97.93585205078125\n",
      "Loss at step 500: 98.4735107421875\n",
      "Loss at step 600: 100.17906188964844\n",
      "Loss at step 700: 98.9366683959961\n",
      "Loss at step 800: 99.13821411132812\n",
      "Loss at step 900: 99.71554565429688\n",
      "Loss at step 1000: 98.35212707519531\n",
      "Loss at step 1100: 98.50279235839844\n",
      "Loss at step 1200: 99.0174560546875\n",
      "Epoch 55/1000, Avg Train Loss: 99.46237546803496\n",
      "Loss at step 0: 101.3294906616211\n",
      "Loss at step 100: 98.93250274658203\n",
      "Loss at step 200: 98.11021423339844\n",
      "Loss at step 300: 97.90863800048828\n",
      "Loss at step 400: 98.32942199707031\n",
      "Loss at step 500: 99.02523040771484\n",
      "Loss at step 600: 98.15682983398438\n",
      "Loss at step 700: 97.18860626220703\n",
      "Loss at step 800: 99.71870422363281\n",
      "Loss at step 900: 98.43714141845703\n",
      "Loss at step 1000: 98.4264907836914\n",
      "Loss at step 1100: 99.59746551513672\n",
      "Loss at step 1200: 99.02873992919922\n",
      "Epoch 56/1000, Avg Train Loss: 99.45670779083153\n",
      "Loss at step 0: 98.5643081665039\n",
      "Loss at step 100: 100.79517364501953\n",
      "Loss at step 200: 100.71900177001953\n",
      "Loss at step 300: 106.54450988769531\n",
      "Loss at step 400: 99.89337921142578\n",
      "Loss at step 500: 100.74293518066406\n",
      "Loss at step 600: 98.04653930664062\n",
      "Loss at step 700: 99.26766204833984\n",
      "Loss at step 800: 100.77110290527344\n",
      "Loss at step 900: 97.94596099853516\n",
      "Loss at step 1000: 96.74168395996094\n",
      "Loss at step 1100: 99.66921997070312\n",
      "Loss at step 1200: 99.64181518554688\n",
      "Epoch 57/1000, Avg Train Loss: 99.4440732789271\n",
      "Loss at step 0: 98.70718383789062\n",
      "Loss at step 100: 98.64085388183594\n",
      "Loss at step 200: 98.6878890991211\n",
      "Loss at step 300: 99.82596588134766\n",
      "Loss at step 400: 99.0514907836914\n",
      "Loss at step 500: 97.16020965576172\n",
      "Loss at step 600: 101.30632019042969\n",
      "Loss at step 700: 99.17585754394531\n",
      "Loss at step 800: 113.15746307373047\n",
      "Loss at step 900: 101.80062866210938\n",
      "Loss at step 1000: 101.509521484375\n",
      "Loss at step 1100: 98.95843505859375\n",
      "Loss at step 1200: 99.11322784423828\n",
      "Epoch 58/1000, Avg Train Loss: 99.4170097672052\n",
      "Loss at step 0: 100.45968627929688\n",
      "Loss at step 100: 98.04994201660156\n",
      "Loss at step 200: 99.12348175048828\n",
      "Loss at step 300: 98.9128646850586\n",
      "Loss at step 400: 98.84819793701172\n",
      "Loss at step 500: 99.12712860107422\n",
      "Loss at step 600: 97.22896575927734\n",
      "Loss at step 700: 98.4595718383789\n",
      "Loss at step 800: 99.24711608886719\n",
      "Loss at step 900: 106.56897735595703\n",
      "Loss at step 1000: 97.61797332763672\n",
      "Loss at step 1100: 98.01626586914062\n",
      "Loss at step 1200: 99.81167602539062\n",
      "Epoch 59/1000, Avg Train Loss: 99.42930988817925\n",
      "Loss at step 0: 97.52667236328125\n",
      "Loss at step 100: 97.8297348022461\n",
      "Loss at step 200: 98.29971313476562\n",
      "Loss at step 300: 100.3984603881836\n",
      "Loss at step 400: 100.29608154296875\n",
      "Loss at step 500: 98.57195281982422\n",
      "Loss at step 600: 99.14669036865234\n",
      "Loss at step 700: 101.72979736328125\n",
      "Loss at step 800: 98.61697387695312\n",
      "Loss at step 900: 98.65692138671875\n",
      "Loss at step 1000: 98.99244689941406\n",
      "Loss at step 1100: 98.33063507080078\n",
      "Loss at step 1200: 100.71995544433594\n",
      "Epoch 60/1000, Avg Train Loss: 99.43740569426404\n",
      "Loss at step 0: 98.78730773925781\n",
      "Loss at step 100: 98.37648010253906\n",
      "Loss at step 200: 109.54058074951172\n",
      "Loss at step 300: 98.97640991210938\n",
      "Loss at step 400: 99.04342651367188\n",
      "Loss at step 500: 99.08331298828125\n",
      "Loss at step 600: 98.87004852294922\n",
      "Loss at step 700: 99.25273132324219\n",
      "Loss at step 800: 99.64693450927734\n",
      "Loss at step 900: 103.13658905029297\n",
      "Loss at step 1000: 98.65816497802734\n",
      "Loss at step 1100: 99.38893127441406\n",
      "Loss at step 1200: 97.7468490600586\n",
      "Epoch 61/1000, Avg Train Loss: 99.42751253615691\n",
      "Loss at step 0: 98.47123718261719\n",
      "Loss at step 100: 99.29669952392578\n",
      "Loss at step 200: 98.38116455078125\n",
      "Loss at step 300: 99.22476196289062\n",
      "Loss at step 400: 98.46267700195312\n",
      "Loss at step 500: 98.94880676269531\n",
      "Loss at step 600: 98.67288208007812\n",
      "Loss at step 700: 98.01404571533203\n",
      "Loss at step 800: 98.54190826416016\n",
      "Loss at step 900: 98.55596923828125\n",
      "Loss at step 1000: 97.90408325195312\n",
      "Loss at step 1100: 98.28266143798828\n",
      "Loss at step 1200: 104.20854187011719\n",
      "Epoch 62/1000, Avg Train Loss: 99.43679515134941\n",
      "Loss at step 0: 99.21049499511719\n",
      "Loss at step 100: 98.85072326660156\n",
      "Loss at step 200: 99.20178985595703\n",
      "Loss at step 300: 101.64790344238281\n",
      "Loss at step 400: 98.18375396728516\n",
      "Loss at step 500: 99.13944244384766\n",
      "Loss at step 600: 98.7238998413086\n",
      "Loss at step 700: 98.22604370117188\n",
      "Loss at step 800: 98.80028533935547\n",
      "Loss at step 900: 98.58857727050781\n",
      "Loss at step 1000: 98.73040771484375\n",
      "Loss at step 1100: 98.8393783569336\n",
      "Loss at step 1200: 98.40071105957031\n",
      "Epoch 63/1000, Avg Train Loss: 99.40649670844711\n",
      "Loss at step 0: 99.6985092163086\n",
      "Loss at step 100: 97.88488006591797\n",
      "Loss at step 200: 98.63440704345703\n",
      "Loss at step 300: 99.13846588134766\n",
      "Loss at step 400: 98.10250091552734\n",
      "Loss at step 500: 97.99559020996094\n",
      "Loss at step 600: 98.21595764160156\n",
      "Loss at step 700: 107.27114868164062\n",
      "Loss at step 800: 98.12382507324219\n",
      "Loss at step 900: 97.79143524169922\n",
      "Loss at step 1000: 98.0141372680664\n",
      "Loss at step 1100: 98.58341217041016\n",
      "Loss at step 1200: 102.86174774169922\n",
      "Epoch 64/1000, Avg Train Loss: 99.41156694109772\n",
      "Loss at step 0: 98.6075668334961\n",
      "Loss at step 100: 98.9782943725586\n",
      "Loss at step 200: 99.66950225830078\n",
      "Loss at step 300: 97.99374389648438\n",
      "Loss at step 400: 98.25167846679688\n",
      "Loss at step 500: 98.84135437011719\n",
      "Loss at step 600: 98.36784362792969\n",
      "Loss at step 700: 98.79557037353516\n",
      "Loss at step 800: 99.63021087646484\n",
      "Loss at step 900: 99.31880950927734\n",
      "Loss at step 1000: 101.17247009277344\n",
      "Loss at step 1100: 99.29360961914062\n",
      "Loss at step 1200: 100.53653717041016\n",
      "Epoch 65/1000, Avg Train Loss: 99.44287207520124\n",
      "Loss at step 0: 99.52576446533203\n",
      "Loss at step 100: 98.81497192382812\n",
      "Loss at step 200: 98.84320831298828\n",
      "Loss at step 300: 105.08384704589844\n",
      "Loss at step 400: 99.861083984375\n",
      "Loss at step 500: 98.16633605957031\n",
      "Loss at step 600: 98.2879638671875\n",
      "Loss at step 700: 103.33425903320312\n",
      "Loss at step 800: 98.84009552001953\n",
      "Loss at step 900: 100.19700622558594\n",
      "Loss at step 1000: 99.15862274169922\n",
      "Loss at step 1100: 98.2445068359375\n",
      "Loss at step 1200: 98.0272216796875\n",
      "Epoch 66/1000, Avg Train Loss: 99.4579867637659\n",
      "Loss at step 0: 97.8437271118164\n",
      "Loss at step 100: 98.48939514160156\n",
      "Loss at step 200: 100.86617279052734\n",
      "Loss at step 300: 98.74220275878906\n",
      "Loss at step 400: 99.16051483154297\n",
      "Loss at step 500: 98.88088989257812\n",
      "Loss at step 600: 100.57508087158203\n",
      "Loss at step 700: 104.82201385498047\n",
      "Loss at step 800: 99.09935760498047\n",
      "Loss at step 900: 98.28642272949219\n",
      "Loss at step 1000: 99.85662841796875\n",
      "Loss at step 1100: 96.77115631103516\n",
      "Loss at step 1200: 98.73966217041016\n",
      "Epoch 67/1000, Avg Train Loss: 99.43426224791888\n",
      "Loss at step 0: 98.48119354248047\n",
      "Loss at step 100: 98.65690612792969\n",
      "Loss at step 200: 98.41978454589844\n",
      "Loss at step 300: 99.09471130371094\n",
      "Loss at step 400: 98.5538101196289\n",
      "Loss at step 500: 99.51422119140625\n",
      "Loss at step 600: 99.79293060302734\n",
      "Loss at step 700: 98.98694610595703\n",
      "Loss at step 800: 98.59284973144531\n",
      "Loss at step 900: 98.61383819580078\n",
      "Loss at step 1000: 98.25523376464844\n",
      "Loss at step 1100: 97.8546142578125\n",
      "Loss at step 1200: 98.14083862304688\n",
      "Epoch 68/1000, Avg Train Loss: 99.44162310134246\n",
      "Loss at step 0: 98.8870620727539\n",
      "Loss at step 100: 98.67996215820312\n",
      "Loss at step 200: 101.4001235961914\n",
      "Loss at step 300: 97.38027954101562\n",
      "Loss at step 400: 98.60700225830078\n",
      "Loss at step 500: 98.7277603149414\n",
      "Loss at step 600: 98.19330596923828\n",
      "Loss at step 700: 98.58353424072266\n",
      "Loss at step 800: 98.30986022949219\n",
      "Loss at step 900: 98.57737731933594\n",
      "Loss at step 1000: 99.19538116455078\n",
      "Loss at step 1100: 99.06759643554688\n",
      "Loss at step 1200: 96.8539810180664\n",
      "Epoch 69/1000, Avg Train Loss: 99.43589572844768\n",
      "Loss at step 0: 103.8797607421875\n",
      "Loss at step 100: 98.69597625732422\n",
      "Loss at step 200: 98.60078430175781\n",
      "Loss at step 300: 98.96257019042969\n",
      "Loss at step 400: 98.71923828125\n",
      "Loss at step 500: 98.92364501953125\n",
      "Loss at step 600: 101.13008880615234\n",
      "Loss at step 700: 98.89549255371094\n",
      "Loss at step 800: 99.94295501708984\n",
      "Loss at step 900: 98.1689453125\n",
      "Loss at step 1000: 98.31969451904297\n",
      "Loss at step 1100: 99.15044403076172\n",
      "Loss at step 1200: 99.17000579833984\n",
      "Epoch 70/1000, Avg Train Loss: 99.4294973885743\n",
      "Loss at step 0: 98.7234115600586\n",
      "Loss at step 100: 100.60236358642578\n",
      "Loss at step 200: 98.28243255615234\n",
      "Loss at step 300: 98.18630981445312\n",
      "Loss at step 400: 99.9829330444336\n",
      "Loss at step 500: 100.12764739990234\n",
      "Loss at step 600: 99.19029235839844\n",
      "Loss at step 700: 98.08318328857422\n",
      "Loss at step 800: 99.52753448486328\n",
      "Loss at step 900: 98.45933532714844\n",
      "Loss at step 1000: 100.79930114746094\n",
      "Loss at step 1100: 101.84408569335938\n",
      "Loss at step 1200: 98.59561920166016\n",
      "Epoch 71/1000, Avg Train Loss: 99.4321880710935\n",
      "Loss at step 0: 97.55937194824219\n",
      "Loss at step 100: 98.5306625366211\n",
      "Loss at step 200: 97.75355529785156\n",
      "Loss at step 300: 98.6589584350586\n",
      "Loss at step 400: 98.38752746582031\n",
      "Loss at step 500: 98.42035675048828\n",
      "Loss at step 600: 99.66165924072266\n",
      "Loss at step 700: 97.84738159179688\n",
      "Loss at step 800: 116.8681640625\n",
      "Loss at step 900: 98.3364486694336\n",
      "Loss at step 1000: 98.17674255371094\n",
      "Loss at step 1100: 97.13715362548828\n",
      "Loss at step 1200: 98.61480712890625\n",
      "Epoch 72/1000, Avg Train Loss: 99.43992644992075\n",
      "Loss at step 0: 98.2231216430664\n",
      "Loss at step 100: 100.14313507080078\n",
      "Loss at step 200: 98.2308349609375\n",
      "Loss at step 300: 100.27993774414062\n",
      "Loss at step 400: 98.85165405273438\n",
      "Loss at step 500: 98.47093963623047\n",
      "Loss at step 600: 99.4357681274414\n",
      "Loss at step 700: 98.4188461303711\n",
      "Loss at step 800: 98.61775207519531\n",
      "Loss at step 900: 97.93067169189453\n",
      "Loss at step 1000: 98.78480529785156\n",
      "Loss at step 1100: 110.22193145751953\n",
      "Loss at step 1200: 97.87635803222656\n",
      "Epoch 73/1000, Avg Train Loss: 99.41308041297889\n",
      "Loss at step 0: 100.6755599975586\n",
      "Loss at step 100: 105.77770233154297\n",
      "Loss at step 200: 98.87620544433594\n",
      "Loss at step 300: 99.72442626953125\n",
      "Loss at step 400: 99.22628784179688\n",
      "Loss at step 500: 101.10076904296875\n",
      "Loss at step 600: 100.46393585205078\n",
      "Loss at step 700: 98.73639678955078\n",
      "Loss at step 800: 98.02604675292969\n",
      "Loss at step 900: 98.98796081542969\n",
      "Loss at step 1000: 112.4206314086914\n",
      "Loss at step 1100: 98.16779327392578\n",
      "Loss at step 1200: 98.88420867919922\n",
      "Epoch 74/1000, Avg Train Loss: 99.42486523501695\n",
      "Loss at step 0: 98.67688751220703\n",
      "Loss at step 100: 100.49031829833984\n",
      "Loss at step 200: 98.85292053222656\n",
      "Loss at step 300: 99.5367660522461\n",
      "Loss at step 400: 97.79436492919922\n",
      "Loss at step 500: 98.05934143066406\n",
      "Loss at step 600: 98.28865814208984\n",
      "Loss at step 700: 98.56121826171875\n",
      "Loss at step 800: 99.006103515625\n",
      "Loss at step 900: 100.02771759033203\n",
      "Loss at step 1000: 99.67981719970703\n",
      "Loss at step 1100: 100.42724609375\n",
      "Loss at step 1200: 98.03364562988281\n",
      "Epoch 75/1000, Avg Train Loss: 99.48790203329042\n",
      "Loss at step 0: 97.32176971435547\n",
      "Loss at step 100: 98.73907470703125\n",
      "Loss at step 200: 97.75594329833984\n",
      "Loss at step 300: 105.06129455566406\n",
      "Loss at step 400: 98.74032592773438\n",
      "Loss at step 500: 98.9732894897461\n",
      "Loss at step 600: 99.8351821899414\n",
      "Loss at step 700: 98.75724029541016\n",
      "Loss at step 800: 100.5594482421875\n",
      "Loss at step 900: 98.46807861328125\n",
      "Loss at step 1000: 99.25524139404297\n",
      "Loss at step 1100: 99.16704559326172\n",
      "Loss at step 1200: 98.96434783935547\n",
      "Epoch 76/1000, Avg Train Loss: 99.45824902728923\n",
      "Loss at step 0: 98.9895248413086\n",
      "Loss at step 100: 98.56157684326172\n",
      "Loss at step 200: 105.16969299316406\n",
      "Loss at step 300: 101.78337097167969\n",
      "Loss at step 400: 98.81460571289062\n",
      "Loss at step 500: 100.19947052001953\n",
      "Loss at step 600: 100.47982025146484\n",
      "Loss at step 700: 102.51153564453125\n",
      "Loss at step 800: 98.1823959350586\n",
      "Loss at step 900: 97.53411102294922\n",
      "Loss at step 1000: 97.95339965820312\n",
      "Loss at step 1100: 98.56544494628906\n",
      "Loss at step 1200: 99.75027465820312\n",
      "Epoch 77/1000, Avg Train Loss: 99.41628882028525\n",
      "Loss at step 0: 98.39795684814453\n",
      "Loss at step 100: 99.07670593261719\n",
      "Loss at step 200: 98.77902221679688\n",
      "Loss at step 300: 98.40713500976562\n",
      "Loss at step 400: 100.52392578125\n",
      "Loss at step 500: 98.71588897705078\n",
      "Loss at step 600: 99.38982391357422\n",
      "Loss at step 700: 97.8807601928711\n",
      "Loss at step 800: 101.98744201660156\n",
      "Loss at step 900: 100.5329818725586\n",
      "Loss at step 1000: 98.73070526123047\n",
      "Loss at step 1100: 100.1159896850586\n",
      "Loss at step 1200: 99.48816680908203\n",
      "Epoch 78/1000, Avg Train Loss: 99.43186795132831\n",
      "Loss at step 0: 98.0963134765625\n",
      "Loss at step 100: 98.96920013427734\n",
      "Loss at step 200: 100.55787658691406\n",
      "Loss at step 300: 98.81082916259766\n",
      "Loss at step 400: 98.9135971069336\n",
      "Loss at step 500: 99.23900604248047\n",
      "Loss at step 600: 98.3319320678711\n",
      "Loss at step 700: 98.50398254394531\n",
      "Loss at step 800: 98.80925750732422\n",
      "Loss at step 900: 105.73954772949219\n",
      "Loss at step 1000: 100.83699035644531\n",
      "Loss at step 1100: 101.8339614868164\n",
      "Loss at step 1200: 98.65959167480469\n",
      "Epoch 79/1000, Avg Train Loss: 99.48651277363108\n",
      "Loss at step 0: 98.51629638671875\n",
      "Loss at step 100: 98.354248046875\n",
      "Loss at step 200: 98.360107421875\n",
      "Loss at step 300: 98.79512786865234\n",
      "Loss at step 400: 98.36304473876953\n",
      "Loss at step 500: 97.5051040649414\n",
      "Loss at step 600: 97.1113052368164\n",
      "Loss at step 700: 98.11669921875\n",
      "Loss at step 800: 98.83967590332031\n",
      "Loss at step 900: 99.62957763671875\n",
      "Loss at step 1000: 101.38130187988281\n",
      "Loss at step 1100: 99.21442413330078\n",
      "Loss at step 1200: 97.94268798828125\n",
      "Epoch 80/1000, Avg Train Loss: 99.44717452286902\n",
      "Loss at step 0: 99.33528900146484\n",
      "Loss at step 100: 98.76969909667969\n",
      "Loss at step 200: 98.90849304199219\n",
      "Loss at step 300: 98.97425842285156\n",
      "Loss at step 400: 99.98025512695312\n",
      "Loss at step 500: 98.8577651977539\n",
      "Loss at step 600: 98.38522338867188\n",
      "Loss at step 700: 100.7818832397461\n",
      "Loss at step 800: 98.30245208740234\n",
      "Loss at step 900: 99.23521423339844\n",
      "Loss at step 1000: 100.87805938720703\n",
      "Loss at step 1100: 99.09082794189453\n",
      "Loss at step 1200: 98.05953216552734\n",
      "Epoch 81/1000, Avg Train Loss: 99.42770832641996\n",
      "Loss at step 0: 98.31246948242188\n",
      "Loss at step 100: 98.43489837646484\n",
      "Loss at step 200: 99.69950866699219\n",
      "Loss at step 300: 99.35357666015625\n",
      "Loss at step 400: 97.49585723876953\n",
      "Loss at step 500: 101.07447814941406\n",
      "Loss at step 600: 99.22078704833984\n",
      "Loss at step 700: 99.77906036376953\n",
      "Loss at step 800: 98.17528533935547\n",
      "Loss at step 900: 98.03836059570312\n",
      "Loss at step 1000: 97.93122863769531\n",
      "Loss at step 1100: 97.32240295410156\n",
      "Loss at step 1200: 98.90135192871094\n",
      "Epoch 82/1000, Avg Train Loss: 99.44832160480586\n",
      "Loss at step 0: 99.40995788574219\n",
      "Loss at step 100: 101.36146545410156\n",
      "Loss at step 200: 98.04304504394531\n",
      "Loss at step 300: 98.65473175048828\n",
      "Loss at step 400: 98.51368713378906\n",
      "Loss at step 500: 100.2283935546875\n",
      "Loss at step 600: 99.2270736694336\n",
      "Loss at step 700: 98.5230941772461\n",
      "Loss at step 800: 101.16036224365234\n",
      "Loss at step 900: 97.6231689453125\n",
      "Loss at step 1000: 100.40694427490234\n",
      "Loss at step 1100: 98.76478576660156\n",
      "Loss at step 1200: 97.92386627197266\n",
      "Epoch 83/1000, Avg Train Loss: 99.420399261524\n",
      "Loss at step 0: 98.51710510253906\n",
      "Loss at step 100: 99.74163818359375\n",
      "Loss at step 200: 99.7619857788086\n",
      "Loss at step 300: 99.05982971191406\n",
      "Loss at step 400: 98.56298065185547\n",
      "Loss at step 500: 97.8265380859375\n",
      "Loss at step 600: 99.15679931640625\n",
      "Loss at step 700: 99.10433197021484\n",
      "Loss at step 800: 103.155517578125\n",
      "Loss at step 900: 99.02200317382812\n",
      "Loss at step 1000: 99.45140075683594\n",
      "Loss at step 1100: 98.57527923583984\n",
      "Loss at step 1200: 99.00029754638672\n",
      "Epoch 84/1000, Avg Train Loss: 99.44634149915578\n",
      "Loss at step 0: 98.89800262451172\n",
      "Loss at step 100: 98.37109375\n",
      "Loss at step 200: 99.29183959960938\n",
      "Loss at step 300: 98.40464782714844\n",
      "Loss at step 400: 99.41321563720703\n",
      "Loss at step 500: 98.07551574707031\n",
      "Loss at step 600: 97.56978607177734\n",
      "Loss at step 700: 97.91152954101562\n",
      "Loss at step 800: 103.13555145263672\n",
      "Loss at step 900: 98.00968170166016\n",
      "Loss at step 1000: 98.40180969238281\n",
      "Loss at step 1100: 101.72595977783203\n",
      "Loss at step 1200: 98.89308166503906\n",
      "Epoch 85/1000, Avg Train Loss: 99.42337177486482\n",
      "Loss at step 0: 98.56743621826172\n",
      "Loss at step 100: 98.53289031982422\n",
      "Loss at step 200: 108.4468002319336\n",
      "Loss at step 300: 98.41444396972656\n",
      "Loss at step 400: 99.31521606445312\n",
      "Loss at step 500: 99.45679473876953\n",
      "Loss at step 600: 97.4544906616211\n",
      "Loss at step 700: 97.40583801269531\n",
      "Loss at step 800: 98.20537567138672\n",
      "Loss at step 900: 99.12967681884766\n",
      "Loss at step 1000: 97.76914978027344\n",
      "Loss at step 1100: 98.87068176269531\n",
      "Loss at step 1200: 99.17575073242188\n",
      "Epoch 86/1000, Avg Train Loss: 99.42139340372918\n",
      "Loss at step 0: 100.26316833496094\n",
      "Loss at step 100: 102.4579849243164\n",
      "Loss at step 200: 101.3654556274414\n",
      "Loss at step 300: 99.18196868896484\n",
      "Loss at step 400: 98.67993927001953\n",
      "Loss at step 500: 101.67317199707031\n",
      "Loss at step 600: 97.23290252685547\n",
      "Loss at step 700: 98.49604034423828\n",
      "Loss at step 800: 104.537109375\n",
      "Loss at step 900: 99.1526870727539\n",
      "Loss at step 1000: 97.73446655273438\n",
      "Loss at step 1100: 99.75959777832031\n",
      "Loss at step 1200: 99.17259979248047\n",
      "Epoch 87/1000, Avg Train Loss: 99.44368882163829\n",
      "Loss at step 0: 99.98634338378906\n",
      "Loss at step 100: 100.68416595458984\n",
      "Loss at step 200: 99.84613800048828\n",
      "Loss at step 300: 99.22505187988281\n",
      "Loss at step 400: 98.82473754882812\n",
      "Loss at step 500: 100.62853240966797\n",
      "Loss at step 600: 104.8823013305664\n",
      "Loss at step 700: 99.66084289550781\n",
      "Loss at step 800: 98.28980255126953\n",
      "Loss at step 900: 98.421875\n",
      "Loss at step 1000: 98.80838012695312\n",
      "Loss at step 1100: 98.87738037109375\n",
      "Loss at step 1200: 99.07171630859375\n",
      "Epoch 88/1000, Avg Train Loss: 99.43356734340631\n",
      "Loss at step 0: 97.78307342529297\n",
      "Loss at step 100: 98.45367431640625\n",
      "Loss at step 200: 98.7829360961914\n",
      "Loss at step 300: 98.6952133178711\n",
      "Loss at step 400: 99.65802001953125\n",
      "Loss at step 500: 98.20732879638672\n",
      "Loss at step 600: 99.1479263305664\n",
      "Loss at step 700: 98.74971008300781\n",
      "Loss at step 800: 105.35227966308594\n",
      "Loss at step 900: 101.12862396240234\n",
      "Loss at step 1000: 99.42611694335938\n",
      "Loss at step 1100: 97.81427001953125\n",
      "Loss at step 1200: 97.91032409667969\n",
      "Epoch 89/1000, Avg Train Loss: 99.41035283653481\n",
      "Loss at step 0: 99.01563262939453\n",
      "Loss at step 100: 98.56075286865234\n",
      "Loss at step 200: 98.37834930419922\n",
      "Loss at step 300: 97.53681182861328\n",
      "Loss at step 400: 98.23351287841797\n",
      "Loss at step 500: 97.28994750976562\n",
      "Loss at step 600: 144.67405700683594\n",
      "Loss at step 700: 99.98611450195312\n",
      "Loss at step 800: 98.93827819824219\n",
      "Loss at step 900: 98.81068420410156\n",
      "Loss at step 1000: 98.11772155761719\n",
      "Loss at step 1100: 100.60069274902344\n",
      "Loss at step 1200: 98.01260375976562\n",
      "Epoch 90/1000, Avg Train Loss: 99.44186563707864\n",
      "Loss at step 0: 98.94425964355469\n",
      "Loss at step 100: 97.65181732177734\n",
      "Loss at step 200: 100.97482299804688\n",
      "Loss at step 300: 98.206787109375\n",
      "Loss at step 400: 98.92607879638672\n",
      "Loss at step 500: 98.12635040283203\n",
      "Loss at step 600: 99.64324951171875\n",
      "Loss at step 700: 98.958984375\n",
      "Loss at step 800: 97.77244567871094\n",
      "Loss at step 900: 106.66499328613281\n",
      "Loss at step 1000: 98.22728729248047\n",
      "Loss at step 1100: 99.0723648071289\n",
      "Loss at step 1200: 98.76113891601562\n",
      "Epoch 91/1000, Avg Train Loss: 99.41262059998743\n",
      "Loss at step 0: 98.14127349853516\n",
      "Loss at step 100: 98.58724212646484\n",
      "Loss at step 200: 99.83554077148438\n",
      "Loss at step 300: 97.57637023925781\n",
      "Loss at step 400: 99.12259674072266\n",
      "Loss at step 500: 98.75469207763672\n",
      "Loss at step 600: 97.40081787109375\n",
      "Loss at step 700: 98.69779205322266\n",
      "Loss at step 800: 97.99124145507812\n",
      "Loss at step 900: 98.6467514038086\n",
      "Loss at step 1000: 103.02100372314453\n",
      "Loss at step 1100: 97.4049301147461\n",
      "Loss at step 1200: 98.54592895507812\n",
      "Epoch 92/1000, Avg Train Loss: 99.39114021251888\n",
      "Loss at step 0: 97.29534149169922\n",
      "Loss at step 100: 99.09593200683594\n",
      "Loss at step 200: 98.28681182861328\n",
      "Loss at step 300: 98.13077545166016\n",
      "Loss at step 400: 98.86886596679688\n",
      "Loss at step 500: 98.99427032470703\n",
      "Loss at step 600: 97.86011505126953\n",
      "Loss at step 700: 105.17158508300781\n",
      "Loss at step 800: 100.05240631103516\n",
      "Loss at step 900: 99.10856628417969\n",
      "Loss at step 1000: 98.16373443603516\n",
      "Loss at step 1100: 99.17486572265625\n",
      "Loss at step 1200: 99.66087341308594\n",
      "Epoch 93/1000, Avg Train Loss: 99.3966854058423\n",
      "Loss at step 0: 98.83277130126953\n",
      "Loss at step 100: 100.50553894042969\n",
      "Loss at step 200: 103.97322082519531\n",
      "Loss at step 300: 97.9315185546875\n",
      "Loss at step 400: 97.6526107788086\n",
      "Loss at step 500: 98.8875732421875\n",
      "Loss at step 600: 98.63109588623047\n",
      "Loss at step 700: 99.198486328125\n",
      "Loss at step 800: 101.98273468017578\n",
      "Loss at step 900: 98.63005065917969\n",
      "Loss at step 1000: 98.75891876220703\n",
      "Loss at step 1100: 98.1483154296875\n",
      "Loss at step 1200: 99.14456939697266\n",
      "Epoch 94/1000, Avg Train Loss: 99.42426692629323\n",
      "Loss at step 0: 99.02479553222656\n",
      "Loss at step 100: 100.8051528930664\n",
      "Loss at step 200: 100.06101989746094\n",
      "Loss at step 300: 98.20024108886719\n",
      "Loss at step 400: 97.77880859375\n",
      "Loss at step 500: 100.18783569335938\n",
      "Loss at step 600: 98.65290069580078\n",
      "Loss at step 700: 102.53636932373047\n",
      "Loss at step 800: 102.2743911743164\n",
      "Loss at step 900: 99.58464813232422\n",
      "Loss at step 1000: 98.65788269042969\n",
      "Loss at step 1100: 98.14039611816406\n",
      "Loss at step 1200: 98.84474182128906\n",
      "Epoch 95/1000, Avg Train Loss: 99.41297639149292\n",
      "Loss at step 0: 99.65960693359375\n",
      "Loss at step 100: 98.23516082763672\n",
      "Loss at step 200: 98.56016540527344\n",
      "Loss at step 300: 97.38015747070312\n",
      "Loss at step 400: 98.17048645019531\n",
      "Loss at step 500: 98.18327331542969\n",
      "Loss at step 600: 100.23194885253906\n",
      "Loss at step 700: 97.70147705078125\n",
      "Loss at step 800: 99.13382720947266\n",
      "Loss at step 900: 98.8449935913086\n",
      "Loss at step 1000: 97.5284194946289\n",
      "Loss at step 1100: 96.72454833984375\n",
      "Loss at step 1200: 105.29391479492188\n",
      "Epoch 96/1000, Avg Train Loss: 99.39942603743964\n",
      "Loss at step 0: 98.462158203125\n",
      "Loss at step 100: 98.85884857177734\n",
      "Loss at step 200: 97.76483154296875\n",
      "Loss at step 300: 101.32183074951172\n",
      "Loss at step 400: 97.93840789794922\n",
      "Loss at step 500: 98.3419189453125\n",
      "Loss at step 600: 100.81927490234375\n",
      "Loss at step 700: 98.84253692626953\n",
      "Loss at step 800: 99.11912536621094\n",
      "Loss at step 900: 98.78404235839844\n",
      "Loss at step 1000: 98.2622299194336\n",
      "Loss at step 1100: 97.25767517089844\n",
      "Loss at step 1200: 109.88201904296875\n",
      "Epoch 97/1000, Avg Train Loss: 99.41905646339589\n",
      "Loss at step 0: 97.92769622802734\n",
      "Loss at step 100: 98.33268737792969\n",
      "Loss at step 200: 99.31363677978516\n",
      "Loss at step 300: 98.0753173828125\n",
      "Loss at step 400: 98.79016876220703\n",
      "Loss at step 500: 100.48451232910156\n",
      "Loss at step 600: 98.81616973876953\n",
      "Loss at step 700: 99.09542083740234\n",
      "Loss at step 800: 99.13007354736328\n",
      "Loss at step 900: 98.15253448486328\n",
      "Loss at step 1000: 100.07807922363281\n",
      "Loss at step 1100: 98.92396545410156\n",
      "Loss at step 1200: 100.62776947021484\n",
      "Epoch 98/1000, Avg Train Loss: 99.42567114845448\n",
      "Loss at step 0: 99.1402587890625\n",
      "Loss at step 100: 98.57392883300781\n",
      "Loss at step 200: 110.42601776123047\n",
      "Loss at step 300: 107.7625503540039\n",
      "Loss at step 400: 98.7890396118164\n",
      "Loss at step 500: 97.28209686279297\n",
      "Loss at step 600: 98.23121643066406\n",
      "Loss at step 700: 99.11793518066406\n",
      "Loss at step 800: 100.69457244873047\n",
      "Loss at step 900: 97.80675506591797\n",
      "Loss at step 1000: 97.8544692993164\n",
      "Loss at step 1100: 98.80247497558594\n",
      "Loss at step 1200: 99.79702758789062\n",
      "Epoch 99/1000, Avg Train Loss: 99.42241073040515\n",
      "Loss at step 0: 100.41116333007812\n",
      "Loss at step 100: 102.49097442626953\n",
      "Loss at step 200: 97.8735580444336\n",
      "Loss at step 300: 101.48308563232422\n",
      "Loss at step 400: 99.01860046386719\n",
      "Loss at step 500: 98.1439437866211\n",
      "Loss at step 600: 99.44956970214844\n",
      "Loss at step 700: 99.6633529663086\n",
      "Loss at step 800: 98.75601959228516\n",
      "Loss at step 900: 98.24612426757812\n",
      "Loss at step 1000: 99.46631622314453\n",
      "Loss at step 1100: 98.5885238647461\n",
      "Loss at step 1200: 98.86731719970703\n",
      "Epoch 100/1000, Avg Train Loss: 99.41820131690757\n",
      "Loss at step 0: 97.7797622680664\n",
      "Loss at step 100: 100.35932159423828\n",
      "Loss at step 200: 98.29460144042969\n",
      "Loss at step 300: 99.10276794433594\n",
      "Loss at step 400: 97.9287338256836\n",
      "Loss at step 500: 99.72962188720703\n",
      "Loss at step 600: 99.20935821533203\n",
      "Loss at step 700: 99.69047546386719\n",
      "Loss at step 800: 99.33244323730469\n",
      "Loss at step 900: 97.61801147460938\n",
      "Loss at step 1000: 103.31761932373047\n",
      "Loss at step 1100: 98.18146514892578\n",
      "Loss at step 1200: 99.09843444824219\n",
      "Epoch 101/1000, Avg Train Loss: 99.41469417806582\n",
      "Loss at step 0: 99.64875793457031\n",
      "Loss at step 100: 98.95538330078125\n",
      "Loss at step 200: 98.42219543457031\n",
      "Loss at step 300: 105.68917083740234\n",
      "Loss at step 400: 99.21625518798828\n",
      "Loss at step 500: 97.80167388916016\n",
      "Loss at step 600: 98.82886505126953\n",
      "Loss at step 700: 98.2432632446289\n",
      "Loss at step 800: 99.04621887207031\n",
      "Loss at step 900: 98.27510070800781\n",
      "Loss at step 1000: 98.17726135253906\n",
      "Loss at step 1100: 99.61685180664062\n",
      "Loss at step 1200: 98.8527603149414\n",
      "Epoch 102/1000, Avg Train Loss: 99.41242350729539\n",
      "Loss at step 0: 99.54618072509766\n",
      "Loss at step 100: 99.1638412475586\n",
      "Loss at step 200: 97.6837387084961\n",
      "Loss at step 300: 97.83808898925781\n",
      "Loss at step 400: 97.89603424072266\n",
      "Loss at step 500: 100.90965270996094\n",
      "Loss at step 600: 98.70885467529297\n",
      "Loss at step 700: 99.10157012939453\n",
      "Loss at step 800: 97.95777130126953\n",
      "Loss at step 900: 98.77344512939453\n",
      "Loss at step 1000: 100.33738708496094\n",
      "Loss at step 1100: 98.80842590332031\n",
      "Loss at step 1200: 98.88230895996094\n",
      "Epoch 103/1000, Avg Train Loss: 99.39215063200028\n",
      "Loss at step 0: 97.92353057861328\n",
      "Loss at step 100: 98.8907699584961\n",
      "Loss at step 200: 99.1782455444336\n",
      "Loss at step 300: 98.1422348022461\n",
      "Loss at step 400: 99.25731658935547\n",
      "Loss at step 500: 99.96910858154297\n",
      "Loss at step 600: 100.44701385498047\n",
      "Loss at step 700: 98.46846008300781\n",
      "Loss at step 800: 99.15071868896484\n",
      "Loss at step 900: 100.04484558105469\n",
      "Loss at step 1000: 98.50947570800781\n",
      "Loss at step 1100: 98.02775573730469\n",
      "Loss at step 1200: 98.4852523803711\n",
      "Epoch 104/1000, Avg Train Loss: 99.41286962935068\n",
      "Loss at step 0: 98.8061294555664\n",
      "Loss at step 100: 99.06617736816406\n",
      "Loss at step 200: 98.02459716796875\n",
      "Loss at step 300: 98.92251586914062\n",
      "Loss at step 400: 98.8129653930664\n",
      "Loss at step 500: 98.06358337402344\n",
      "Loss at step 600: 101.73077392578125\n",
      "Loss at step 700: 98.74288940429688\n",
      "Loss at step 800: 99.65371704101562\n",
      "Loss at step 900: 97.20635986328125\n",
      "Loss at step 1000: 99.388671875\n",
      "Loss at step 1100: 98.2457046508789\n",
      "Loss at step 1200: 99.52484130859375\n",
      "Epoch 105/1000, Avg Train Loss: 99.44595870230962\n",
      "Loss at step 0: 99.15377044677734\n",
      "Loss at step 100: 98.52643585205078\n",
      "Loss at step 200: 99.58118438720703\n",
      "Loss at step 300: 98.45532989501953\n",
      "Loss at step 400: 108.03368377685547\n",
      "Loss at step 500: 98.88195037841797\n",
      "Loss at step 600: 97.380859375\n",
      "Loss at step 700: 98.21683502197266\n",
      "Loss at step 800: 102.742431640625\n",
      "Loss at step 900: 98.75326538085938\n",
      "Loss at step 1000: 99.844482421875\n",
      "Loss at step 1100: 98.66292572021484\n",
      "Loss at step 1200: 98.36080169677734\n",
      "Epoch 106/1000, Avg Train Loss: 99.42565126017846\n",
      "Loss at step 0: 98.33731842041016\n",
      "Loss at step 100: 98.318603515625\n",
      "Loss at step 200: 99.90814971923828\n",
      "Loss at step 300: 98.57699584960938\n",
      "Loss at step 400: 98.58023834228516\n",
      "Loss at step 500: 97.64844512939453\n",
      "Loss at step 600: 99.38794708251953\n",
      "Loss at step 700: 99.51881408691406\n",
      "Loss at step 800: 98.6750717163086\n",
      "Loss at step 900: 99.46009826660156\n",
      "Loss at step 1000: 98.71588897705078\n",
      "Loss at step 1100: 97.92549133300781\n",
      "Loss at step 1200: 99.21342468261719\n",
      "Epoch 107/1000, Avg Train Loss: 99.4096697588183\n",
      "Loss at step 0: 101.60537719726562\n",
      "Loss at step 100: 98.7362060546875\n",
      "Loss at step 200: 98.45018005371094\n",
      "Loss at step 300: 98.33819580078125\n",
      "Loss at step 400: 98.87550354003906\n",
      "Loss at step 500: 99.60202026367188\n",
      "Loss at step 600: 98.54444122314453\n",
      "Loss at step 700: 108.93643188476562\n",
      "Loss at step 800: 97.55583953857422\n",
      "Loss at step 900: 96.8573226928711\n",
      "Loss at step 1000: 99.2791976928711\n",
      "Loss at step 1100: 100.64090728759766\n",
      "Loss at step 1200: 98.84004211425781\n",
      "Epoch 108/1000, Avg Train Loss: 99.44437293018724\n",
      "Loss at step 0: 99.40818786621094\n",
      "Loss at step 100: 99.180419921875\n",
      "Loss at step 200: 99.21179962158203\n",
      "Loss at step 300: 98.77649688720703\n",
      "Loss at step 400: 98.6247787475586\n",
      "Loss at step 500: 104.75839233398438\n",
      "Loss at step 600: 99.92298889160156\n",
      "Loss at step 700: 99.89863586425781\n",
      "Loss at step 800: 97.92310333251953\n",
      "Loss at step 900: 109.69145202636719\n",
      "Loss at step 1000: 98.72782135009766\n",
      "Loss at step 1100: 100.14612579345703\n",
      "Loss at step 1200: 97.65841674804688\n",
      "Epoch 109/1000, Avg Train Loss: 99.46294373447455\n",
      "Loss at step 0: 97.67537689208984\n",
      "Loss at step 100: 97.45960235595703\n",
      "Loss at step 200: 99.87945556640625\n",
      "Loss at step 300: 103.18280792236328\n",
      "Loss at step 400: 98.64219665527344\n",
      "Loss at step 500: 98.39649200439453\n",
      "Loss at step 600: 98.42426300048828\n",
      "Loss at step 700: 99.03146362304688\n",
      "Loss at step 800: 99.56369018554688\n",
      "Loss at step 900: 99.45166778564453\n",
      "Loss at step 1000: 100.87873077392578\n",
      "Loss at step 1100: 100.04975891113281\n",
      "Loss at step 1200: 97.2364273071289\n",
      "Epoch 110/1000, Avg Train Loss: 99.41905314868322\n",
      "Loss at step 0: 102.30591583251953\n",
      "Loss at step 100: 98.40979766845703\n",
      "Loss at step 200: 98.605224609375\n",
      "Loss at step 300: 100.00798797607422\n",
      "Loss at step 400: 98.60282135009766\n",
      "Loss at step 500: 100.11317443847656\n",
      "Loss at step 600: 97.79180908203125\n",
      "Loss at step 700: 98.74436950683594\n",
      "Loss at step 800: 99.71562957763672\n",
      "Loss at step 900: 98.41018676757812\n",
      "Loss at step 1000: 98.6612319946289\n",
      "Loss at step 1100: 99.61200714111328\n",
      "Loss at step 1200: 99.35508728027344\n",
      "Epoch 111/1000, Avg Train Loss: 99.42735512742719\n",
      "Loss at step 0: 101.52218627929688\n",
      "Loss at step 100: 100.12576293945312\n",
      "Loss at step 200: 98.77111053466797\n",
      "Loss at step 300: 97.8195571899414\n",
      "Loss at step 400: 104.33784484863281\n",
      "Loss at step 500: 99.62494659423828\n",
      "Loss at step 600: 99.67633819580078\n",
      "Loss at step 700: 100.45405578613281\n",
      "Loss at step 800: 101.36141967773438\n",
      "Loss at step 900: 99.01971435546875\n",
      "Loss at step 1000: 98.3555908203125\n",
      "Loss at step 1100: 99.95536041259766\n",
      "Loss at step 1200: 99.31184387207031\n",
      "Epoch 112/1000, Avg Train Loss: 99.40112889854653\n",
      "Loss at step 0: 98.88699340820312\n",
      "Loss at step 100: 98.51012420654297\n",
      "Loss at step 200: 99.0301284790039\n",
      "Loss at step 300: 98.0047607421875\n",
      "Loss at step 400: 98.55459594726562\n",
      "Loss at step 500: 98.51835632324219\n",
      "Loss at step 600: 100.5950698852539\n",
      "Loss at step 700: 98.85458374023438\n",
      "Loss at step 800: 99.51325988769531\n",
      "Loss at step 900: 98.10813903808594\n",
      "Loss at step 1000: 98.6939697265625\n",
      "Loss at step 1100: 98.43424224853516\n",
      "Loss at step 1200: 98.71351623535156\n",
      "Epoch 113/1000, Avg Train Loss: 99.41428589435071\n",
      "Loss at step 0: 99.34903717041016\n",
      "Loss at step 100: 99.74747467041016\n",
      "Loss at step 200: 100.61006164550781\n",
      "Loss at step 300: 99.67381286621094\n",
      "Loss at step 400: 98.52522277832031\n",
      "Loss at step 500: 98.77690887451172\n",
      "Loss at step 600: 98.60395050048828\n",
      "Loss at step 700: 97.99683380126953\n",
      "Loss at step 800: 98.50337219238281\n",
      "Loss at step 900: 98.92909240722656\n",
      "Loss at step 1000: 100.83673858642578\n",
      "Loss at step 1100: 97.48931884765625\n",
      "Loss at step 1200: 99.14842224121094\n",
      "Epoch 114/1000, Avg Train Loss: 99.41258303324382\n",
      "Loss at step 0: 100.09307861328125\n",
      "Loss at step 100: 99.44217681884766\n",
      "Loss at step 200: 97.87055206298828\n",
      "Loss at step 300: 97.84048461914062\n",
      "Loss at step 400: 99.33607482910156\n",
      "Loss at step 500: 98.84050750732422\n",
      "Loss at step 600: 97.04937744140625\n",
      "Loss at step 700: 137.0098419189453\n",
      "Loss at step 800: 98.83528137207031\n",
      "Loss at step 900: 98.4121322631836\n",
      "Loss at step 1000: 97.25912475585938\n",
      "Loss at step 1100: 99.09040069580078\n",
      "Loss at step 1200: 98.89424133300781\n",
      "Epoch 115/1000, Avg Train Loss: 99.42357507724206\n",
      "Loss at step 0: 97.62249755859375\n",
      "Loss at step 100: 98.62702941894531\n",
      "Loss at step 200: 99.24502563476562\n",
      "Loss at step 300: 97.41724395751953\n",
      "Loss at step 400: 99.6837387084961\n",
      "Loss at step 500: 99.13060760498047\n",
      "Loss at step 600: 100.90253448486328\n",
      "Loss at step 700: 98.76806640625\n",
      "Loss at step 800: 98.32299041748047\n",
      "Loss at step 900: 97.97684478759766\n",
      "Loss at step 1000: 102.7766342163086\n",
      "Loss at step 1100: 99.16200256347656\n",
      "Loss at step 1200: 98.6450424194336\n",
      "Epoch 116/1000, Avg Train Loss: 99.45930433427631\n",
      "Loss at step 0: 98.06877136230469\n",
      "Loss at step 100: 98.33323669433594\n",
      "Loss at step 200: 98.44277954101562\n",
      "Loss at step 300: 98.23931121826172\n",
      "Loss at step 400: 97.8031997680664\n",
      "Loss at step 500: 99.124755859375\n",
      "Loss at step 600: 97.61742401123047\n",
      "Loss at step 700: 99.38825988769531\n",
      "Loss at step 800: 98.28901672363281\n",
      "Loss at step 900: 99.34469604492188\n",
      "Loss at step 1000: 102.50171661376953\n",
      "Loss at step 1100: 99.12767028808594\n",
      "Loss at step 1200: 98.87106323242188\n",
      "Epoch 117/1000, Avg Train Loss: 99.37889632437994\n",
      "Loss at step 0: 99.87733459472656\n",
      "Loss at step 100: 98.73164367675781\n",
      "Loss at step 200: 99.0948715209961\n",
      "Loss at step 300: 98.4993896484375\n",
      "Loss at step 400: 98.83975219726562\n",
      "Loss at step 500: 98.83355712890625\n",
      "Loss at step 600: 97.78376007080078\n",
      "Loss at step 700: 105.47432708740234\n",
      "Loss at step 800: 99.02986145019531\n",
      "Loss at step 900: 98.91300201416016\n",
      "Loss at step 1000: 97.42681121826172\n",
      "Loss at step 1100: 98.96710205078125\n",
      "Loss at step 1200: 98.75333404541016\n",
      "Epoch 118/1000, Avg Train Loss: 99.43435215255589\n",
      "Loss at step 0: 97.59608459472656\n",
      "Loss at step 100: 101.06076049804688\n",
      "Loss at step 200: 98.0951919555664\n",
      "Loss at step 300: 97.97879791259766\n",
      "Loss at step 400: 101.32161712646484\n",
      "Loss at step 500: 98.05427551269531\n",
      "Loss at step 600: 98.56202697753906\n",
      "Loss at step 700: 100.01763153076172\n",
      "Loss at step 800: 105.985107421875\n",
      "Loss at step 900: 98.45519256591797\n",
      "Loss at step 1000: 101.29356384277344\n",
      "Loss at step 1100: 101.041748046875\n",
      "Loss at step 1200: 97.81774139404297\n",
      "Epoch 119/1000, Avg Train Loss: 99.4160169521196\n",
      "Loss at step 0: 99.16238403320312\n",
      "Loss at step 100: 98.99588012695312\n",
      "Loss at step 200: 98.63579559326172\n",
      "Loss at step 300: 97.52141571044922\n",
      "Loss at step 400: 98.41802978515625\n",
      "Loss at step 500: 97.91475677490234\n",
      "Loss at step 600: 99.22833251953125\n",
      "Loss at step 700: 100.24855041503906\n",
      "Loss at step 800: 97.8786392211914\n",
      "Loss at step 900: 98.9863052368164\n",
      "Loss at step 1000: 98.04151153564453\n",
      "Loss at step 1100: 99.20513916015625\n",
      "Loss at step 1200: 98.86997985839844\n",
      "Epoch 120/1000, Avg Train Loss: 99.43665895184267\n",
      "Loss at step 0: 100.7973403930664\n",
      "Loss at step 100: 97.47984313964844\n",
      "Loss at step 200: 99.50192260742188\n",
      "Loss at step 300: 98.93987274169922\n",
      "Loss at step 400: 101.3482437133789\n",
      "Loss at step 500: 98.55306243896484\n",
      "Loss at step 600: 99.51541137695312\n",
      "Loss at step 700: 98.6053466796875\n",
      "Loss at step 800: 98.67053985595703\n",
      "Loss at step 900: 98.09162902832031\n",
      "Loss at step 1000: 98.8974838256836\n",
      "Loss at step 1100: 98.50687408447266\n",
      "Loss at step 1200: 98.4166488647461\n",
      "Epoch 121/1000, Avg Train Loss: 99.43513215321167\n",
      "Loss at step 0: 98.80175018310547\n",
      "Loss at step 100: 98.82746887207031\n",
      "Loss at step 200: 97.70599365234375\n",
      "Loss at step 300: 99.03848266601562\n",
      "Loss at step 400: 100.25562286376953\n",
      "Loss at step 500: 100.2710189819336\n",
      "Loss at step 600: 105.30401611328125\n",
      "Loss at step 700: 98.6146469116211\n",
      "Loss at step 800: 108.64210510253906\n",
      "Loss at step 900: 111.93500518798828\n",
      "Loss at step 1000: 100.21224975585938\n",
      "Loss at step 1100: 102.23596954345703\n",
      "Loss at step 1200: 98.89864349365234\n",
      "Epoch 122/1000, Avg Train Loss: 99.43821826799017\n",
      "Loss at step 0: 98.35289001464844\n",
      "Loss at step 100: 98.3927993774414\n",
      "Loss at step 200: 98.44447326660156\n",
      "Loss at step 300: 98.93568420410156\n",
      "Loss at step 400: 98.50376892089844\n",
      "Loss at step 500: 99.24362182617188\n",
      "Loss at step 600: 98.35438537597656\n",
      "Loss at step 700: 97.98931884765625\n",
      "Loss at step 800: 98.98380279541016\n",
      "Loss at step 900: 99.60330200195312\n",
      "Loss at step 1000: 99.2691650390625\n",
      "Loss at step 1100: 97.84932708740234\n",
      "Loss at step 1200: 113.50686645507812\n",
      "Epoch 123/1000, Avg Train Loss: 99.40939807583213\n",
      "Loss at step 0: 98.08246612548828\n",
      "Loss at step 100: 98.72830200195312\n",
      "Loss at step 200: 98.03681945800781\n",
      "Loss at step 300: 99.0794448852539\n",
      "Loss at step 400: 99.38385772705078\n",
      "Loss at step 500: 97.65241241455078\n",
      "Loss at step 600: 99.29150390625\n",
      "Loss at step 700: 98.9045639038086\n",
      "Loss at step 800: 98.78541564941406\n",
      "Loss at step 900: 110.10623931884766\n",
      "Loss at step 1000: 96.64881134033203\n",
      "Loss at step 1100: 99.06279754638672\n",
      "Loss at step 1200: 98.70601654052734\n",
      "Epoch 124/1000, Avg Train Loss: 99.42110556691982\n",
      "Loss at step 0: 97.73263549804688\n",
      "Loss at step 100: 98.51121520996094\n",
      "Loss at step 200: 97.50555419921875\n",
      "Loss at step 300: 98.48558044433594\n",
      "Loss at step 400: 99.87137603759766\n",
      "Loss at step 500: 98.58735656738281\n",
      "Loss at step 600: 99.0041732788086\n",
      "Loss at step 700: 98.68486785888672\n",
      "Loss at step 800: 99.4059829711914\n",
      "Loss at step 900: 98.68312072753906\n",
      "Loss at step 1000: 99.20541381835938\n",
      "Loss at step 1100: 98.7533950805664\n",
      "Loss at step 1200: 98.09075927734375\n",
      "Epoch 125/1000, Avg Train Loss: 99.4178493215814\n",
      "Loss at step 0: 98.15074157714844\n",
      "Loss at step 100: 99.66110229492188\n",
      "Loss at step 200: 98.75493621826172\n",
      "Loss at step 300: 99.52336883544922\n",
      "Loss at step 400: 99.87140655517578\n",
      "Loss at step 500: 100.22099304199219\n",
      "Loss at step 600: 98.56416320800781\n",
      "Loss at step 700: 99.1394271850586\n",
      "Loss at step 800: 99.16563415527344\n",
      "Loss at step 900: 98.50192260742188\n",
      "Loss at step 1000: 98.05479431152344\n",
      "Loss at step 1100: 97.47877502441406\n",
      "Loss at step 1200: 97.71012878417969\n",
      "Epoch 126/1000, Avg Train Loss: 99.43307302530529\n",
      "Loss at step 0: 98.29908752441406\n",
      "Loss at step 100: 99.33085632324219\n",
      "Loss at step 200: 98.61482238769531\n",
      "Loss at step 300: 97.4586410522461\n",
      "Loss at step 400: 98.6185531616211\n",
      "Loss at step 500: 98.09507751464844\n",
      "Loss at step 600: 98.35549926757812\n",
      "Loss at step 700: 98.59173583984375\n",
      "Loss at step 800: 102.98605346679688\n",
      "Loss at step 900: 98.73269653320312\n",
      "Loss at step 1000: 98.46328735351562\n",
      "Loss at step 1100: 98.87482452392578\n",
      "Loss at step 1200: 98.54051208496094\n",
      "Epoch 127/1000, Avg Train Loss: 99.41710694939573\n",
      "Loss at step 0: 100.1050796508789\n",
      "Loss at step 100: 97.1619873046875\n",
      "Loss at step 200: 99.217041015625\n",
      "Loss at step 300: 97.9894790649414\n",
      "Loss at step 400: 108.68025207519531\n",
      "Loss at step 500: 98.18620300292969\n",
      "Loss at step 600: 99.37258911132812\n",
      "Loss at step 700: 101.90916442871094\n",
      "Loss at step 800: 114.52738952636719\n",
      "Loss at step 900: 98.3287124633789\n",
      "Loss at step 1000: 98.87850952148438\n",
      "Loss at step 1100: 101.61177062988281\n",
      "Loss at step 1200: 98.04158020019531\n",
      "Epoch 128/1000, Avg Train Loss: 99.42071214077157\n",
      "Loss at step 0: 98.13653564453125\n",
      "Loss at step 100: 98.56707763671875\n",
      "Loss at step 200: 98.00985717773438\n",
      "Loss at step 300: 98.94037628173828\n",
      "Loss at step 400: 100.32793426513672\n",
      "Loss at step 500: 98.43955993652344\n",
      "Loss at step 600: 98.59046936035156\n",
      "Loss at step 700: 99.4165267944336\n",
      "Loss at step 800: 98.66476440429688\n",
      "Loss at step 900: 99.56118774414062\n",
      "Loss at step 1000: 99.73712158203125\n",
      "Loss at step 1100: 97.91929626464844\n",
      "Loss at step 1200: 101.31780242919922\n",
      "Epoch 129/1000, Avg Train Loss: 99.45653351225128\n",
      "Loss at step 0: 100.61566925048828\n",
      "Loss at step 100: 98.41667175292969\n",
      "Loss at step 200: 104.38675689697266\n",
      "Loss at step 300: 98.05823516845703\n",
      "Loss at step 400: 98.78910827636719\n",
      "Loss at step 500: 98.73310089111328\n",
      "Loss at step 600: 99.85360717773438\n",
      "Loss at step 700: 99.8243408203125\n",
      "Loss at step 800: 97.7467269897461\n",
      "Loss at step 900: 98.00641632080078\n",
      "Loss at step 1000: 98.5578384399414\n",
      "Loss at step 1100: 101.73200225830078\n",
      "Loss at step 1200: 98.51776885986328\n",
      "Epoch 130/1000, Avg Train Loss: 99.38819185892741\n",
      "Loss at step 0: 99.7903823852539\n",
      "Loss at step 100: 98.95677947998047\n",
      "Loss at step 200: 98.35913848876953\n",
      "Loss at step 300: 98.30062866210938\n",
      "Loss at step 400: 99.05057525634766\n",
      "Loss at step 500: 98.93101501464844\n",
      "Loss at step 600: 97.812255859375\n",
      "Loss at step 700: 98.88809204101562\n",
      "Loss at step 800: 98.5320816040039\n",
      "Loss at step 900: 97.89791107177734\n",
      "Loss at step 1000: 97.86402893066406\n",
      "Loss at step 1100: 99.6201400756836\n",
      "Loss at step 1200: 99.32379913330078\n",
      "Epoch 131/1000, Avg Train Loss: 99.46989655726165\n",
      "Loss at step 0: 97.50770568847656\n",
      "Loss at step 100: 100.50934600830078\n",
      "Loss at step 200: 98.19303131103516\n",
      "Loss at step 300: 98.02978515625\n",
      "Loss at step 400: 106.86331176757812\n",
      "Loss at step 500: 99.0023193359375\n",
      "Loss at step 600: 98.99092102050781\n",
      "Loss at step 700: 98.32901000976562\n",
      "Loss at step 800: 98.90260314941406\n",
      "Loss at step 900: 98.0112533569336\n",
      "Loss at step 1000: 97.9206314086914\n",
      "Loss at step 1100: 99.31535339355469\n",
      "Loss at step 1200: 99.19081115722656\n",
      "Epoch 132/1000, Avg Train Loss: 99.3862515051388\n",
      "Loss at step 0: 100.88445281982422\n",
      "Loss at step 100: 97.8799819946289\n",
      "Loss at step 200: 98.11882781982422\n",
      "Loss at step 300: 98.67399597167969\n",
      "Loss at step 400: 98.3357925415039\n",
      "Loss at step 500: 97.83097076416016\n",
      "Loss at step 600: 98.02027893066406\n",
      "Loss at step 700: 100.04988861083984\n",
      "Loss at step 800: 98.294677734375\n",
      "Loss at step 900: 98.63056945800781\n",
      "Loss at step 1000: 99.93289947509766\n",
      "Loss at step 1100: 97.6765365600586\n",
      "Loss at step 1200: 97.54801177978516\n",
      "Epoch 133/1000, Avg Train Loss: 99.40110347957673\n",
      "Loss at step 0: 98.65799713134766\n",
      "Loss at step 100: 99.46495056152344\n",
      "Loss at step 200: 98.68146514892578\n",
      "Loss at step 300: 99.17955017089844\n",
      "Loss at step 400: 99.6843032836914\n",
      "Loss at step 500: 102.4291763305664\n",
      "Loss at step 600: 100.28466796875\n",
      "Loss at step 700: 100.25556182861328\n",
      "Loss at step 800: 97.60416412353516\n",
      "Loss at step 900: 98.98827362060547\n",
      "Loss at step 1000: 99.00658416748047\n",
      "Loss at step 1100: 99.51302337646484\n",
      "Loss at step 1200: 103.28742218017578\n",
      "Epoch 134/1000, Avg Train Loss: 99.40143311756714\n",
      "Loss at step 0: 99.17463684082031\n",
      "Loss at step 100: 98.39490509033203\n",
      "Loss at step 200: 98.72602081298828\n",
      "Loss at step 300: 98.44587707519531\n",
      "Loss at step 400: 98.13227081298828\n",
      "Loss at step 500: 97.69442749023438\n",
      "Loss at step 600: 98.12993621826172\n",
      "Loss at step 700: 98.3531494140625\n",
      "Loss at step 800: 97.6491928100586\n",
      "Loss at step 900: 98.16055297851562\n",
      "Loss at step 1000: 100.18199920654297\n",
      "Loss at step 1100: 98.90294647216797\n",
      "Loss at step 1200: 99.7620620727539\n",
      "Epoch 135/1000, Avg Train Loss: 99.42206583362567\n",
      "Loss at step 0: 100.09716796875\n",
      "Loss at step 100: 97.97035217285156\n",
      "Loss at step 200: 99.3158950805664\n",
      "Loss at step 300: 98.42867279052734\n",
      "Loss at step 400: 98.52659606933594\n",
      "Loss at step 500: 98.08818817138672\n",
      "Loss at step 600: 97.44441223144531\n",
      "Loss at step 700: 98.98450469970703\n",
      "Loss at step 800: 97.95625305175781\n",
      "Loss at step 900: 97.61664581298828\n",
      "Loss at step 1000: 99.08729553222656\n",
      "Loss at step 1100: 98.49225616455078\n",
      "Loss at step 1200: 98.80816650390625\n",
      "Epoch 136/1000, Avg Train Loss: 99.39801461796931\n",
      "Loss at step 0: 98.3363037109375\n",
      "Loss at step 100: 97.7509765625\n",
      "Loss at step 200: 98.52642822265625\n",
      "Loss at step 300: 97.57190704345703\n",
      "Loss at step 400: 98.19120788574219\n",
      "Loss at step 500: 98.16993713378906\n",
      "Loss at step 600: 103.12124633789062\n",
      "Loss at step 700: 98.58592987060547\n",
      "Loss at step 800: 98.52296447753906\n",
      "Loss at step 900: 99.99181365966797\n",
      "Loss at step 1000: 98.24022674560547\n",
      "Loss at step 1100: 98.04737854003906\n",
      "Loss at step 1200: 98.56310272216797\n",
      "Epoch 137/1000, Avg Train Loss: 99.39079085908661\n",
      "Loss at step 0: 98.34777069091797\n",
      "Loss at step 100: 97.75895690917969\n",
      "Loss at step 200: 102.57032775878906\n",
      "Loss at step 300: 99.432861328125\n",
      "Loss at step 400: 99.18833923339844\n",
      "Loss at step 500: 104.66302490234375\n",
      "Loss at step 600: 103.37994384765625\n",
      "Loss at step 700: 100.69834899902344\n",
      "Loss at step 800: 99.25534057617188\n",
      "Loss at step 900: 98.5399398803711\n",
      "Loss at step 1000: 98.73651123046875\n",
      "Loss at step 1100: 98.31534576416016\n",
      "Loss at step 1200: 99.60197448730469\n",
      "Epoch 138/1000, Avg Train Loss: 99.41671561577559\n",
      "Loss at step 0: 98.66143798828125\n",
      "Loss at step 100: 102.61908721923828\n",
      "Loss at step 200: 101.62084197998047\n",
      "Loss at step 300: 98.55256652832031\n",
      "Loss at step 400: 98.47113037109375\n",
      "Loss at step 500: 98.71775817871094\n",
      "Loss at step 600: 97.93994140625\n",
      "Loss at step 700: 100.2578125\n",
      "Loss at step 800: 101.10598754882812\n",
      "Loss at step 900: 97.91738891601562\n",
      "Loss at step 1000: 101.3487548828125\n",
      "Loss at step 1100: 99.21692657470703\n",
      "Loss at step 1200: 98.41915130615234\n",
      "Epoch 139/1000, Avg Train Loss: 99.40987670923128\n",
      "Loss at step 0: 100.81218719482422\n",
      "Loss at step 100: 98.13975524902344\n",
      "Loss at step 200: 97.72696685791016\n",
      "Loss at step 300: 98.80097198486328\n",
      "Loss at step 400: 106.7841796875\n",
      "Loss at step 500: 98.84017944335938\n",
      "Loss at step 600: 101.07343292236328\n",
      "Loss at step 700: 98.96977233886719\n",
      "Loss at step 800: 97.66295623779297\n",
      "Loss at step 900: 98.2920913696289\n",
      "Loss at step 1000: 100.30998992919922\n",
      "Loss at step 1100: 98.35233306884766\n",
      "Loss at step 1200: 98.61932373046875\n",
      "Epoch 140/1000, Avg Train Loss: 99.45857896773947\n",
      "Loss at step 0: 100.24070739746094\n",
      "Loss at step 100: 97.52752685546875\n",
      "Loss at step 200: 105.35897827148438\n",
      "Loss at step 300: 97.84162139892578\n",
      "Loss at step 400: 97.40158081054688\n",
      "Loss at step 500: 98.40885162353516\n",
      "Loss at step 600: 98.71372985839844\n",
      "Loss at step 700: 98.30180358886719\n",
      "Loss at step 800: 100.26240539550781\n",
      "Loss at step 900: 100.97261810302734\n",
      "Loss at step 1000: 97.86536407470703\n",
      "Loss at step 1100: 99.2931900024414\n",
      "Loss at step 1200: 102.30494689941406\n",
      "Epoch 141/1000, Avg Train Loss: 99.39294987280392\n",
      "Loss at step 0: 99.68495178222656\n",
      "Loss at step 100: 98.55313110351562\n",
      "Loss at step 200: 97.8680419921875\n",
      "Loss at step 300: 99.2166976928711\n",
      "Loss at step 400: 99.46451568603516\n",
      "Loss at step 500: 98.13811492919922\n",
      "Loss at step 600: 98.3798599243164\n",
      "Loss at step 700: 97.61793518066406\n",
      "Loss at step 800: 97.56202697753906\n",
      "Loss at step 900: 97.99874114990234\n",
      "Loss at step 1000: 98.41522216796875\n",
      "Loss at step 1100: 98.11650848388672\n",
      "Loss at step 1200: 98.24748992919922\n",
      "Epoch 142/1000, Avg Train Loss: 99.42774211550221\n",
      "Loss at step 0: 98.15607452392578\n",
      "Loss at step 100: 98.73873138427734\n",
      "Loss at step 200: 99.5768814086914\n",
      "Loss at step 300: 102.08566284179688\n",
      "Loss at step 400: 98.88863372802734\n",
      "Loss at step 500: 99.16691589355469\n",
      "Loss at step 600: 99.20624542236328\n",
      "Loss at step 700: 97.70935821533203\n",
      "Loss at step 800: 99.22563171386719\n",
      "Loss at step 900: 99.5641860961914\n",
      "Loss at step 1000: 102.05233001708984\n",
      "Loss at step 1100: 97.66161346435547\n",
      "Loss at step 1200: 99.05001831054688\n",
      "Epoch 143/1000, Avg Train Loss: 99.42692349875243\n",
      "Loss at step 0: 99.33723449707031\n",
      "Loss at step 100: 98.249267578125\n",
      "Loss at step 200: 98.63522338867188\n",
      "Loss at step 300: 98.39199829101562\n",
      "Loss at step 400: 98.00003051757812\n",
      "Loss at step 500: 98.38763427734375\n",
      "Loss at step 600: 98.60968017578125\n",
      "Loss at step 700: 100.44182586669922\n",
      "Loss at step 800: 99.07679748535156\n",
      "Loss at step 900: 99.07505798339844\n",
      "Loss at step 1000: 98.43942260742188\n",
      "Loss at step 1100: 98.88890075683594\n",
      "Loss at step 1200: 98.34416198730469\n",
      "Epoch 144/1000, Avg Train Loss: 99.4221507445894\n",
      "Loss at step 0: 98.52569580078125\n",
      "Loss at step 100: 100.53022003173828\n",
      "Loss at step 200: 98.36573791503906\n",
      "Loss at step 300: 98.35718536376953\n",
      "Loss at step 400: 99.68946838378906\n",
      "Loss at step 500: 97.94466400146484\n",
      "Loss at step 600: 99.017822265625\n",
      "Loss at step 700: 99.29766845703125\n",
      "Loss at step 800: 97.60074615478516\n",
      "Loss at step 900: 99.22301483154297\n",
      "Loss at step 1000: 110.79264831542969\n",
      "Loss at step 1100: 100.35336303710938\n",
      "Loss at step 1200: 98.3280029296875\n",
      "Epoch 145/1000, Avg Train Loss: 99.42378496583612\n",
      "Loss at step 0: 100.98918914794922\n",
      "Loss at step 100: 98.00585174560547\n",
      "Loss at step 200: 99.604248046875\n",
      "Loss at step 300: 98.62613677978516\n",
      "Loss at step 400: 98.22364807128906\n",
      "Loss at step 500: 100.66706848144531\n",
      "Loss at step 600: 99.7496566772461\n",
      "Loss at step 700: 103.20439147949219\n",
      "Loss at step 800: 98.23877716064453\n",
      "Loss at step 900: 98.13966369628906\n",
      "Loss at step 1000: 98.13931274414062\n",
      "Loss at step 1100: 99.06222534179688\n",
      "Loss at step 1200: 99.75841522216797\n",
      "Epoch 146/1000, Avg Train Loss: 99.42155520738521\n",
      "Loss at step 0: 98.2728271484375\n",
      "Loss at step 100: 98.46571350097656\n",
      "Loss at step 200: 100.8844985961914\n",
      "Loss at step 300: 99.47136688232422\n",
      "Loss at step 400: 98.07177734375\n",
      "Loss at step 500: 99.20957946777344\n",
      "Loss at step 600: 97.87720489501953\n",
      "Loss at step 700: 101.42681884765625\n",
      "Loss at step 800: 98.83079528808594\n",
      "Loss at step 900: 97.25531005859375\n",
      "Loss at step 1000: 97.6715087890625\n",
      "Loss at step 1100: 97.11998748779297\n",
      "Loss at step 1200: 100.06451416015625\n",
      "Epoch 147/1000, Avg Train Loss: 99.3793896239938\n",
      "Loss at step 0: 97.53690338134766\n",
      "Loss at step 100: 98.7317123413086\n",
      "Loss at step 200: 99.0434799194336\n",
      "Loss at step 300: 98.88795471191406\n",
      "Loss at step 400: 98.25740814208984\n",
      "Loss at step 500: 98.97151184082031\n",
      "Loss at step 600: 97.79312896728516\n",
      "Loss at step 700: 99.60759735107422\n",
      "Loss at step 800: 98.9141845703125\n",
      "Loss at step 900: 100.16751098632812\n",
      "Loss at step 1000: 101.08515167236328\n",
      "Loss at step 1100: 98.71576690673828\n",
      "Loss at step 1200: 97.71183776855469\n",
      "Epoch 148/1000, Avg Train Loss: 99.4038713881113\n",
      "Loss at step 0: 97.89549255371094\n",
      "Loss at step 100: 105.5953598022461\n",
      "Loss at step 200: 99.4113998413086\n",
      "Loss at step 300: 101.6795425415039\n",
      "Loss at step 400: 99.32715606689453\n",
      "Loss at step 500: 99.59977722167969\n",
      "Loss at step 600: 99.6052474975586\n",
      "Loss at step 700: 98.12399291992188\n",
      "Loss at step 800: 99.83134460449219\n",
      "Loss at step 900: 98.94002532958984\n",
      "Loss at step 1000: 98.23078918457031\n",
      "Loss at step 1100: 98.23565673828125\n",
      "Loss at step 1200: 98.89627838134766\n",
      "Epoch 149/1000, Avg Train Loss: 99.40059981608468\n",
      "Loss at step 0: 98.58473205566406\n",
      "Loss at step 100: 99.28767395019531\n",
      "Loss at step 200: 98.10579681396484\n",
      "Loss at step 300: 100.80115509033203\n",
      "Loss at step 400: 98.58406829833984\n",
      "Loss at step 500: 97.89531707763672\n",
      "Loss at step 600: 102.29377746582031\n",
      "Loss at step 700: 100.2787857055664\n",
      "Loss at step 800: 98.71045684814453\n",
      "Loss at step 900: 98.81633758544922\n",
      "Loss at step 1000: 98.4577865600586\n",
      "Loss at step 1100: 98.26322174072266\n",
      "Loss at step 1200: 98.07001495361328\n",
      "Epoch 150/1000, Avg Train Loss: 99.41210558499333\n",
      "Loss at step 0: 97.75789642333984\n",
      "Loss at step 100: 98.53926849365234\n",
      "Loss at step 200: 100.76582336425781\n",
      "Loss at step 300: 99.64094543457031\n",
      "Loss at step 400: 100.1175308227539\n",
      "Loss at step 500: 99.20137786865234\n",
      "Loss at step 600: 99.35382080078125\n",
      "Loss at step 700: 98.32823944091797\n",
      "Loss at step 800: 98.4767837524414\n",
      "Loss at step 900: 97.57237243652344\n",
      "Loss at step 1000: 98.7789077758789\n",
      "Loss at step 1100: 99.07464599609375\n",
      "Loss at step 1200: 97.39533233642578\n",
      "Epoch 151/1000, Avg Train Loss: 99.42228412319541\n",
      "Loss at step 0: 98.07647705078125\n",
      "Loss at step 100: 97.05619049072266\n",
      "Loss at step 200: 99.47811126708984\n",
      "Loss at step 300: 99.20703125\n",
      "Loss at step 400: 100.00843811035156\n",
      "Loss at step 500: 98.52945709228516\n",
      "Loss at step 600: 112.87055206298828\n",
      "Loss at step 700: 99.04887390136719\n",
      "Loss at step 800: 99.26647186279297\n",
      "Loss at step 900: 97.24012756347656\n",
      "Loss at step 1000: 98.4921875\n",
      "Loss at step 1100: 99.85047912597656\n",
      "Loss at step 1200: 98.7643814086914\n",
      "Epoch 152/1000, Avg Train Loss: 99.41661449543481\n",
      "Loss at step 0: 98.53185272216797\n",
      "Loss at step 100: 98.793701171875\n",
      "Loss at step 200: 99.5838851928711\n",
      "Loss at step 300: 98.0404281616211\n",
      "Loss at step 400: 98.56253051757812\n",
      "Loss at step 500: 99.11051177978516\n",
      "Loss at step 600: 111.5565414428711\n",
      "Loss at step 700: 98.84661102294922\n",
      "Loss at step 800: 99.2354965209961\n",
      "Loss at step 900: 99.520751953125\n",
      "Loss at step 1000: 99.49039459228516\n",
      "Loss at step 1100: 98.13508605957031\n",
      "Loss at step 1200: 98.65511322021484\n",
      "Epoch 153/1000, Avg Train Loss: 99.38223966579993\n",
      "Loss at step 0: 98.1593017578125\n",
      "Loss at step 100: 99.01410675048828\n",
      "Loss at step 200: 97.82991027832031\n",
      "Loss at step 300: 99.41963958740234\n",
      "Loss at step 400: 105.61976623535156\n",
      "Loss at step 500: 100.60437774658203\n",
      "Loss at step 600: 104.25469970703125\n",
      "Loss at step 700: 99.08992004394531\n",
      "Loss at step 800: 112.98575592041016\n",
      "Loss at step 900: 99.26683807373047\n",
      "Loss at step 1000: 100.99063873291016\n",
      "Loss at step 1100: 99.04975891113281\n",
      "Loss at step 1200: 98.49331665039062\n",
      "Epoch 154/1000, Avg Train Loss: 99.40728948031429\n",
      "Loss at step 0: 98.56192779541016\n",
      "Loss at step 100: 98.42979431152344\n",
      "Loss at step 200: 98.79019927978516\n",
      "Loss at step 300: 99.42940521240234\n",
      "Loss at step 400: 99.67372131347656\n",
      "Loss at step 500: 102.43267822265625\n",
      "Loss at step 600: 98.56929779052734\n",
      "Loss at step 700: 99.01443481445312\n",
      "Loss at step 800: 100.08702087402344\n",
      "Loss at step 900: 98.47581481933594\n",
      "Loss at step 1000: 96.67942810058594\n",
      "Loss at step 1100: 99.24758911132812\n",
      "Loss at step 1200: 98.9379653930664\n",
      "Epoch 155/1000, Avg Train Loss: 99.39453857693472\n",
      "Loss at step 0: 102.6000747680664\n",
      "Loss at step 100: 99.17425537109375\n",
      "Loss at step 200: 98.5639419555664\n",
      "Loss at step 300: 100.75577545166016\n",
      "Loss at step 400: 106.29448699951172\n",
      "Loss at step 500: 99.2509994506836\n",
      "Loss at step 600: 97.70201110839844\n",
      "Loss at step 700: 98.27427673339844\n",
      "Loss at step 800: 97.5223388671875\n",
      "Loss at step 900: 98.453857421875\n",
      "Loss at step 1000: 98.709228515625\n",
      "Loss at step 1100: 97.8601303100586\n",
      "Loss at step 1200: 98.43162536621094\n",
      "Epoch 156/1000, Avg Train Loss: 99.4404766922244\n",
      "Loss at step 0: 99.7879409790039\n",
      "Loss at step 100: 98.93599700927734\n",
      "Loss at step 200: 98.59391784667969\n",
      "Loss at step 300: 98.97044372558594\n",
      "Loss at step 400: 99.36422729492188\n",
      "Loss at step 500: 99.19441223144531\n",
      "Loss at step 600: 97.74018859863281\n",
      "Loss at step 700: 98.91197967529297\n",
      "Loss at step 800: 97.40607452392578\n",
      "Loss at step 900: 98.17549896240234\n",
      "Loss at step 1000: 98.29139709472656\n",
      "Loss at step 1100: 99.58417510986328\n",
      "Loss at step 1200: 99.51058959960938\n",
      "Epoch 157/1000, Avg Train Loss: 99.41884167989095\n",
      "Loss at step 0: 97.9437026977539\n",
      "Loss at step 100: 97.57438659667969\n",
      "Loss at step 200: 99.60320281982422\n",
      "Loss at step 300: 99.0814208984375\n",
      "Loss at step 400: 98.92112731933594\n",
      "Loss at step 500: 99.66262817382812\n",
      "Loss at step 600: 99.01237487792969\n",
      "Loss at step 700: 99.7707748413086\n",
      "Loss at step 800: 99.36393737792969\n",
      "Loss at step 900: 98.14251708984375\n",
      "Loss at step 1000: 98.7756118774414\n",
      "Loss at step 1100: 97.5721435546875\n",
      "Loss at step 1200: 98.05580139160156\n",
      "Epoch 158/1000, Avg Train Loss: 99.4048619007987\n",
      "Loss at step 0: 98.73540496826172\n",
      "Loss at step 100: 98.94002532958984\n",
      "Loss at step 200: 97.03010559082031\n",
      "Loss at step 300: 97.27284240722656\n",
      "Loss at step 400: 98.46672821044922\n",
      "Loss at step 500: 97.78553771972656\n",
      "Loss at step 600: 98.49451446533203\n",
      "Loss at step 700: 98.37133026123047\n",
      "Loss at step 800: 97.90502166748047\n",
      "Loss at step 900: 98.44091796875\n",
      "Loss at step 1000: 102.57258605957031\n",
      "Loss at step 1100: 98.90702056884766\n",
      "Loss at step 1200: 98.62103271484375\n",
      "Epoch 159/1000, Avg Train Loss: 99.41074918851884\n",
      "Loss at step 0: 98.54600524902344\n",
      "Loss at step 100: 103.39817810058594\n",
      "Loss at step 200: 97.92012023925781\n",
      "Loss at step 300: 100.05577850341797\n",
      "Loss at step 400: 98.33219909667969\n",
      "Loss at step 500: 98.52777862548828\n",
      "Loss at step 600: 98.4499282836914\n",
      "Loss at step 700: 103.34371185302734\n",
      "Loss at step 800: 98.23068237304688\n",
      "Loss at step 900: 98.78157043457031\n",
      "Loss at step 1000: 99.51387023925781\n",
      "Loss at step 1100: 98.57054138183594\n",
      "Loss at step 1200: 97.99027252197266\n",
      "Epoch 160/1000, Avg Train Loss: 99.47764741101311\n",
      "Loss at step 0: 100.2480239868164\n",
      "Loss at step 100: 98.18206787109375\n",
      "Loss at step 200: 98.83903503417969\n",
      "Loss at step 300: 98.74744415283203\n",
      "Loss at step 400: 98.8989486694336\n",
      "Loss at step 500: 97.68751525878906\n",
      "Loss at step 600: 98.00972747802734\n",
      "Loss at step 700: 99.76805114746094\n",
      "Loss at step 800: 99.10516357421875\n",
      "Loss at step 900: 98.30209350585938\n",
      "Loss at step 1000: 97.7076416015625\n",
      "Loss at step 1100: 98.37506866455078\n",
      "Loss at step 1200: 100.95529174804688\n",
      "Epoch 161/1000, Avg Train Loss: 99.42635042381904\n",
      "Loss at step 0: 98.90138244628906\n",
      "Loss at step 100: 99.17349243164062\n",
      "Loss at step 200: 97.38967895507812\n",
      "Loss at step 300: 97.72201538085938\n",
      "Loss at step 400: 99.70326232910156\n",
      "Loss at step 500: 98.66555786132812\n",
      "Loss at step 600: 100.48101806640625\n",
      "Loss at step 700: 98.95040130615234\n",
      "Loss at step 800: 99.2167739868164\n",
      "Loss at step 900: 97.99919891357422\n",
      "Loss at step 1000: 98.20986938476562\n",
      "Loss at step 1100: 98.50399780273438\n",
      "Loss at step 1200: 98.885498046875\n",
      "Epoch 162/1000, Avg Train Loss: 99.39731746043974\n",
      "Loss at step 0: 99.32960510253906\n",
      "Loss at step 100: 99.9857177734375\n",
      "Loss at step 200: 101.84480285644531\n",
      "Loss at step 300: 98.19132995605469\n",
      "Loss at step 400: 99.77456665039062\n",
      "Loss at step 500: 100.67374420166016\n",
      "Loss at step 600: 98.23699951171875\n",
      "Loss at step 700: 99.27552795410156\n",
      "Loss at step 800: 99.33287811279297\n",
      "Loss at step 900: 99.50152587890625\n",
      "Loss at step 1000: 99.90740203857422\n",
      "Loss at step 1100: 102.14131164550781\n",
      "Loss at step 1200: 97.48944091796875\n",
      "Epoch 163/1000, Avg Train Loss: 99.4264091812677\n",
      "Loss at step 0: 99.91287231445312\n",
      "Loss at step 100: 100.07294464111328\n",
      "Loss at step 200: 98.9134521484375\n",
      "Loss at step 300: 99.03271484375\n",
      "Loss at step 400: 99.35596466064453\n",
      "Loss at step 500: 99.29219055175781\n",
      "Loss at step 600: 111.56494903564453\n",
      "Loss at step 700: 98.69789123535156\n",
      "Loss at step 800: 103.64015197753906\n",
      "Loss at step 900: 100.96348571777344\n",
      "Loss at step 1000: 98.33454895019531\n",
      "Loss at step 1100: 98.5965576171875\n",
      "Loss at step 1200: 97.96974182128906\n",
      "Epoch 164/1000, Avg Train Loss: 99.41541246457393\n",
      "Loss at step 0: 99.54541015625\n",
      "Loss at step 100: 97.53568267822266\n",
      "Loss at step 200: 99.63533020019531\n",
      "Loss at step 300: 99.8488540649414\n",
      "Loss at step 400: 98.4527359008789\n",
      "Loss at step 500: 97.22969055175781\n",
      "Loss at step 600: 99.99758911132812\n",
      "Loss at step 700: 97.93949890136719\n",
      "Loss at step 800: 100.2022705078125\n",
      "Loss at step 900: 97.9722900390625\n",
      "Loss at step 1000: 97.892333984375\n",
      "Loss at step 1100: 97.77252960205078\n",
      "Loss at step 1200: 97.51116943359375\n",
      "Epoch 165/1000, Avg Train Loss: 99.3849745938693\n",
      "Loss at step 0: 99.84027099609375\n",
      "Loss at step 100: 105.81671905517578\n",
      "Loss at step 200: 99.90763092041016\n",
      "Loss at step 300: 100.24066925048828\n",
      "Loss at step 400: 99.87440490722656\n",
      "Loss at step 500: 109.6008071899414\n",
      "Loss at step 600: 99.94144439697266\n",
      "Loss at step 700: 101.60751342773438\n",
      "Loss at step 800: 99.43317413330078\n",
      "Loss at step 900: 98.26189422607422\n",
      "Loss at step 1000: 97.98829650878906\n",
      "Loss at step 1100: 99.13774871826172\n",
      "Loss at step 1200: 97.98566436767578\n",
      "Epoch 166/1000, Avg Train Loss: 99.39742136464535\n",
      "Loss at step 0: 98.7016372680664\n",
      "Loss at step 100: 100.20883178710938\n",
      "Loss at step 200: 98.5498275756836\n",
      "Loss at step 300: 99.58733367919922\n",
      "Loss at step 400: 98.15754699707031\n",
      "Loss at step 500: 99.91638946533203\n",
      "Loss at step 600: 97.37443542480469\n",
      "Loss at step 700: 100.16229248046875\n",
      "Loss at step 800: 97.5099868774414\n",
      "Loss at step 900: 98.87244415283203\n",
      "Loss at step 1000: 98.73682403564453\n",
      "Loss at step 1100: 98.83893585205078\n",
      "Loss at step 1200: 98.2117919921875\n",
      "Epoch 167/1000, Avg Train Loss: 99.40815408947398\n",
      "Loss at step 0: 98.83454132080078\n",
      "Loss at step 100: 99.24911499023438\n",
      "Loss at step 200: 99.49111938476562\n",
      "Loss at step 300: 100.84188079833984\n",
      "Loss at step 400: 98.64860534667969\n",
      "Loss at step 500: 98.24060821533203\n",
      "Loss at step 600: 97.96070861816406\n",
      "Loss at step 700: 97.90580749511719\n",
      "Loss at step 800: 101.94871520996094\n",
      "Loss at step 900: 101.54694366455078\n",
      "Loss at step 1000: 99.52110290527344\n",
      "Loss at step 1100: 99.63053131103516\n",
      "Loss at step 1200: 98.25238037109375\n",
      "Epoch 168/1000, Avg Train Loss: 99.41467130222753\n",
      "Loss at step 0: 98.2085952758789\n",
      "Loss at step 100: 98.59312438964844\n",
      "Loss at step 200: 98.71133422851562\n",
      "Loss at step 300: 98.50666809082031\n",
      "Loss at step 400: 98.50143432617188\n",
      "Loss at step 500: 98.62509155273438\n",
      "Loss at step 600: 98.0890884399414\n",
      "Loss at step 700: 98.1473159790039\n",
      "Loss at step 800: 98.97481536865234\n",
      "Loss at step 900: 98.23099517822266\n",
      "Loss at step 1000: 97.97380828857422\n",
      "Loss at step 1100: 100.56597137451172\n",
      "Loss at step 1200: 99.09114074707031\n",
      "Epoch 169/1000, Avg Train Loss: 99.42065545733307\n",
      "Loss at step 0: 99.2555160522461\n",
      "Loss at step 100: 97.87007904052734\n",
      "Loss at step 200: 98.55276489257812\n",
      "Loss at step 300: 99.13406372070312\n",
      "Loss at step 400: 99.73099517822266\n",
      "Loss at step 500: 117.56230926513672\n",
      "Loss at step 600: 99.48870086669922\n",
      "Loss at step 700: 97.9056625366211\n",
      "Loss at step 800: 98.44775390625\n",
      "Loss at step 900: 99.33196258544922\n",
      "Loss at step 1000: 98.31098937988281\n",
      "Loss at step 1100: 97.19536590576172\n",
      "Loss at step 1200: 99.35234832763672\n",
      "Epoch 170/1000, Avg Train Loss: 99.37706319568227\n",
      "Loss at step 0: 98.43057250976562\n",
      "Loss at step 100: 98.83451080322266\n",
      "Loss at step 200: 100.13845825195312\n",
      "Loss at step 300: 99.19737243652344\n",
      "Loss at step 400: 98.51461791992188\n",
      "Loss at step 500: 98.7954330444336\n",
      "Loss at step 600: 97.70246124267578\n",
      "Loss at step 700: 98.024658203125\n",
      "Loss at step 800: 98.64008331298828\n",
      "Loss at step 900: 98.0656509399414\n",
      "Loss at step 1000: 97.97577667236328\n",
      "Loss at step 1100: 98.75454711914062\n",
      "Loss at step 1200: 98.9383544921875\n",
      "Epoch 171/1000, Avg Train Loss: 99.43784238451121\n",
      "Loss at step 0: 97.538330078125\n",
      "Loss at step 100: 98.35317993164062\n",
      "Loss at step 200: 99.46128845214844\n",
      "Loss at step 300: 98.10763549804688\n",
      "Loss at step 400: 97.96238708496094\n",
      "Loss at step 500: 98.4434814453125\n",
      "Loss at step 600: 100.21150970458984\n",
      "Loss at step 700: 99.20907592773438\n",
      "Loss at step 800: 99.35719299316406\n",
      "Loss at step 900: 101.87921142578125\n",
      "Loss at step 1000: 97.76289367675781\n",
      "Loss at step 1100: 99.59004211425781\n",
      "Loss at step 1200: 97.44700622558594\n",
      "Epoch 172/1000, Avg Train Loss: 99.40749315922314\n",
      "Loss at step 0: 97.8332290649414\n",
      "Loss at step 100: 98.6728515625\n",
      "Loss at step 200: 97.40272521972656\n",
      "Loss at step 300: 97.69282531738281\n",
      "Loss at step 400: 99.42369079589844\n",
      "Loss at step 500: 98.57682037353516\n",
      "Loss at step 600: 98.48167419433594\n",
      "Loss at step 700: 102.05471801757812\n",
      "Loss at step 800: 98.61402893066406\n",
      "Loss at step 900: 101.04737091064453\n",
      "Loss at step 1000: 98.42021179199219\n",
      "Loss at step 1100: 99.07765197753906\n",
      "Loss at step 1200: 97.47775268554688\n",
      "Epoch 173/1000, Avg Train Loss: 99.42582755104237\n",
      "Loss at step 0: 104.59201049804688\n",
      "Loss at step 100: 99.41732025146484\n",
      "Loss at step 200: 97.39806365966797\n",
      "Loss at step 300: 98.72183227539062\n",
      "Loss at step 400: 98.59123229980469\n",
      "Loss at step 500: 97.58037567138672\n",
      "Loss at step 600: 97.80160522460938\n",
      "Loss at step 700: 98.22307586669922\n",
      "Loss at step 800: 99.62300109863281\n",
      "Loss at step 900: 98.45985412597656\n",
      "Loss at step 1000: 98.87686157226562\n",
      "Loss at step 1100: 98.23292541503906\n",
      "Loss at step 1200: 98.19522094726562\n",
      "Epoch 174/1000, Avg Train Loss: 99.40974889835493\n",
      "Loss at step 0: 98.76470947265625\n",
      "Loss at step 100: 98.286376953125\n",
      "Loss at step 200: 99.40225219726562\n",
      "Loss at step 300: 97.66590881347656\n",
      "Loss at step 400: 99.04483795166016\n",
      "Loss at step 500: 99.36761474609375\n",
      "Loss at step 600: 98.20419311523438\n",
      "Loss at step 700: 98.37435150146484\n",
      "Loss at step 800: 98.40672302246094\n",
      "Loss at step 900: 99.09814453125\n",
      "Loss at step 1000: 98.45655822753906\n",
      "Loss at step 1100: 99.87602996826172\n",
      "Loss at step 1200: 101.22671508789062\n",
      "Epoch 175/1000, Avg Train Loss: 99.38545421489233\n",
      "Loss at step 0: 97.36304473876953\n",
      "Loss at step 100: 99.509033203125\n",
      "Loss at step 200: 100.50021362304688\n",
      "Loss at step 300: 97.59732818603516\n",
      "Loss at step 400: 98.58779907226562\n",
      "Loss at step 500: 98.88372039794922\n",
      "Loss at step 600: 99.2834243774414\n",
      "Loss at step 700: 98.84320831298828\n",
      "Loss at step 800: 98.65975189208984\n",
      "Loss at step 900: 98.63189697265625\n",
      "Loss at step 1000: 98.3040771484375\n",
      "Loss at step 1100: 98.0638656616211\n",
      "Loss at step 1200: 99.83452606201172\n",
      "Epoch 176/1000, Avg Train Loss: 99.38032023729244\n",
      "Loss at step 0: 99.5822525024414\n",
      "Loss at step 100: 98.055419921875\n",
      "Loss at step 200: 98.10359191894531\n",
      "Loss at step 300: 99.75773620605469\n",
      "Loss at step 400: 99.12588500976562\n",
      "Loss at step 500: 102.00755310058594\n",
      "Loss at step 600: 98.49047088623047\n",
      "Loss at step 700: 98.32030487060547\n",
      "Loss at step 800: 103.22986602783203\n",
      "Loss at step 900: 97.92190551757812\n",
      "Loss at step 1000: 99.48046112060547\n",
      "Loss at step 1100: 98.44834899902344\n",
      "Loss at step 1200: 98.93132781982422\n",
      "Epoch 177/1000, Avg Train Loss: 99.41597659533849\n",
      "Loss at step 0: 97.87255859375\n",
      "Loss at step 100: 98.2392807006836\n",
      "Loss at step 200: 98.17488098144531\n",
      "Loss at step 300: 100.86089324951172\n",
      "Loss at step 400: 104.65852355957031\n",
      "Loss at step 500: 102.50814819335938\n",
      "Loss at step 600: 102.16214752197266\n",
      "Loss at step 700: 98.89556121826172\n",
      "Loss at step 800: 98.57606506347656\n",
      "Loss at step 900: 99.19961547851562\n",
      "Loss at step 1000: 99.8663101196289\n",
      "Loss at step 1100: 98.58268737792969\n",
      "Loss at step 1200: 97.70613098144531\n",
      "Epoch 178/1000, Avg Train Loss: 99.39060494506244\n",
      "Loss at step 0: 96.86683654785156\n",
      "Loss at step 100: 99.92300415039062\n",
      "Loss at step 200: 100.18012237548828\n",
      "Loss at step 300: 97.43646240234375\n",
      "Loss at step 400: 99.28071594238281\n",
      "Loss at step 500: 97.61648559570312\n",
      "Loss at step 600: 98.25572204589844\n",
      "Loss at step 700: 100.98355865478516\n",
      "Loss at step 800: 98.52898406982422\n",
      "Loss at step 900: 100.15269470214844\n",
      "Loss at step 1000: 98.03593444824219\n",
      "Loss at step 1100: 97.59925079345703\n",
      "Loss at step 1200: 99.91426086425781\n",
      "Epoch 179/1000, Avg Train Loss: 99.38752200919834\n",
      "Loss at step 0: 99.20538330078125\n",
      "Loss at step 100: 98.64803314208984\n",
      "Loss at step 200: 112.74003601074219\n",
      "Loss at step 300: 100.8503646850586\n",
      "Loss at step 400: 97.41278076171875\n",
      "Loss at step 500: 97.91613006591797\n",
      "Loss at step 600: 97.93067169189453\n",
      "Loss at step 700: 99.32917022705078\n",
      "Loss at step 800: 99.92896270751953\n",
      "Loss at step 900: 97.64291381835938\n",
      "Loss at step 1000: 100.78028106689453\n",
      "Loss at step 1100: 98.13041687011719\n",
      "Loss at step 1200: 100.0456314086914\n",
      "Epoch 180/1000, Avg Train Loss: 99.43003417450248\n",
      "Loss at step 0: 104.4594497680664\n",
      "Loss at step 100: 98.55274200439453\n",
      "Loss at step 200: 100.30581665039062\n",
      "Loss at step 300: 99.25066375732422\n",
      "Loss at step 400: 101.97652435302734\n",
      "Loss at step 500: 100.88729858398438\n",
      "Loss at step 600: 98.28492736816406\n",
      "Loss at step 700: 101.55538177490234\n",
      "Loss at step 800: 99.48118591308594\n",
      "Loss at step 900: 99.95164489746094\n",
      "Loss at step 1000: 98.48301696777344\n",
      "Loss at step 1100: 101.43556213378906\n",
      "Loss at step 1200: 99.75399780273438\n",
      "Epoch 181/1000, Avg Train Loss: 99.39724351210116\n",
      "Loss at step 0: 105.35002899169922\n",
      "Loss at step 100: 98.26042938232422\n",
      "Loss at step 200: 98.40707397460938\n",
      "Loss at step 300: 98.66162872314453\n",
      "Loss at step 400: 97.67375946044922\n",
      "Loss at step 500: 102.29545593261719\n",
      "Loss at step 600: 98.39492797851562\n",
      "Loss at step 700: 98.40869903564453\n",
      "Loss at step 800: 98.55802917480469\n",
      "Loss at step 900: 98.58478546142578\n",
      "Loss at step 1000: 99.56352996826172\n",
      "Loss at step 1100: 98.79015350341797\n",
      "Loss at step 1200: 98.3301773071289\n",
      "Epoch 182/1000, Avg Train Loss: 99.36756412419686\n",
      "Loss at step 0: 98.60630798339844\n",
      "Loss at step 100: 97.8673324584961\n",
      "Loss at step 200: 102.89846801757812\n",
      "Loss at step 300: 101.25340270996094\n",
      "Loss at step 400: 99.24710083007812\n",
      "Loss at step 500: 98.944580078125\n",
      "Loss at step 600: 98.10747528076172\n",
      "Loss at step 700: 98.96729278564453\n",
      "Loss at step 800: 98.112548828125\n",
      "Loss at step 900: 99.74178314208984\n",
      "Loss at step 1000: 98.5455093383789\n",
      "Loss at step 1100: 102.55113983154297\n",
      "Loss at step 1200: 100.08589172363281\n",
      "Epoch 183/1000, Avg Train Loss: 99.3740040677265\n",
      "Loss at step 0: 99.75511169433594\n",
      "Loss at step 100: 98.50286865234375\n",
      "Loss at step 200: 99.07913970947266\n",
      "Loss at step 300: 110.01313781738281\n",
      "Loss at step 400: 98.62785339355469\n",
      "Loss at step 500: 100.67109680175781\n",
      "Loss at step 600: 98.71460723876953\n",
      "Loss at step 700: 99.6637191772461\n",
      "Loss at step 800: 99.42802429199219\n",
      "Loss at step 900: 98.3280029296875\n",
      "Loss at step 1000: 98.89879608154297\n",
      "Loss at step 1100: 100.06478118896484\n",
      "Loss at step 1200: 98.67007446289062\n",
      "Epoch 184/1000, Avg Train Loss: 99.42502755717553\n",
      "Loss at step 0: 98.947998046875\n",
      "Loss at step 100: 100.54476165771484\n",
      "Loss at step 200: 97.63190460205078\n",
      "Loss at step 300: 99.97579193115234\n",
      "Loss at step 400: 98.89930725097656\n",
      "Loss at step 500: 99.60424041748047\n",
      "Loss at step 600: 98.67257690429688\n",
      "Loss at step 700: 101.65380096435547\n",
      "Loss at step 800: 98.33953094482422\n",
      "Loss at step 900: 97.69070434570312\n",
      "Loss at step 1000: 98.970947265625\n",
      "Loss at step 1100: 98.64325714111328\n",
      "Loss at step 1200: 99.18341064453125\n",
      "Epoch 185/1000, Avg Train Loss: 99.38846229503841\n",
      "Loss at step 0: 108.01190185546875\n",
      "Loss at step 100: 99.48031616210938\n",
      "Loss at step 200: 99.2923583984375\n",
      "Loss at step 300: 97.83235931396484\n",
      "Loss at step 400: 98.2127685546875\n",
      "Loss at step 500: 117.26907348632812\n",
      "Loss at step 600: 97.70323181152344\n",
      "Loss at step 700: 99.64309692382812\n",
      "Loss at step 800: 98.01326751708984\n",
      "Loss at step 900: 99.60990142822266\n",
      "Loss at step 1000: 98.7422103881836\n",
      "Loss at step 1100: 98.68217468261719\n",
      "Loss at step 1200: 99.36659240722656\n",
      "Epoch 186/1000, Avg Train Loss: 99.39375945896778\n",
      "Loss at step 0: 98.30899047851562\n",
      "Loss at step 100: 97.80963897705078\n",
      "Loss at step 200: 99.75507354736328\n",
      "Loss at step 300: 99.21233367919922\n",
      "Loss at step 400: 98.59101104736328\n",
      "Loss at step 500: 99.0617904663086\n",
      "Loss at step 600: 99.28382873535156\n",
      "Loss at step 700: 100.05235290527344\n",
      "Loss at step 800: 98.93618774414062\n",
      "Loss at step 900: 117.93870544433594\n",
      "Loss at step 1000: 99.03999328613281\n",
      "Loss at step 1100: 100.02641296386719\n",
      "Loss at step 1200: 97.9298324584961\n",
      "Epoch 187/1000, Avg Train Loss: 99.43454545524128\n",
      "Loss at step 0: 98.93814086914062\n",
      "Loss at step 100: 98.57718658447266\n",
      "Loss at step 200: 99.58061981201172\n",
      "Loss at step 300: 105.76822662353516\n",
      "Loss at step 400: 99.36578369140625\n",
      "Loss at step 500: 101.62731170654297\n",
      "Loss at step 600: 98.51691436767578\n",
      "Loss at step 700: 98.22872161865234\n",
      "Loss at step 800: 99.05669403076172\n",
      "Loss at step 900: 98.36015319824219\n",
      "Loss at step 1000: 99.17842864990234\n",
      "Loss at step 1100: 98.51673889160156\n",
      "Loss at step 1200: 101.2737045288086\n",
      "Epoch 188/1000, Avg Train Loss: 99.37103430121462\n",
      "Loss at step 0: 109.45317840576172\n",
      "Loss at step 100: 98.59632110595703\n",
      "Loss at step 200: 101.22479248046875\n",
      "Loss at step 300: 97.61847686767578\n",
      "Loss at step 400: 97.47577667236328\n",
      "Loss at step 500: 101.82999420166016\n",
      "Loss at step 600: 98.69549560546875\n",
      "Loss at step 700: 97.28511810302734\n",
      "Loss at step 800: 97.94435119628906\n",
      "Loss at step 900: 98.02157592773438\n",
      "Loss at step 1000: 99.62083435058594\n",
      "Loss at step 1100: 98.36412811279297\n",
      "Loss at step 1200: 98.08081817626953\n",
      "Epoch 189/1000, Avg Train Loss: 99.39701159949442\n",
      "Loss at step 0: 97.69268035888672\n",
      "Loss at step 100: 99.56813049316406\n",
      "Loss at step 200: 99.68610382080078\n",
      "Loss at step 300: 98.09464263916016\n",
      "Loss at step 400: 98.42799377441406\n",
      "Loss at step 500: 98.56403350830078\n",
      "Loss at step 600: 98.77666473388672\n",
      "Loss at step 700: 98.27525329589844\n",
      "Loss at step 800: 102.7525405883789\n",
      "Loss at step 900: 98.33800506591797\n",
      "Loss at step 1000: 97.68367767333984\n",
      "Loss at step 1100: 99.20427703857422\n",
      "Loss at step 1200: 98.81771087646484\n",
      "Epoch 190/1000, Avg Train Loss: 99.37633018123293\n",
      "Loss at step 0: 98.67366790771484\n",
      "Loss at step 100: 99.74003601074219\n",
      "Loss at step 200: 98.14239501953125\n",
      "Loss at step 300: 98.13584899902344\n",
      "Loss at step 400: 98.85108184814453\n",
      "Loss at step 500: 100.72820281982422\n",
      "Loss at step 600: 99.52708435058594\n",
      "Loss at step 700: 103.3969497680664\n",
      "Loss at step 800: 98.2587890625\n",
      "Loss at step 900: 101.16722869873047\n",
      "Loss at step 1000: 104.2138442993164\n",
      "Loss at step 1100: 98.70054626464844\n",
      "Loss at step 1200: 98.615478515625\n",
      "Epoch 191/1000, Avg Train Loss: 99.36390964421639\n",
      "Loss at step 0: 98.08338928222656\n",
      "Loss at step 100: 98.5472640991211\n",
      "Loss at step 200: 98.9415054321289\n",
      "Loss at step 300: 100.79398345947266\n",
      "Loss at step 400: 99.56409454345703\n",
      "Loss at step 500: 98.49333190917969\n",
      "Loss at step 600: 102.90226745605469\n",
      "Loss at step 700: 99.07573699951172\n",
      "Loss at step 800: 101.64322662353516\n",
      "Loss at step 900: 98.63371276855469\n",
      "Loss at step 1000: 100.7457275390625\n",
      "Loss at step 1100: 99.85152435302734\n",
      "Loss at step 1200: 99.02005004882812\n",
      "Epoch 192/1000, Avg Train Loss: 99.39970791764244\n",
      "Loss at step 0: 98.88617706298828\n",
      "Loss at step 100: 98.35292053222656\n",
      "Loss at step 200: 98.80541229248047\n",
      "Loss at step 300: 99.9566650390625\n",
      "Loss at step 400: 97.3255844116211\n",
      "Loss at step 500: 98.54531860351562\n",
      "Loss at step 600: 99.32609558105469\n",
      "Loss at step 700: 98.59420776367188\n",
      "Loss at step 800: 98.39035034179688\n",
      "Loss at step 900: 98.91664123535156\n",
      "Loss at step 1000: 97.66368103027344\n",
      "Loss at step 1100: 98.24969482421875\n",
      "Loss at step 1200: 99.39229583740234\n",
      "Epoch 193/1000, Avg Train Loss: 99.38039336466866\n",
      "Loss at step 0: 99.50162506103516\n",
      "Loss at step 100: 99.08061981201172\n",
      "Loss at step 200: 98.0235824584961\n",
      "Loss at step 300: 99.17447662353516\n",
      "Loss at step 400: 101.87971496582031\n",
      "Loss at step 500: 100.10301208496094\n",
      "Loss at step 600: 109.36449432373047\n",
      "Loss at step 700: 108.25834655761719\n",
      "Loss at step 800: 99.17157745361328\n",
      "Loss at step 900: 98.70711517333984\n",
      "Loss at step 1000: 98.08653259277344\n",
      "Loss at step 1100: 97.84963989257812\n",
      "Loss at step 1200: 97.9505386352539\n",
      "Epoch 194/1000, Avg Train Loss: 99.40275556447051\n",
      "Loss at step 0: 99.404541015625\n",
      "Loss at step 100: 99.63825225830078\n",
      "Loss at step 200: 99.9426040649414\n",
      "Loss at step 300: 100.33399200439453\n",
      "Loss at step 400: 107.20185852050781\n",
      "Loss at step 500: 97.3072738647461\n",
      "Loss at step 600: 98.74827575683594\n",
      "Loss at step 700: 98.28805541992188\n",
      "Loss at step 800: 97.73016357421875\n",
      "Loss at step 900: 98.97699737548828\n",
      "Loss at step 1000: 98.1830825805664\n",
      "Loss at step 1100: 97.97632598876953\n",
      "Loss at step 1200: 99.13103485107422\n",
      "Epoch 195/1000, Avg Train Loss: 99.37524345546093\n",
      "Loss at step 0: 98.57898712158203\n",
      "Loss at step 100: 100.2027359008789\n",
      "Loss at step 200: 99.3623275756836\n",
      "Loss at step 300: 103.77163696289062\n",
      "Loss at step 400: 97.68933868408203\n",
      "Loss at step 500: 98.23018646240234\n",
      "Loss at step 600: 100.84941101074219\n",
      "Loss at step 700: 98.8995132446289\n",
      "Loss at step 800: 99.26840209960938\n",
      "Loss at step 900: 98.20813751220703\n",
      "Loss at step 1000: 103.63186645507812\n",
      "Loss at step 1100: 99.3031005859375\n",
      "Loss at step 1200: 98.6355972290039\n",
      "Epoch 196/1000, Avg Train Loss: 99.43279745355008\n",
      "Loss at step 0: 101.54798126220703\n",
      "Loss at step 100: 98.13321685791016\n",
      "Loss at step 200: 100.46051788330078\n",
      "Loss at step 300: 98.91963958740234\n",
      "Loss at step 400: 98.20592498779297\n",
      "Loss at step 500: 99.29781341552734\n",
      "Loss at step 600: 98.91590118408203\n",
      "Loss at step 700: 98.92304992675781\n",
      "Loss at step 800: 98.52865600585938\n",
      "Loss at step 900: 99.41542053222656\n",
      "Loss at step 1000: 98.25157165527344\n",
      "Loss at step 1100: 98.64812469482422\n",
      "Loss at step 1200: 98.47966766357422\n",
      "Epoch 197/1000, Avg Train Loss: 99.39355460725555\n",
      "Loss at step 0: 98.3351821899414\n",
      "Loss at step 100: 99.4998550415039\n",
      "Loss at step 200: 97.5904541015625\n",
      "Loss at step 300: 99.15955352783203\n",
      "Loss at step 400: 98.7597885131836\n",
      "Loss at step 500: 99.11290740966797\n",
      "Loss at step 600: 98.54808807373047\n",
      "Loss at step 700: 98.2662353515625\n",
      "Loss at step 800: 98.7719497680664\n",
      "Loss at step 900: 101.83899688720703\n",
      "Loss at step 1000: 99.39369201660156\n",
      "Loss at step 1100: 106.21993255615234\n",
      "Loss at step 1200: 97.22572326660156\n",
      "Epoch 198/1000, Avg Train Loss: 99.36245783092906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 42, in <module>\n",
      "    glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 32, in load_glove_embeddings\n",
      "    vector = np.asarray(values[1:], dtype='float32')\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m average \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, criterion, optimizer)\n\u001b[1;32m      8\u001b[0m     loss_plot\u001b[38;5;241m.\u001b[39mextend(batch_losses)  \u001b[38;5;66;03m# Store losses for each batch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Print average loss for the epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m step_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# loops over all mini-batches in the dataloader \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, price \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     price_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(price_pred, price)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Training Loop \n",
    "num_epochs = 1000\n",
    "loss_plot = []  # keep a vector to plot the loss as we go \n",
    "average = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    loss_plot.extend(batch_losses)  # Store losses for each batch\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg Train Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Update the learning rate \n",
    "    scheduler.step()\n",
    "\n",
    "    # if the average epoch loss goes below human error significantly, stop training \n",
    "    if avg_epoch_loss < 30: \n",
    "        break\n",
    "\n",
    "    if (epoch % 100) == 0: \n",
    "        torch.save(model.state_dict(), 'model_weights.txt')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(loss_plot, label=\"Batch Loss\")\n",
    "plt.plot(average, label=\"Average Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4213067-3809-46fa-91c3-442678fdcbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average, label=\"Average Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31cbe535-ac17-4ad2-b76f-6e0b7fec80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: A pair of yellow metal coin set cufflinks, a turquoise set ring (stone deficient), a stone set ring, a pair of sapphire set earrings, one earring and a biwa cultured pearl bracelet.X75568\n",
      "Title: nan\n",
      "Real Price: $97.70\n",
      "Model Prediction: $2759.90\n",
      "3750.13427734375\n"
     ]
    }
   ],
   "source": [
    "def prediction_to_real_price(price_tensor): \n",
    "    return (price_tensor * dataset.price_std) + dataset.price_median\n",
    "\n",
    "# Sample the Training Set \n",
    "index = 1004\n",
    "x_test, price_test = dataset.__getitem__(index)\n",
    "artist_str, title_str = dataset.__getstring__(index)\n",
    "print(f\"Artist: {artist_str}\")\n",
    "print(f\"Title: {title_str}\")\n",
    "print(f\"Real Price: ${prediction_to_real_price(price_test).item():.2f}\")\n",
    "prediction = model(x_test.view(1, -1))\n",
    "print(f\"Model Prediction: ${prediction_to_real_price(prediction).item():.2f}\")\n",
    "with torch.no_grad(): \n",
    "    print(criterion(price_test, prediction).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cc8aeab-5ef6-4f0f-9fc8-67710cd44e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: A pink lustre 'Freemason's' jug\n",
      "Title: nan\n",
      "Real Price: $219.52\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'artist_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Price: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_to_real_price(price_test)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model(artist_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), title_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), numerics_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Prediction: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_to_real_price(prediction)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'artist_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Artist: {artist_str}\")\n",
    "print(f\"Title: {title_str}\")\n",
    "print(f\"Real Price: ${prediction_to_real_price(price_test).item():.2f}\")\n",
    "prediction = model(artist_test.view(1, -1), title_test.view(1, -1), numerics_test.view(1, -1))\n",
    "print(f\"Model Prediction: ${prediction_to_real_price(prediction).item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "35729b1f-12eb-4630-8629-bed3cb9e1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54.4121)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    print(criterion(price_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8ab652e-d671-456d-9a3e-364fb098e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Price Normalized:  tensor([-0.0092])\n",
      "Model Prediction Normalized:  tensor([[-2.1695e-08]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Price Normalized: \", price_test)\n",
    "print(\"Model Prediction Normalized: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5423a23-3678-4b67-9e99-aee0cb76d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight:\n",
      "tensor([[-0.2612, -0.3165, -0.0925, -0.1503, -0.1302,  0.0803,  0.1983, -0.0735,\n",
      "          0.0091,  0.3834, -0.1505,  0.0686,  0.0193],\n",
      "        [ 0.2101,  0.3574,  0.4466, -0.3402,  0.1273,  0.0058, -0.3784,  0.0053,\n",
      "         -0.5275,  0.3315, -0.1071, -0.1518, -0.0346],\n",
      "        [-0.0795, -0.2030,  0.3063, -0.0636, -0.1748,  0.0216,  0.0373, -0.1911,\n",
      "         -0.2639,  0.0804,  0.1417, -0.0013, -0.0812],\n",
      "        [ 0.1090,  0.1107, -0.2536,  0.3599, -0.1825,  0.2160,  0.1724, -0.4350,\n",
      "          0.2164, -0.4358,  0.1931,  0.3384,  0.0540],\n",
      "        [-0.3601, -0.1574, -0.1143, -0.0907, -0.2680,  0.1386,  0.2232, -0.1045,\n",
      "          0.1425, -0.0342,  0.0586, -0.0394, -0.0060],\n",
      "        [-0.1171, -0.5389,  0.1044,  0.1699, -0.1923, -0.3639, -0.0117, -0.0824,\n",
      "         -0.1775, -0.2600, -0.2471,  0.2647, -0.0533],\n",
      "        [ 0.1294,  0.0744, -0.0263,  0.4745, -0.2291,  0.1951,  0.3845,  0.1570,\n",
      "          0.2338, -0.2118,  0.5391,  0.0463,  0.4172],\n",
      "        [-0.2072, -0.0140, -0.0899,  0.0605, -0.2144, -0.1092,  0.0264, -0.0385,\n",
      "         -0.0209, -0.0822,  0.0858, -0.2819, -0.0158]])\n",
      "\n",
      "fc1.bias:\n",
      "tensor([-0.1312,  0.3242, -0.0517, -0.1531,  0.1452, -0.1008,  0.2891, -0.0679])\n",
      "\n",
      "fc2.weight:\n",
      "tensor([[ 0.1471, -0.3638, -0.0070, -0.0702, -0.3050, -0.1307, -0.2299, -0.0376],\n",
      "        [-0.0179, -0.2813, -0.0876,  0.0059, -0.2308,  0.0426, -0.0613, -0.3399],\n",
      "        [-0.5157, -0.1543,  0.0142, -0.4407,  0.4583,  0.2321, -0.2439, -0.0939],\n",
      "        [ 0.4911, -0.4961,  0.2836, -0.0364, -0.2880, -0.0405, -0.0236, -0.1612]])\n",
      "\n",
      "fc2.bias:\n",
      "tensor([ 0.2232, -0.0777, -0.2261, -0.2100])\n",
      "\n",
      "fc3.weight:\n",
      "tensor([[-0.2992, -0.0023, -0.0525,  0.0641]])\n",
      "\n",
      "fc3.bias:\n",
      "tensor([-2.1695e-08])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all weights and biases\n",
    "for name, param in model.state_dict().items():\n",
    "    print(f\"{name}:\\n{param}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dae16-82c1-4b11-92eb-23a6dbdaa020",
   "metadata": {},
   "source": [
    "Compute human error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2039366-5817-4d25-aafd-fb8294cc6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (MAPE): 39.08%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'clean_art.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the human predicted price (midpoint)\n",
    "data['Human Predicted Price'] = (data['Real LB Estimate USD'] + data['Real UB Estimate USD']) / 2\n",
    "\n",
    "# Calculate the absolute percentage error for each row\n",
    "data['Absolute Percentage Error'] = abs(data['Human Predicted Price'] - data['Real Price USD']) / data['Real Price USD']\n",
    "\n",
    "# Calculate the mean of the absolute percentage error (MAPE)\n",
    "mape = data['Absolute Percentage Error'].mean() * 100\n",
    "\n",
    "# Output the result\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93666450-0d6e-4efa-b20a-9172a66a829d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
