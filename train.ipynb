{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:51.785873Z",
     "start_time": "2024-11-12T23:37:51.783696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataload import ArtDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from stringproc import create_vocab_csv, text_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b232412f332d13c",
   "metadata": {},
   "source": [
    "## Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5feafcb1d70f0103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:29:19.684963Z",
     "start_time": "2024-11-12T23:29:13.939927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist Indexed Tokens: tensor([[  101,  3434,  3158,  ...,     0,     0,     0],\n",
      "        [  101,  2394,  2082,  ...,     0,     0,     0],\n",
      "        [  101,  9586,  1012,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  3744,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  8149,  1011,  ...,     0,     0,     0],\n",
      "        [  101,  2726, 12154,  ...,     0,     0,     0]])\n",
      " Artist Segment IDs: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "Title Indexed Tokens: tensor([[  101,  1037,  3803,  ...,     0,     0,     0],\n",
      "        [  101, 14783,  3317,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  3193,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1996,  3752,  ...,     0,     0,     0],\n",
      "        [  101,  7095,  3417,  ...,     0,     0,     0],\n",
      "        [  101,  6533,  1997,  ...,  1037,  2911,   102]])\n",
      " Title Segment IDs: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# initialize the dataset and the dataloader\n",
    "dataset = ArtDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22762236c99383",
   "metadata": {},
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "Train: 95%\n",
    "Validation: 2.5%\n",
    "Test: 2.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36c1cd3c142596b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:14.087736Z",
     "start_time": "2024-11-12T23:36:14.084854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.10 * total_size)\n",
    "val_size = int(0.40 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0d334a62a30ae9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:25.806319Z",
     "start_time": "2024-11-12T23:36:25.803823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33305 133223 166531\n"
     ]
    }
   ],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66344851359cb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:55.298758Z",
     "start_time": "2024-11-12T23:37:55.283177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9da68da70bb17489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:05.395086Z",
     "start_time": "2024-11-12T23:38:05.392195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33305\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdc63acd350c92bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:56.288337Z",
     "start_time": "2024-11-12T23:38:56.284944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Higher batch size seems to make the model train faster, but converge happens slower \n",
    "batch = 128 \n",
    "\n",
    "# Create dataloaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd7115-5619-4a2d-9500-ca2066af99d8",
   "metadata": {},
   "source": [
    "## Importing and Testing BERT embeddings\n",
    "BERT allows us to extract context dependent embeddings of each word in a given sentence. For instance, in the phrases \"river bank\" and \"bank of America\" the word bank would end up having a drastically different embedding. We test BERT's ability to recognize artist names, and embed them to vectors close to other artists of similar statures and time periods. \n",
    "\n",
    "#### INCLUDE CITATION\n",
    "Chris McCormick and Nick Ryan. (2019, May 14). *BERT Word Embeddings Tutorial*. Retrieved from http://www.mccormickml.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6c9ffc-3faf-418a-aa1b-81f9ea00ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pretrained BERT Model\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "\n",
    "# if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27352cc8-4c91-469e-bd6c-f2f35c8d98ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "base_model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states\n",
    "                                  num_labels=13                            \n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe6964dc-686d-4608-9be1-df1dc8f3dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1037,  3940,  1997,  3756,  3384,  9226,  2275, 26450, 13767,\n",
      "          2015,  1010,  1037, 28653,  2275,  3614,  1006,  2962, 13366, 20132,\n",
      "          1007,  1010,  1037,  2962,  2275,  3614,  1010,  1037,  3940,   102]])\n",
      "torch.Size([1, 30])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([1, 30])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# function: prediction_to_real_price \n",
    "# ------------------------------------------------------\n",
    "# converts the normalized price tensor back to the original tensor \n",
    "# \n",
    "# @param price_tensor Tensor containing the price we would like to convert \n",
    "# to the original price \n",
    "# \n",
    "# @returns Original Price\n",
    "def prediction_to_real_price(price_tensor): \n",
    "    return (price_tensor * dataset.price_std) + dataset.price_median\n",
    "\n",
    "# Sample the Training Set \n",
    "index = 1004\n",
    "artist_tokens_tensor, artist_segids, price_tensor, price_classifier = dataset.__getitem__(index)\n",
    "artist_tokens_tensor = artist_tokens_tensor.view(1, -1)\n",
    "artist_segids = artist_segids.view(1, -1)\n",
    "print(artist_tokens_tensor)\n",
    "print(artist_tokens_tensor.shape)\n",
    "print(artist_segids)\n",
    "print(artist_segids.shape)\n",
    "print(price_classifier)\n",
    "#artist_str, title_str = dataset.__getstring__(index)\n",
    "#print(f\"Artist: {artist_str}\")\n",
    "#print(f\"Title: {title_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3fe2aec-da99-4130-96f2-3b9a135533e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Takes in a string and outputs a list of the indices of the \n",
    "# words inside, and the tokens \n",
    "def encode_text(text): \n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    return tokenized_text, indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009ad2d5-2d51-47b3-9b9a-f7cd14399fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "artist        3,063\n",
      "name          2,171\n",
      "from          2,013\n",
      "christie     13,144\n",
      "auction      10,470\n",
      "lot           2,843\n",
      ":             1,024\n",
      "duane        27,319\n",
      "hanson       17,179\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "tokenized_text, indexed_tokens = encode_text(\"Artist Name from Christie Auction Lot: Duane Hanson\")\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8643fcb-7575-4e2e-953b-9fb9fe637a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b930cf99-849f-4c66-8b5e-43f750eba099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor_test = torch.tensor([indexed_tokens])\n",
    "segments_tensors_test = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eea227c-b286-4323-bb49-9cbcaa5b99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers.\n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = base_model(tokens_tensor_test, segments_tensors_test)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on\n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "    # becase we set `output_hidden_states = True`, the third item will be the\n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49722aec-de31-4bdf-b022-61826557dc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 11, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f81ec110-e078-4637-a144-2fc97daf965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "352c7682-b1b6-4d5f-b359-d19757536c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 13 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last\n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55b48421-efe8-41ac-9893-bc6bb2efbba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 13 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e397e6e-00bf-4e97-b3e9-a827f0917f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 artist\n",
      "2 name\n",
      "3 from\n",
      "4 christie\n",
      "5 auction\n",
      "6 lot\n",
      "7 :\n",
      "8 duane\n",
      "9 hanson\n",
      "10 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca651711-ded9-45fe-9b1e-70fe0735610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duane tensor([ 0.0679, -1.3539,  0.1005, -0.4218,  0.3941,  1.3565,  2.5718])\n",
      "Hanson tensor([-2.2877e-01, -6.0235e-01, -1.6311e-03, -5.5483e-01,  1.5981e-01,\n",
      "         1.8523e+00,  2.3278e+00])\n"
     ]
    }
   ],
   "source": [
    "print(\"Duane\", str(token_vecs_sum[8][:7]))\n",
    "print(\"Hanson\", str(token_vecs_sum[9][:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ab14464-5976-4af5-b7c0-6d6277b6ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  two sculptors:  0.80\n",
      "Vector similarity for *different* Hansons:  0.58\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word Hanson\n",
    "# in \"Duane Hanson\" vs the band \"Hanson\" (different meanings).\n",
    "diff_hanson = 1 - cosine(token_vecs_sum[2], token_vecs_sum[10])\n",
    "\n",
    "# Calculate the cosine similarity between the word Hanson\n",
    "# in \"Duane Hanson\" vs \"Henry Moore\" (another sculptor).\n",
    "sculptors = 1 - cosine(token_vecs_sum[2], token_vecs_sum[5])\n",
    "\n",
    "print('Vector similarity for  two sculptors:  %.2f' % sculptors)\n",
    "print('Vector similarity for *different* Hansons:  %.2f' % diff_hanson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bf433-893c-4d1d-ba2a-e987166a486e",
   "metadata": {},
   "source": [
    "## Define the Models\n",
    "We define the following models: \n",
    "+ A BERT classifier: A pretrained transformer model for classification of sequence text data into a specified number of categories. We will specialize this model to classify the predicted real price ($) of artworks into 13 different price categories: 0-50, 50-100, 100-250, 250-500, 500-750, 750-1000, 1000-5000, 5000-10000, 10000-50000, 50000-100000, 100000-500000, 500000-1M, 1M+. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95cd91-0076-472f-bb3b-03cc1c7e6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(set(dataset.price_classifier)),  # Number of classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1d0b49f7927db",
   "metadata": {},
   "source": [
    "## Define the loss function as mean average percentage error\n",
    "We are using this for when we predict the price itself. The reason for this is that we do not want to penalize a prediction that is off by 5 dollars equally for a 1M dollar painting as for a 50 dollar painting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d0dc9ae1-ccde-4a60-a56c-886b786110e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define percentage error loss\n",
    "# We use percentage error so that errors on large prices are treated more leniently than errors on small prices \n",
    "class MAPE(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(MAPE, self).__init__()\n",
    "        self.epsilon = epsilon  # Small constant to avoid division by zero\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Calculate MAPE\n",
    "        percentage_errors = torch.abs((targets - predictions) / (targets + self.epsilon))\n",
    "        mape = 100.0 * torch.mean(percentage_errors)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730f645-16b7-44e1-a43d-a654bfdd2b02",
   "metadata": {},
   "source": [
    "## Train the models using Mini-Batch Gradient Descent with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9712b95-e956-4575-9843-39aa1bc2812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c87bc2-65a4-48a9-9d87-721764c57660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.6128101348876953\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.4017815589904785\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.9808945655822754\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.351076602935791\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0037050247192383\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.2800509929656982\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0588364601135254\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.971658706665039\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.20938777923584\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.147477865219116\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.093181848526001\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0845491886138916\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0265800952911377\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.80612850189209\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.3256614208221436\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.2317044734954834\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.282628297805786\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.321744918823242\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.069626569747925\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.3449931144714355\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0035691261291504\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.9275224208831787\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.2585129737854004\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.2996976375579834\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.2019026279449463\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.6965551376342773\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.1267247200012207\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0394647121429443\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.978070020675659\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.027958869934082\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.855013608932495\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.71594500541687\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.817000150680542\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.7798116207122803\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0100655555725098\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.2242090702056885\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.775245428085327\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.156662702560425\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.0486772060394287\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.503202438354492\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.635279893875122\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.7582409381866455\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.7581756114959717\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.660926580429077\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.8502347469329834\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.7912254333496094\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 3.069357395172119\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.985666036605835\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.405480146408081\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.898028612136841\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4996583461761475\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.6949076652526855\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.790498733520508\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.577810764312744\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.8378853797912598\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4137401580810547\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.346961736679077\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5086469650268555\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4218156337738037\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.677490472793579\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3502235412597656\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.7450671195983887\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.720172643661499\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.885560989379883\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.8745975494384766\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.792461633682251\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.471285820007324\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5989301204681396\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.668224334716797\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5685815811157227\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.7755930423736572\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.485421657562256\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5378406047821045\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.412020683288574\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.566650629043579\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.422820568084717\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4556353092193604\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5440804958343506\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3141491413116455\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4397830963134766\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2702202796936035\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.7281439304351807\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3314027786254883\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4609005451202393\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5086283683776855\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.60381817817688\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5319173336029053\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.500793695449829\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.6968255043029785\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.276305675506592\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.327761173248291\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4225783348083496\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4324254989624023\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2135226726531982\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5686838626861572\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.474558115005493\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4207396507263184\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.5028090476989746\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3443984985351562\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.140075206756592\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3490564823150635\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3003604412078857\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2707254886627197\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.38651180267334\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.212346076965332\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3406171798706055\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3109288215637207\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3611347675323486\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.362248659133911\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.388821601867676\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.485909938812256\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3699302673339844\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.33362078666687\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1433684825897217\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3435757160186768\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.319483757019043\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.460520029067993\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4179792404174805\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3918251991271973\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2158117294311523\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.098499298095703\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3001933097839355\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1341521739959717\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1416821479797363\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4387454986572266\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1657352447509766\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.292879104614258\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3011531829833984\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.372455358505249\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2173800468444824\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2195796966552734\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3316490650177\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.284700870513916\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.173671007156372\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.283184766769409\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.373793601989746\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1206374168395996\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4018285274505615\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.372800350189209\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4605255126953125\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.153517246246338\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3725454807281494\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3378586769104004\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.444340467453003\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2112483978271484\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2480130195617676\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1180663108825684\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2699294090270996\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2772769927978516\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.065206289291382\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.14158034324646\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3945345878601074\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0468122959136963\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.335312604904175\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.144592523574829\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.205380439758301\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.139258861541748\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.4147541522979736\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1313650608062744\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.136409282684326\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2346646785736084\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2111361026763916\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2733235359191895\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2956297397613525\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2830381393432617\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.253696918487549\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0255253314971924\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3738067150115967\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2236216068267822\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.129753828048706\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9813283681869507\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2997117042541504\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.189080238342285\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1420059204101562\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0746288299560547\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1402478218078613\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.045717239379883\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.45829439163208\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2172305583953857\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9793297052383423\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2206530570983887\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.271466016769409\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.3381035327911377\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.217238187789917\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2690725326538086\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2485108375549316\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9867771863937378\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0771236419677734\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1272456645965576\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0981781482696533\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0989296436309814\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.237898826599121\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1428582668304443\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.077427864074707\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9645311832427979\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0256268978118896\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.270872116088867\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2010326385498047\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2303948402404785\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1801910400390625\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2038114070892334\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.254687547683716\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0402469635009766\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.222316026687622\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1993508338928223\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9412577152252197\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0153656005859375\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.166860580444336\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1085240840911865\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.199110269546509\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.151207685470581\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0258522033691406\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.107424736022949\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.005204200744629\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1543755531311035\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.14725923538208\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.132514715194702\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.133974313735962\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.071378469467163\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0343775749206543\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0490851402282715\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.2123379707336426\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1840100288391113\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1636979579925537\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9942560195922852\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.02284574508667\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9685636758804321\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.178623914718628\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.1719439029693604\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 1.9648563861846924\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0857605934143066\n",
      "Doing backward propagation\n",
      "Adjusting the parameters\n",
      "Zeroing out the gradients\n",
      "Making the forward pass\n",
      "Computing the loss\n",
      "Loss: 2.0119166374206543\n",
      "Doing backward propagation\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "losses = []\n",
    "epoch_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for artist, artist_seg_ids, price, price_classifier in train_loader:\n",
    "        print(\"Zeroing out the gradients\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        print(\"Making the forward pass\")\n",
    "        outputs = model(input_ids=artist, attention_mask=artist_seg_ids, \n",
    "                        labels=price_classifier)\n",
    "        print(\"Computing the loss\")\n",
    "        loss = outputs.loss\n",
    "        print(f'Loss: {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        print(\"Doing backward propagation\")\n",
    "        loss.backward()\n",
    "\n",
    "        print(\"Adjusting the parameters\")\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save a copy of the model weights after each epoch so we don't lose our progress\n",
    "    model.save_pretrained(\"./fine_tuned_bert\")\n",
    "    epoch_losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e3329a7a-c67b-4f9d-bbdd-364b3a805689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one epoch...\n",
      "Loss at step 0: 100.01216888427734\n",
      "Loss at step 100: 104.21656036376953\n",
      "Loss at step 200: 98.885498046875\n",
      "Loss at step 300: 97.80712127685547\n",
      "Loss at step 400: 99.3489761352539\n",
      "Loss at step 500: 98.05636596679688\n",
      "Loss at step 600: 99.31829833984375\n",
      "Loss at step 700: 99.65673828125\n",
      "Loss at step 800: 99.76976013183594\n",
      "Loss at step 900: 98.59403228759766\n",
      "Loss at step 1000: 98.342041015625\n",
      "Loss at step 1100: 98.83724212646484\n",
      "Loss at step 1200: 98.95890045166016\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(losses, label=\"Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6499a-e5c2-413d-b53d-e1a526a7ba17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94629f30-b969-4f8e-b20e-5988676c0e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ddc07-8048-4b8f-9481-0e95a019a8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "516dae16-82c1-4b11-92eb-23a6dbdaa020",
   "metadata": {},
   "source": [
    "## Compute human error\n",
    "Compute the average human error based on choosing the middle of the auctioneer's estimate as the auctioneer's prediction for that lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2039366-5817-4d25-aafd-fb8294cc6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (MAPE): 39.08%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'clean_art.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the human predicted price (midpoint)\n",
    "data['Human Predicted Price'] = (data['Real LB Estimate USD'] + data['Real UB Estimate USD']) / 2\n",
    "\n",
    "# Calculate the absolute percentage error for each row\n",
    "data['Absolute Percentage Error'] = abs(data['Human Predicted Price'] - data['Real Price USD']) / data['Real Price USD']\n",
    "\n",
    "# Calculate the mean of the absolute percentage error (MAPE)\n",
    "mape = data['Absolute Percentage Error'].mean() * 100\n",
    "\n",
    "# Output the result\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93666450-0d6e-4efa-b20a-9172a66a829d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
