{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:51.785873Z",
     "start_time": "2024-11-12T23:37:51.783696Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from scratch import ArtDataset\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from model import create_vocab_csv, text_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b232412f332d13c",
   "metadata": {},
   "source": [
    "## Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5feafcb1d70f0103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:29:19.684963Z",
     "start_time": "2024-11-12T23:29:13.939927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# initialize the dataset and the dataloader\n",
    "dataset = ArtDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22762236c99383",
   "metadata": {},
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "Train: 95%\n",
    "Validation: 2.5%\n",
    "Test: 2.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c1cd3c142596b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:14.087736Z",
     "start_time": "2024-11-12T23:36:14.084854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.95 * total_size)\n",
    "val_size = int(0.025 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d334a62a30ae9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:36:25.806319Z",
     "start_time": "2024-11-12T23:36:25.803823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316406 8326 8327\n"
     ]
    }
   ],
   "source": [
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66344851359cb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:37:55.298758Z",
     "start_time": "2024-11-12T23:37:55.283177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train, validation (dev), and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da68da70bb17489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:05.395086Z",
     "start_time": "2024-11-12T23:38:05.392195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316406\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc63acd350c92bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T23:38:56.288337Z",
     "start_time": "2024-11-12T23:38:56.284944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Higher batch size seems to make the model train faster, but converge happens slower \n",
    "batch = 256  \n",
    "\n",
    "# Create dataloaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442959e-8212-45c4-bb00-7dba3d009d6a",
   "metadata": {},
   "source": [
    "## Load The GloVe Embeddings\n",
    "Below we load the GloVe embeddings, and define functions to search the vicinity of given target words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5afb8366-ecd0-4924-868d-75e51b20fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Set path to GloVe file and embedding dimension\n",
    "glove_path = \"glove.6B.50d.txt\"  # Update this to your GloVe file path\n",
    "embedding_dim = 50\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b2b35c-82cd-44f4-8244-2f9de0d7f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36373b2-41e0-4331-8670-481ea4efb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_neighbors(word, glove_embeddings, k=5):\n",
    "    if word not in glove_embeddings:\n",
    "        print(f\"{word} not found in GloVe embeddings.\")\n",
    "        return []\n",
    "\n",
    "    # Get the embedding vector of the target word\n",
    "    target_vector = glove_embeddings[word]\n",
    "    \n",
    "    # Calculate similarity between the target word and every other word in the embeddings\n",
    "    similarities = {}\n",
    "    for other_word, other_vector in glove_embeddings.items():\n",
    "        if other_word != word:\n",
    "            similarity = cosine_similarity(target_vector, other_vector)\n",
    "            similarities[other_word] = similarity\n",
    "\n",
    "    # Sort words by similarity and get the top k\n",
    "    nearest_neighbors = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d704e2f-7951-4ff7-a90c-dcf845fba0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('contemporary', np.float32(0.8596234)), ('works', np.float32(0.83878714)), ('arts', np.float32(0.8278873)), ('museum', np.float32(0.82194173)), ('collection', np.float32(0.8105687))]\n"
     ]
    }
   ],
   "source": [
    "print(find_k_nearest_neighbors(\"art\", glove_embeddings, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1192718-ead1-4e97-af2d-5de22b0a41f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.2013035)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_embeddings[\"arbus\"], glove_embeddings[\"rare\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bf433-893c-4d1d-ba2a-e987166a486e",
   "metadata": {},
   "source": [
    "## Define the Models\n",
    "We define the model to have 6 fully-connected layers, with 16, 16, 16, 16, 16, 8, 1 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc95cd91-0076-472f-bb3b-03cc1c7e6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtPricePredictor(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        super(ArtPricePredictor, self).__init__()\n",
    "        \n",
    "        # Fully connected layers for combined features\n",
    "        self.fc1 = nn.Linear(features_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Final output for price prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17180bea-0b82-468d-89b1-46e993873e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtPriceClassifier(nn.Module):\n",
    "    def __init__(self, features_dim):\n",
    "        super(ArtPricePredictor, self).__init__()\n",
    "        \n",
    "        # Fully connected layers for combined features\n",
    "        self.fc1 = nn.Linear(features_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # Final output for price prediction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1d0b49f7927db",
   "metadata": {},
   "source": [
    "## Instantiate the Art Price Predictor Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937adf300be14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dimensions of the inputs \n",
    "vocab_size_artist = dataset.artist_vocab_len + 1\n",
    "vocab_size_title = dataset.title_vocab_len + 1\n",
    "numerical_features_dim = dataset.numerics.shape[1]\n",
    "print(f' Artist Vocab Size: {vocab_size_artist}\\n Title Vocab Size: {vocab_size_title}\\n Number of numerical features: {numerical_features_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8a1cf26-6522-42f5-8870-7f95bdec7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model hyperparameters \n",
    "learning_rate =0.0000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c4499f930c43f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtPricePredictor(\n",
      "  (fc1): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "x, y = dataset.__getitem__(0)\n",
    "model = ArtPricePredictor(x.size()[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0dc9ae1-ccde-4a60-a56c-886b786110e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define percentage error loss\n",
    "# We use percentage error so that errors on large prices are treated more leniently than errors on small prices \n",
    "class MAPE(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(MAPE, self).__init__()\n",
    "        self.epsilon = epsilon  # Small constant to avoid division by zero\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Calculate MAPE\n",
    "        percentage_errors = torch.abs((targets - predictions) / (targets + self.epsilon))\n",
    "        mape = 100.0 * torch.mean(percentage_errors)\n",
    "        return mape\n",
    "\n",
    "criterion = MAPE() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.90, 0.999))  # ADAM Optimization (first beta controls momentum)\n",
    "\n",
    "# APPLY LEARNING RATE DECAY \n",
    "# Reduces the learning rate by a factor of gamma every step_size epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57a91a76-1ba1-440d-bd13-8220ec298ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtPricePredictor(\n",
      "  (fc1): Linear(in_features=13, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc5): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a38f452c-9536-4f0e-a58f-11c45a9825d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss  tensor(1481.3180, grad_fn=<MulBackward0>)\n",
      "Model Prediction:  tensor([[0.1241],\n",
      "        [0.1237]], grad_fn=<AddmmBackward0>)\n",
      "Actual Label:  tensor([[-0.0083],\n",
      "        [-0.0097]])\n"
     ]
    }
   ],
   "source": [
    "# make a prediction and extract the actual value\n",
    "prediction = model(dataset.x[0:2])\n",
    "label = dataset.price[0:2]\n",
    "\n",
    "# evaluate the loss\n",
    "print(\"Loss \", criterion(prediction, label))\n",
    "print(\"Model Prediction: \", prediction)\n",
    "print(\"Actual Label: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730f645-16b7-44e1-a43d-a654bfdd2b02",
   "metadata": {},
   "source": [
    "## Train the model using Mini-Batch Gradient Descent with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9a54ca6-f74a-4c3e-a5ad-02a3ac48d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    batch_losses = []  # Store each batch's loss\n",
    "    step_num = 0\n",
    "\n",
    "    # loops over all mini-batches in the dataloader \n",
    "    for x, price in dataloader:\n",
    "        \n",
    "        # Forward pass\n",
    "        price_pred = model(x)\n",
    "        loss = criterion(price_pred, price)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        batch_losses.append(loss.item())\n",
    "        if (step_num % 100) == 0: \n",
    "            print(f'Loss at step {step_num}: {loss.item()}')\n",
    "\n",
    "        # increment the step count \n",
    "        step_num += 1\n",
    "    \n",
    "    return batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3329a7a-c67b-4f9d-bbdd-364b3a805689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one epoch...\n",
      "Loss at step 0: 6723.677734375\n",
      "Loss at step 100: 3818.71826171875\n",
      "Loss at step 200: 2377.814208984375\n",
      "Loss at step 300: 1246.66455078125\n",
      "Loss at step 400: 807.837646484375\n",
      "Loss at step 500: 919.9601440429688\n",
      "Loss at step 600: 347.30743408203125\n",
      "Loss at step 700: 266.62493896484375\n",
      "Loss at step 800: 25694.98046875\n",
      "Loss at step 900: 1278.3765869140625\n",
      "Loss at step 1000: 591.1775512695312\n",
      "Loss at step 1100: 657.5821533203125\n",
      "Loss at step 1200: 228.69334411621094\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One example training epopppch \n",
    "print(\"Training one epoch...\")\n",
    "losses = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(losses, label=\"Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29eb082c-cbea-4f43-8f7d-457c33fe331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the model's total percentage error on a given dataset \n",
    "def evaluate_percent(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_percentage_error = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, price in dataloader:\n",
    "            # Calculate the model's prediction on the given artist, title and numerics\n",
    "            outputs = model(x)\n",
    "            \n",
    "            # Calculate absolute percentage error\n",
    "            abs_percentage_error = torch.abs((outputs - price) / price) * 100\n",
    "            \n",
    "            # Accumulate the sum of percentage errors\n",
    "            total_percentage_error += abs_percentage_error.sum().item()\n",
    "            \n",
    "            # Count the number of samples\n",
    "            num_samples += price.size(0)\n",
    "    \n",
    "    # Calculate the mean percentage error\n",
    "    mean_percentage_error = total_percentage_error / num_samples\n",
    "    return mean_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b45ff947-455f-4e77-8cb2-4c72a9810124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 131, in <module>\n",
      "    dataset = ArtDataset()\n",
      "              ^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 94, in __init__\n",
      "    avg_cosine_similarity_with_museum(title, glove_embeddings) for title in title_col\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 56, in avg_cosine_similarity_with_museum\n",
      "    similarities.append(cosine_similarity(embedding, target_embedding))\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 37, in cosine_similarity\n",
      "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/numpy/linalg/_linalg.py\", line 2717, in norm\n",
      "    x = asarray(x)\n",
      "        ^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError of the model without training on the training set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate_percent(model,\u001b[38;5;250m \u001b[39mtrain_loader,\u001b[38;5;250m \u001b[39mcriterion)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError of the model without training on the validation set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate_percent(model,\u001b[38;5;250m \u001b[39mval_loader,\u001b[38;5;250m \u001b[39mcriterion)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m, in \u001b[0;36mevaluate_percent\u001b[0;34m(model, dataloader, criterion)\u001b[0m\n\u001b[1;32m      5\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, price \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# Calculate the model's prediction on the given artist, title and numerics\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Calculate absolute percentage error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Error of the model without training on the training set: {evaluate_percent(model, train_loader, criterion)}%\")\n",
    "print(f\"Error of the model without training on the validation set: {evaluate_percent(model, val_loader, criterion)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42137855-9170-42de-a068-0e253cab3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average loss of the model \n",
    "def evaluate_loss(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, price in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(artist, title, numerics)\n",
    "            loss = criterion(outputs, price)\n",
    "            \n",
    "            # Accumulate the loss\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4bb2d50c-6c9e-447e-a90e-bc4f17b7cc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Price: $2830.89\n",
      "Mean Price: $31777.83\n",
      "Normalized to MEDIAN\n",
      "Dataset loaded successfully!\n",
      "Median Price: $2830.89\n",
      "Mean Price: $31777.83\n",
      "Normalized to MEDIAN\n",
      "Dataset loaded successfully!\n",
      "Mean Squared Error of the untrained model on Validation Set: 1807.3191605196655\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Squared Error of the untrained model on Validation Set: {evaluate_loss(model, val_loader, criterion)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09a37ef9-218a-4c1e-a67d-e74114349f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 98.10074615478516\n",
      "Loss at step 100: 98.79345703125\n",
      "Loss at step 200: 98.26093292236328\n",
      "Loss at step 300: 98.58424377441406\n",
      "Loss at step 400: 100.46464538574219\n",
      "Loss at step 500: 98.22772979736328\n",
      "Loss at step 600: 99.21451568603516\n",
      "Loss at step 700: 98.0748519897461\n",
      "Loss at step 800: 100.44029998779297\n",
      "Loss at step 900: 102.77926635742188\n",
      "Loss at step 1000: 98.0472412109375\n",
      "Loss at step 1100: 100.29419708251953\n",
      "Loss at step 1200: 98.06096649169922\n",
      "Epoch 1/1000, Avg Train Loss: 99.5037512732941\n",
      "Loss at step 0: 99.05168151855469\n",
      "Loss at step 100: 98.28726196289062\n",
      "Loss at step 200: 99.45365905761719\n",
      "Loss at step 300: 98.72306823730469\n",
      "Loss at step 400: 99.27804565429688\n",
      "Loss at step 500: 100.13745880126953\n",
      "Loss at step 600: 99.2129135131836\n",
      "Loss at step 700: 98.7256088256836\n",
      "Loss at step 800: 98.78205871582031\n",
      "Loss at step 900: 106.23340606689453\n",
      "Loss at step 1000: 98.32015991210938\n",
      "Loss at step 1100: 99.21719360351562\n",
      "Loss at step 1200: 98.65901184082031\n",
      "Epoch 2/1000, Avg Train Loss: 99.46883629980982\n",
      "Loss at step 0: 114.23286437988281\n",
      "Loss at step 100: 102.439208984375\n",
      "Loss at step 200: 99.58062744140625\n",
      "Loss at step 300: 97.89729309082031\n",
      "Loss at step 400: 102.47976684570312\n",
      "Loss at step 500: 99.32899475097656\n",
      "Loss at step 600: 104.85771942138672\n",
      "Loss at step 700: 97.43453216552734\n",
      "Loss at step 800: 99.3169937133789\n",
      "Loss at step 900: 98.27519989013672\n",
      "Loss at step 1000: 100.54927062988281\n",
      "Loss at step 1100: 98.69076538085938\n",
      "Loss at step 1200: 98.28050231933594\n",
      "Epoch 3/1000, Avg Train Loss: 99.44522490393382\n",
      "Loss at step 0: 97.34456634521484\n",
      "Loss at step 100: 97.0862045288086\n",
      "Loss at step 200: 98.4619140625\n",
      "Loss at step 300: 98.35494995117188\n",
      "Loss at step 400: 98.3983154296875\n",
      "Loss at step 500: 98.56233215332031\n",
      "Loss at step 600: 99.3369140625\n",
      "Loss at step 700: 98.52046966552734\n",
      "Loss at step 800: 98.27034759521484\n",
      "Loss at step 900: 99.00274658203125\n",
      "Loss at step 1000: 98.60382843017578\n",
      "Loss at step 1100: 97.78440856933594\n",
      "Loss at step 1200: 99.61937713623047\n",
      "Epoch 4/1000, Avg Train Loss: 99.46256910404341\n",
      "Loss at step 0: 99.0131607055664\n",
      "Loss at step 100: 100.6212387084961\n",
      "Loss at step 200: 98.82461547851562\n",
      "Loss at step 300: 98.47289276123047\n",
      "Loss at step 400: 98.0979232788086\n",
      "Loss at step 500: 98.24420166015625\n",
      "Loss at step 600: 99.54971313476562\n",
      "Loss at step 700: 104.28426361083984\n",
      "Loss at step 800: 99.72852325439453\n",
      "Loss at step 900: 97.3121337890625\n",
      "Loss at step 1000: 98.90181732177734\n",
      "Loss at step 1100: 98.32402801513672\n",
      "Loss at step 1200: 99.46176147460938\n",
      "Epoch 5/1000, Avg Train Loss: 99.42835204732457\n",
      "Loss at step 0: 98.4942398071289\n",
      "Loss at step 100: 98.72637176513672\n",
      "Loss at step 200: 98.92090606689453\n",
      "Loss at step 300: 98.41749572753906\n",
      "Loss at step 400: 98.99203491210938\n",
      "Loss at step 500: 98.173583984375\n",
      "Loss at step 600: 97.7901611328125\n",
      "Loss at step 700: 97.32470703125\n",
      "Loss at step 800: 100.10142517089844\n",
      "Loss at step 900: 99.04168701171875\n",
      "Loss at step 1000: 98.42611694335938\n",
      "Loss at step 1100: 99.71131896972656\n",
      "Loss at step 1200: 97.94708251953125\n",
      "Epoch 6/1000, Avg Train Loss: 99.48612816819868\n",
      "Loss at step 0: 141.63526916503906\n",
      "Loss at step 100: 98.21344757080078\n",
      "Loss at step 200: 98.98770141601562\n",
      "Loss at step 300: 98.9250717163086\n",
      "Loss at step 400: 98.54882049560547\n",
      "Loss at step 500: 98.89657592773438\n",
      "Loss at step 600: 97.97994995117188\n",
      "Loss at step 700: 98.34391784667969\n",
      "Loss at step 800: 98.76106262207031\n",
      "Loss at step 900: 98.21250915527344\n",
      "Loss at step 1000: 99.11611938476562\n",
      "Loss at step 1100: 98.10933685302734\n",
      "Loss at step 1200: 100.84253692626953\n",
      "Epoch 7/1000, Avg Train Loss: 99.42292475777536\n",
      "Loss at step 0: 98.96146392822266\n",
      "Loss at step 100: 98.80977630615234\n",
      "Loss at step 200: 97.79325866699219\n",
      "Loss at step 300: 100.88274383544922\n",
      "Loss at step 400: 99.21597290039062\n",
      "Loss at step 500: 98.96644592285156\n",
      "Loss at step 600: 98.74678039550781\n",
      "Loss at step 700: 98.70135498046875\n",
      "Loss at step 800: 97.43553161621094\n",
      "Loss at step 900: 103.94763946533203\n",
      "Loss at step 1000: 98.6519775390625\n",
      "Loss at step 1100: 97.84947204589844\n",
      "Loss at step 1200: 98.12051391601562\n",
      "Epoch 8/1000, Avg Train Loss: 99.46286013211247\n",
      "Loss at step 0: 98.86344909667969\n",
      "Loss at step 100: 99.3711929321289\n",
      "Loss at step 200: 97.94136810302734\n",
      "Loss at step 300: 98.24544525146484\n",
      "Loss at step 400: 98.06736755371094\n",
      "Loss at step 500: 97.59100341796875\n",
      "Loss at step 600: 98.39726257324219\n",
      "Loss at step 700: 98.98347473144531\n",
      "Loss at step 800: 97.93890380859375\n",
      "Loss at step 900: 99.48346710205078\n",
      "Loss at step 1000: 99.52703857421875\n",
      "Loss at step 1100: 98.78726196289062\n",
      "Loss at step 1200: 98.54627227783203\n",
      "Epoch 9/1000, Avg Train Loss: 99.41444370430264\n",
      "Loss at step 0: 99.83121490478516\n",
      "Loss at step 100: 98.9925308227539\n",
      "Loss at step 200: 101.0064468383789\n",
      "Loss at step 300: 99.73914337158203\n",
      "Loss at step 400: 101.11251831054688\n",
      "Loss at step 500: 99.02278900146484\n",
      "Loss at step 600: 99.90385437011719\n",
      "Loss at step 700: 96.99967956542969\n",
      "Loss at step 800: 102.75259399414062\n",
      "Loss at step 900: 98.01383972167969\n",
      "Loss at step 1000: 99.04361724853516\n",
      "Loss at step 1100: 98.85601806640625\n",
      "Loss at step 1200: 99.3271713256836\n",
      "Epoch 10/1000, Avg Train Loss: 99.47610779796217\n",
      "Loss at step 0: 99.17875671386719\n",
      "Loss at step 100: 98.44140625\n",
      "Loss at step 200: 98.67911529541016\n",
      "Loss at step 300: 97.63858795166016\n",
      "Loss at step 400: 97.77931213378906\n",
      "Loss at step 500: 98.93869018554688\n",
      "Loss at step 600: 100.1363525390625\n",
      "Loss at step 700: 99.42503356933594\n",
      "Loss at step 800: 98.18350982666016\n",
      "Loss at step 900: 98.68586730957031\n",
      "Loss at step 1000: 111.53740692138672\n",
      "Loss at step 1100: 98.603759765625\n",
      "Loss at step 1200: 99.41483306884766\n",
      "Epoch 11/1000, Avg Train Loss: 99.47299106684318\n",
      "Loss at step 0: 100.1561279296875\n",
      "Loss at step 100: 97.95700073242188\n",
      "Loss at step 200: 97.53093719482422\n",
      "Loss at step 300: 98.58195495605469\n",
      "Loss at step 400: 99.18048858642578\n",
      "Loss at step 500: 99.42781829833984\n",
      "Loss at step 600: 97.45802307128906\n",
      "Loss at step 700: 101.16729736328125\n",
      "Loss at step 800: 97.48135375976562\n",
      "Loss at step 900: 99.23318481445312\n",
      "Loss at step 1000: 98.73458099365234\n",
      "Loss at step 1100: 98.7737808227539\n",
      "Loss at step 1200: 99.62895202636719\n",
      "Epoch 12/1000, Avg Train Loss: 99.53472802862765\n",
      "Loss at step 0: 97.47614288330078\n",
      "Loss at step 100: 97.55874633789062\n",
      "Loss at step 200: 99.28455352783203\n",
      "Loss at step 300: 98.33303833007812\n",
      "Loss at step 400: 98.42324829101562\n",
      "Loss at step 500: 100.0600814819336\n",
      "Loss at step 600: 98.93782043457031\n",
      "Loss at step 700: 101.9170150756836\n",
      "Loss at step 800: 98.66405487060547\n",
      "Loss at step 900: 97.26734161376953\n",
      "Loss at step 1000: 98.48590850830078\n",
      "Loss at step 1100: 98.60394287109375\n",
      "Loss at step 1200: 98.83189392089844\n",
      "Epoch 13/1000, Avg Train Loss: 99.48822324561455\n",
      "Loss at step 0: 99.94747924804688\n",
      "Loss at step 100: 98.09644317626953\n",
      "Loss at step 200: 99.15426635742188\n",
      "Loss at step 300: 98.78775787353516\n",
      "Loss at step 400: 97.59427642822266\n",
      "Loss at step 500: 101.1015396118164\n",
      "Loss at step 600: 97.65599060058594\n",
      "Loss at step 700: 100.18489074707031\n",
      "Loss at step 800: 99.46647644042969\n",
      "Loss at step 900: 100.24170684814453\n",
      "Loss at step 1000: 99.81397247314453\n",
      "Loss at step 1100: 108.75226593017578\n",
      "Loss at step 1200: 98.89600372314453\n",
      "Epoch 14/1000, Avg Train Loss: 99.48604854719538\n",
      "Loss at step 0: 98.45006561279297\n",
      "Loss at step 100: 99.45199584960938\n",
      "Loss at step 200: 99.67079162597656\n",
      "Loss at step 300: 98.43930053710938\n",
      "Loss at step 400: 99.7900390625\n",
      "Loss at step 500: 99.46700286865234\n",
      "Loss at step 600: 104.0704345703125\n",
      "Loss at step 700: 97.75191497802734\n",
      "Loss at step 800: 97.50115966796875\n",
      "Loss at step 900: 98.62761688232422\n",
      "Loss at step 1000: 99.64537048339844\n",
      "Loss at step 1100: 100.7491455078125\n",
      "Loss at step 1200: 99.52666473388672\n",
      "Epoch 15/1000, Avg Train Loss: 99.44165056963183\n",
      "Loss at step 0: 98.24034118652344\n",
      "Loss at step 100: 98.66942596435547\n",
      "Loss at step 200: 97.56651306152344\n",
      "Loss at step 300: 103.16036987304688\n",
      "Loss at step 400: 97.57614135742188\n",
      "Loss at step 500: 98.77432250976562\n",
      "Loss at step 600: 99.5000991821289\n",
      "Loss at step 700: 99.56340026855469\n",
      "Loss at step 800: 99.49274444580078\n",
      "Loss at step 900: 104.97257995605469\n",
      "Loss at step 1000: 99.04541778564453\n",
      "Loss at step 1100: 99.21266174316406\n",
      "Loss at step 1200: 97.40316772460938\n",
      "Epoch 16/1000, Avg Train Loss: 99.4985272922948\n",
      "Loss at step 0: 99.8647689819336\n",
      "Loss at step 100: 100.90351104736328\n",
      "Loss at step 200: 98.70329284667969\n",
      "Loss at step 300: 97.96736907958984\n",
      "Loss at step 400: 98.269775390625\n",
      "Loss at step 500: 98.4417495727539\n",
      "Loss at step 600: 98.77893829345703\n",
      "Loss at step 700: 98.84056091308594\n",
      "Loss at step 800: 99.86004638671875\n",
      "Loss at step 900: 100.01834869384766\n",
      "Loss at step 1000: 98.02464294433594\n",
      "Loss at step 1100: 98.030517578125\n",
      "Loss at step 1200: 99.0965347290039\n",
      "Epoch 17/1000, Avg Train Loss: 99.47293956843009\n",
      "Loss at step 0: 99.94055938720703\n",
      "Loss at step 100: 99.4766845703125\n",
      "Loss at step 200: 98.27134704589844\n",
      "Loss at step 300: 100.44068908691406\n",
      "Loss at step 400: 103.3320541381836\n",
      "Loss at step 500: 100.21896362304688\n",
      "Loss at step 600: 98.96259307861328\n",
      "Loss at step 700: 99.22425842285156\n",
      "Loss at step 800: 98.59566497802734\n",
      "Loss at step 900: 100.50862121582031\n",
      "Loss at step 1000: 101.04633331298828\n",
      "Loss at step 1100: 99.0023193359375\n",
      "Loss at step 1200: 97.4617919921875\n",
      "Epoch 18/1000, Avg Train Loss: 99.46285886671937\n",
      "Loss at step 0: 98.89740753173828\n",
      "Loss at step 100: 98.18154907226562\n",
      "Loss at step 200: 99.54273223876953\n",
      "Loss at step 300: 97.49855041503906\n",
      "Loss at step 400: 97.93003845214844\n",
      "Loss at step 500: 98.78839874267578\n",
      "Loss at step 600: 98.40486907958984\n",
      "Loss at step 700: 97.80555725097656\n",
      "Loss at step 800: 100.1824722290039\n",
      "Loss at step 900: 99.06143188476562\n",
      "Loss at step 1000: 102.3854751586914\n",
      "Loss at step 1100: 98.82329559326172\n",
      "Loss at step 1200: 100.38272094726562\n",
      "Epoch 19/1000, Avg Train Loss: 99.42706482773075\n",
      "Loss at step 0: 97.5101089477539\n",
      "Loss at step 100: 97.94361877441406\n",
      "Loss at step 200: 97.93327331542969\n",
      "Loss at step 300: 100.52076721191406\n",
      "Loss at step 400: 102.57169342041016\n",
      "Loss at step 500: 97.72631072998047\n",
      "Loss at step 600: 98.99435424804688\n",
      "Loss at step 700: 98.79563903808594\n",
      "Loss at step 800: 98.53274536132812\n",
      "Loss at step 900: 98.22349548339844\n",
      "Loss at step 1000: 97.6441879272461\n",
      "Loss at step 1100: 99.08625793457031\n",
      "Loss at step 1200: 98.18135833740234\n",
      "Epoch 20/1000, Avg Train Loss: 99.44122899003014\n",
      "Loss at step 0: 99.12835693359375\n",
      "Loss at step 100: 97.6943130493164\n",
      "Loss at step 200: 98.4764404296875\n",
      "Loss at step 300: 99.96211242675781\n",
      "Loss at step 400: 98.29846954345703\n",
      "Loss at step 500: 98.05007934570312\n",
      "Loss at step 600: 98.14898681640625\n",
      "Loss at step 700: 99.32328796386719\n",
      "Loss at step 800: 98.47427368164062\n",
      "Loss at step 900: 98.29568481445312\n",
      "Loss at step 1000: 97.48082733154297\n",
      "Loss at step 1100: 98.1176986694336\n",
      "Loss at step 1200: 98.99456787109375\n",
      "Epoch 21/1000, Avg Train Loss: 99.47912589014541\n",
      "Loss at step 0: 97.72442626953125\n",
      "Loss at step 100: 99.63103485107422\n",
      "Loss at step 200: 97.75177001953125\n",
      "Loss at step 300: 99.62834930419922\n",
      "Loss at step 400: 98.06027221679688\n",
      "Loss at step 500: 99.05145263671875\n",
      "Loss at step 600: 99.391845703125\n",
      "Loss at step 700: 98.08311462402344\n",
      "Loss at step 800: 98.52691650390625\n",
      "Loss at step 900: 99.5116958618164\n",
      "Loss at step 1000: 99.46150970458984\n",
      "Loss at step 1100: 98.76741027832031\n",
      "Loss at step 1200: 102.0557861328125\n",
      "Epoch 22/1000, Avg Train Loss: 99.46737664725788\n",
      "Loss at step 0: 97.6905746459961\n",
      "Loss at step 100: 98.77987670898438\n",
      "Loss at step 200: 99.02568817138672\n",
      "Loss at step 300: 97.87957763671875\n",
      "Loss at step 400: 108.91878509521484\n",
      "Loss at step 500: 102.11531066894531\n",
      "Loss at step 600: 98.46797943115234\n",
      "Loss at step 700: 97.4563217163086\n",
      "Loss at step 800: 99.24320220947266\n",
      "Loss at step 900: 99.574951171875\n",
      "Loss at step 1000: 101.2796630859375\n",
      "Loss at step 1100: 97.8232192993164\n",
      "Loss at step 1200: 98.71271514892578\n",
      "Epoch 23/1000, Avg Train Loss: 99.45325209484903\n",
      "Loss at step 0: 100.22180938720703\n",
      "Loss at step 100: 98.06661987304688\n",
      "Loss at step 200: 99.35404205322266\n",
      "Loss at step 300: 104.18243408203125\n",
      "Loss at step 400: 100.30464935302734\n",
      "Loss at step 500: 98.12364196777344\n",
      "Loss at step 600: 98.24897003173828\n",
      "Loss at step 700: 99.08444213867188\n",
      "Loss at step 800: 97.16275787353516\n",
      "Loss at step 900: 99.15946960449219\n",
      "Loss at step 1000: 105.49732208251953\n",
      "Loss at step 1100: 98.59799194335938\n",
      "Loss at step 1200: 98.46353149414062\n",
      "Epoch 24/1000, Avg Train Loss: 99.46115971537469\n",
      "Loss at step 0: 98.06212615966797\n",
      "Loss at step 100: 98.20603942871094\n",
      "Loss at step 200: 98.34022521972656\n",
      "Loss at step 300: 99.92964935302734\n",
      "Loss at step 400: 98.13383483886719\n",
      "Loss at step 500: 99.40569305419922\n",
      "Loss at step 600: 98.54119873046875\n",
      "Loss at step 700: 103.7475814819336\n",
      "Loss at step 800: 98.74527740478516\n",
      "Loss at step 900: 101.89757537841797\n",
      "Loss at step 1000: 98.66254425048828\n",
      "Loss at step 1100: 98.60515594482422\n",
      "Loss at step 1200: 101.12060546875\n",
      "Epoch 25/1000, Avg Train Loss: 99.45806300215736\n",
      "Loss at step 0: 102.4006118774414\n",
      "Loss at step 100: 99.43841552734375\n",
      "Loss at step 200: 98.27058410644531\n",
      "Loss at step 300: 98.54637145996094\n",
      "Loss at step 400: 100.29180145263672\n",
      "Loss at step 500: 98.84968566894531\n",
      "Loss at step 600: 97.17278289794922\n",
      "Loss at step 700: 97.87705993652344\n",
      "Loss at step 800: 99.25054168701172\n",
      "Loss at step 900: 98.30873107910156\n",
      "Loss at step 1000: 98.96046447753906\n",
      "Loss at step 1100: 98.96443176269531\n",
      "Loss at step 1200: 98.13478088378906\n",
      "Epoch 26/1000, Avg Train Loss: 99.4468757234345\n",
      "Loss at step 0: 98.54478454589844\n",
      "Loss at step 100: 102.07698059082031\n",
      "Loss at step 200: 98.61394500732422\n",
      "Loss at step 300: 101.77571868896484\n",
      "Loss at step 400: 100.00331115722656\n",
      "Loss at step 500: 99.82672119140625\n",
      "Loss at step 600: 99.57244110107422\n",
      "Loss at step 700: 98.10044860839844\n",
      "Loss at step 800: 98.47476196289062\n",
      "Loss at step 900: 99.31490325927734\n",
      "Loss at step 1000: 98.75655364990234\n",
      "Loss at step 1100: 97.9456787109375\n",
      "Loss at step 1200: 99.25175476074219\n",
      "Epoch 27/1000, Avg Train Loss: 99.433611280324\n",
      "Loss at step 0: 100.40066528320312\n",
      "Loss at step 100: 99.36402130126953\n",
      "Loss at step 200: 99.41181945800781\n",
      "Loss at step 300: 98.53585815429688\n",
      "Loss at step 400: 100.47721862792969\n",
      "Loss at step 500: 98.35809326171875\n",
      "Loss at step 600: 98.88029479980469\n",
      "Loss at step 700: 102.03802490234375\n",
      "Loss at step 800: 98.71421813964844\n",
      "Loss at step 900: 98.5200424194336\n",
      "Loss at step 1000: 101.55477905273438\n",
      "Loss at step 1100: 98.01553344726562\n",
      "Loss at step 1200: 100.23763275146484\n",
      "Epoch 28/1000, Avg Train Loss: 99.4386350452707\n",
      "Loss at step 0: 97.99359130859375\n",
      "Loss at step 100: 98.3389892578125\n",
      "Loss at step 200: 97.5726547241211\n",
      "Loss at step 300: 99.7350845336914\n",
      "Loss at step 400: 98.57538604736328\n",
      "Loss at step 500: 99.3022232055664\n",
      "Loss at step 600: 98.26890563964844\n",
      "Loss at step 700: 99.63731384277344\n",
      "Loss at step 800: 97.6798324584961\n",
      "Loss at step 900: 100.24545288085938\n",
      "Loss at step 1000: 99.64424133300781\n",
      "Loss at step 1100: 99.20396423339844\n",
      "Loss at step 1200: 98.48650360107422\n",
      "Epoch 29/1000, Avg Train Loss: 99.4700584658527\n",
      "Loss at step 0: 98.59691619873047\n",
      "Loss at step 100: 97.78205108642578\n",
      "Loss at step 200: 98.37027740478516\n",
      "Loss at step 300: 98.6024169921875\n",
      "Loss at step 400: 99.51134490966797\n",
      "Loss at step 500: 97.43673706054688\n",
      "Loss at step 600: 99.36622619628906\n",
      "Loss at step 700: 98.35298919677734\n",
      "Loss at step 800: 98.54590606689453\n",
      "Loss at step 900: 98.79840850830078\n",
      "Loss at step 1000: 98.19658660888672\n",
      "Loss at step 1100: 98.18494415283203\n",
      "Loss at step 1200: 100.33120727539062\n",
      "Epoch 30/1000, Avg Train Loss: 99.45563094747105\n",
      "Loss at step 0: 97.90569305419922\n",
      "Loss at step 100: 110.45106506347656\n",
      "Loss at step 200: 101.85569763183594\n",
      "Loss at step 300: 98.23934173583984\n",
      "Loss at step 400: 99.09261322021484\n",
      "Loss at step 500: 99.78021240234375\n",
      "Loss at step 600: 99.16265869140625\n",
      "Loss at step 700: 99.17475891113281\n",
      "Loss at step 800: 98.1874008178711\n",
      "Loss at step 900: 98.34503173828125\n",
      "Loss at step 1000: 100.36375427246094\n",
      "Loss at step 1100: 97.75794982910156\n",
      "Loss at step 1200: 97.94051361083984\n",
      "Epoch 31/1000, Avg Train Loss: 99.42674628199111\n",
      "Loss at step 0: 99.3837890625\n",
      "Loss at step 100: 98.03109741210938\n",
      "Loss at step 200: 99.39292907714844\n",
      "Loss at step 300: 99.16211700439453\n",
      "Loss at step 400: 102.11815643310547\n",
      "Loss at step 500: 98.31298828125\n",
      "Loss at step 600: 99.72704315185547\n",
      "Loss at step 700: 98.16100311279297\n",
      "Loss at step 800: 98.4188461303711\n",
      "Loss at step 900: 100.7970199584961\n",
      "Loss at step 1000: 101.93269348144531\n",
      "Loss at step 1100: 98.22516632080078\n",
      "Loss at step 1200: 98.59504699707031\n",
      "Epoch 32/1000, Avg Train Loss: 99.45822993528496\n",
      "Loss at step 0: 98.637939453125\n",
      "Loss at step 100: 101.59630584716797\n",
      "Loss at step 200: 98.32970428466797\n",
      "Loss at step 300: 98.6567611694336\n",
      "Loss at step 400: 98.75106811523438\n",
      "Loss at step 500: 97.89513397216797\n",
      "Loss at step 600: 97.72584533691406\n",
      "Loss at step 700: 98.53086853027344\n",
      "Loss at step 800: 101.87348175048828\n",
      "Loss at step 900: 98.8894271850586\n",
      "Loss at step 1000: 100.0133285522461\n",
      "Loss at step 1100: 102.04215240478516\n",
      "Loss at step 1200: 99.43020629882812\n",
      "Epoch 33/1000, Avg Train Loss: 99.4467975159679\n",
      "Loss at step 0: 98.61031341552734\n",
      "Loss at step 100: 99.65836334228516\n",
      "Loss at step 200: 98.17961120605469\n",
      "Loss at step 300: 99.26546478271484\n",
      "Loss at step 400: 98.98233032226562\n",
      "Loss at step 500: 98.5331802368164\n",
      "Loss at step 600: 99.38782501220703\n",
      "Loss at step 700: 98.4056625366211\n",
      "Loss at step 800: 99.20402526855469\n",
      "Loss at step 900: 97.33985900878906\n",
      "Loss at step 1000: 98.8814926147461\n",
      "Loss at step 1100: 100.825927734375\n",
      "Loss at step 1200: 97.660400390625\n",
      "Epoch 34/1000, Avg Train Loss: 99.42267317293532\n",
      "Loss at step 0: 97.98304748535156\n",
      "Loss at step 100: 98.60765838623047\n",
      "Loss at step 200: 98.97075653076172\n",
      "Loss at step 300: 103.15713500976562\n",
      "Loss at step 400: 101.5303955078125\n",
      "Loss at step 500: 99.63973999023438\n",
      "Loss at step 600: 97.78909301757812\n",
      "Loss at step 700: 100.1015625\n",
      "Loss at step 800: 98.71007537841797\n",
      "Loss at step 900: 98.14318084716797\n",
      "Loss at step 1000: 99.24371337890625\n",
      "Loss at step 1100: 100.51676177978516\n",
      "Loss at step 1200: 98.65686798095703\n",
      "Epoch 35/1000, Avg Train Loss: 99.45516899874295\n",
      "Loss at step 0: 98.16497802734375\n",
      "Loss at step 100: 114.2078857421875\n",
      "Loss at step 200: 98.94596099853516\n",
      "Loss at step 300: 97.61556243896484\n",
      "Loss at step 400: 98.11547088623047\n",
      "Loss at step 500: 97.88378143310547\n",
      "Loss at step 600: 98.15515899658203\n",
      "Loss at step 700: 98.06461334228516\n",
      "Loss at step 800: 97.76700592041016\n",
      "Loss at step 900: 98.26663970947266\n",
      "Loss at step 1000: 97.95858001708984\n",
      "Loss at step 1100: 98.52234649658203\n",
      "Loss at step 1200: 100.52610778808594\n",
      "Epoch 36/1000, Avg Train Loss: 99.4580771930781\n",
      "Loss at step 0: 98.39164733886719\n",
      "Loss at step 100: 98.65692901611328\n",
      "Loss at step 200: 99.0795669555664\n",
      "Loss at step 300: 99.07805633544922\n",
      "Loss at step 400: 98.76710510253906\n",
      "Loss at step 500: 107.67515563964844\n",
      "Loss at step 600: 99.27609252929688\n",
      "Loss at step 700: 100.40541076660156\n",
      "Loss at step 800: 98.45472717285156\n",
      "Loss at step 900: 97.21033477783203\n",
      "Loss at step 1000: 97.77973175048828\n",
      "Loss at step 1100: 99.0875473022461\n",
      "Loss at step 1200: 98.37265014648438\n",
      "Epoch 37/1000, Avg Train Loss: 99.49908500967673\n",
      "Loss at step 0: 100.46589660644531\n",
      "Loss at step 100: 98.75455474853516\n",
      "Loss at step 200: 98.55006408691406\n",
      "Loss at step 300: 97.86255645751953\n",
      "Loss at step 400: 99.36135864257812\n",
      "Loss at step 500: 98.76165008544922\n",
      "Loss at step 600: 103.60933685302734\n",
      "Loss at step 700: 98.6521987915039\n",
      "Loss at step 800: 99.02896118164062\n",
      "Loss at step 900: 99.81732940673828\n",
      "Loss at step 1000: 98.32710266113281\n",
      "Loss at step 1100: 101.20470428466797\n",
      "Loss at step 1200: 98.47252655029297\n",
      "Epoch 38/1000, Avg Train Loss: 99.43994431974046\n",
      "Loss at step 0: 99.40628051757812\n",
      "Loss at step 100: 99.32466888427734\n",
      "Loss at step 200: 100.7136001586914\n",
      "Loss at step 300: 97.93502044677734\n",
      "Loss at step 400: 98.8976058959961\n",
      "Loss at step 500: 98.8103256225586\n",
      "Loss at step 600: 99.94625091552734\n",
      "Loss at step 700: 99.1606674194336\n",
      "Loss at step 800: 99.53104400634766\n",
      "Loss at step 900: 100.71575927734375\n",
      "Loss at step 1000: 98.1390609741211\n",
      "Loss at step 1100: 98.45625305175781\n",
      "Loss at step 1200: 97.8105697631836\n",
      "Epoch 39/1000, Avg Train Loss: 99.47417013544866\n",
      "Loss at step 0: 98.91270446777344\n",
      "Loss at step 100: 98.68671417236328\n",
      "Loss at step 200: 98.78791809082031\n",
      "Loss at step 300: 102.44664001464844\n",
      "Loss at step 400: 97.86602020263672\n",
      "Loss at step 500: 98.18961334228516\n",
      "Loss at step 600: 98.86972045898438\n",
      "Loss at step 700: 100.00497436523438\n",
      "Loss at step 800: 100.5860824584961\n",
      "Loss at step 900: 97.9542465209961\n",
      "Loss at step 1000: 98.16124725341797\n",
      "Loss at step 1100: 99.24856567382812\n",
      "Loss at step 1200: 99.34829711914062\n",
      "Epoch 40/1000, Avg Train Loss: 99.44508047011293\n",
      "Loss at step 0: 99.19375610351562\n",
      "Loss at step 100: 110.71176147460938\n",
      "Loss at step 200: 98.7552261352539\n",
      "Loss at step 300: 99.35215759277344\n",
      "Loss at step 400: 97.55567169189453\n",
      "Loss at step 500: 101.13877868652344\n",
      "Loss at step 600: 98.4521484375\n",
      "Loss at step 700: 97.915771484375\n",
      "Loss at step 800: 98.27159881591797\n",
      "Loss at step 900: 97.86834716796875\n",
      "Loss at step 1000: 100.96926879882812\n",
      "Loss at step 1100: 98.26795959472656\n",
      "Loss at step 1200: 99.13955688476562\n",
      "Epoch 41/1000, Avg Train Loss: 99.50756920502795\n",
      "Loss at step 0: 98.6996078491211\n",
      "Loss at step 100: 102.30033111572266\n",
      "Loss at step 200: 101.54257202148438\n",
      "Loss at step 300: 126.94248962402344\n",
      "Loss at step 400: 100.76255798339844\n",
      "Loss at step 500: 97.80358123779297\n",
      "Loss at step 600: 102.35299682617188\n",
      "Loss at step 700: 98.258056640625\n",
      "Loss at step 800: 97.92403411865234\n",
      "Loss at step 900: 98.28996276855469\n",
      "Loss at step 1000: 98.6397705078125\n",
      "Loss at step 1100: 98.35067749023438\n",
      "Loss at step 1200: 99.86026763916016\n",
      "Epoch 42/1000, Avg Train Loss: 99.42712252965637\n",
      "Loss at step 0: 98.6053695678711\n",
      "Loss at step 100: 99.56912231445312\n",
      "Loss at step 200: 101.9044189453125\n",
      "Loss at step 300: 98.80606842041016\n",
      "Loss at step 400: 99.07303619384766\n",
      "Loss at step 500: 98.46794891357422\n",
      "Loss at step 600: 98.70370483398438\n",
      "Loss at step 700: 100.8494644165039\n",
      "Loss at step 800: 97.80410766601562\n",
      "Loss at step 900: 98.42881774902344\n",
      "Loss at step 1000: 98.61394500732422\n",
      "Loss at step 1100: 98.77910614013672\n",
      "Loss at step 1200: 97.83622741699219\n",
      "Epoch 43/1000, Avg Train Loss: 99.44345425479234\n",
      "Loss at step 0: 97.78385925292969\n",
      "Loss at step 100: 98.24397277832031\n",
      "Loss at step 200: 99.45404815673828\n",
      "Loss at step 300: 98.503173828125\n",
      "Loss at step 400: 98.12379455566406\n",
      "Loss at step 500: 99.30636596679688\n",
      "Loss at step 600: 99.23616790771484\n",
      "Loss at step 700: 99.63319396972656\n",
      "Loss at step 800: 98.39860534667969\n",
      "Loss at step 900: 98.66447448730469\n",
      "Loss at step 1000: 97.10774230957031\n",
      "Loss at step 1100: 98.3138656616211\n",
      "Loss at step 1200: 98.3145523071289\n",
      "Epoch 44/1000, Avg Train Loss: 99.44998848554\n",
      "Loss at step 0: 98.10530090332031\n",
      "Loss at step 100: 99.06636810302734\n",
      "Loss at step 200: 100.58454132080078\n",
      "Loss at step 300: 114.12024688720703\n",
      "Loss at step 400: 98.9437484741211\n",
      "Loss at step 500: 98.1600570678711\n",
      "Loss at step 600: 98.4561538696289\n",
      "Loss at step 700: 97.06376647949219\n",
      "Loss at step 800: 99.59782409667969\n",
      "Loss at step 900: 98.71459197998047\n",
      "Loss at step 1000: 99.07840728759766\n",
      "Loss at step 1100: 100.54173278808594\n",
      "Loss at step 1200: 98.38411712646484\n",
      "Epoch 45/1000, Avg Train Loss: 99.4256345631621\n",
      "Loss at step 0: 96.28178405761719\n",
      "Loss at step 100: 97.07140350341797\n",
      "Loss at step 200: 101.06942749023438\n",
      "Loss at step 300: 98.3062744140625\n",
      "Loss at step 400: 99.4752197265625\n",
      "Loss at step 500: 99.40460968017578\n",
      "Loss at step 600: 98.83746337890625\n",
      "Loss at step 700: 111.50115966796875\n",
      "Loss at step 800: 100.54440307617188\n",
      "Loss at step 900: 98.3917465209961\n",
      "Loss at step 1000: 99.28646850585938\n",
      "Loss at step 1100: 98.24394989013672\n",
      "Loss at step 1200: 98.3753662109375\n",
      "Epoch 46/1000, Avg Train Loss: 99.45581952807973\n",
      "Loss at step 0: 98.02220153808594\n",
      "Loss at step 100: 98.2580337524414\n",
      "Loss at step 200: 98.42300415039062\n",
      "Loss at step 300: 98.4532470703125\n",
      "Loss at step 400: 98.86590576171875\n",
      "Loss at step 500: 99.00960540771484\n",
      "Loss at step 600: 99.35287475585938\n",
      "Loss at step 700: 137.04510498046875\n",
      "Loss at step 800: 98.34537506103516\n",
      "Loss at step 900: 101.07453155517578\n",
      "Loss at step 1000: 99.68952178955078\n",
      "Loss at step 1100: 98.59769439697266\n",
      "Loss at step 1200: 98.30967712402344\n",
      "Epoch 47/1000, Avg Train Loss: 99.46898236321015\n",
      "Loss at step 0: 106.2591552734375\n",
      "Loss at step 100: 98.8019790649414\n",
      "Loss at step 200: 99.86600494384766\n",
      "Loss at step 300: 98.84996032714844\n",
      "Loss at step 400: 98.91280364990234\n",
      "Loss at step 500: 98.29605865478516\n",
      "Loss at step 600: 98.65702056884766\n",
      "Loss at step 700: 98.12916564941406\n",
      "Loss at step 800: 97.84873962402344\n",
      "Loss at step 900: 99.7775650024414\n",
      "Loss at step 1000: 98.3382568359375\n",
      "Loss at step 1100: 99.6768569946289\n",
      "Loss at step 1200: 98.25503540039062\n",
      "Epoch 48/1000, Avg Train Loss: 99.4057613545637\n",
      "Loss at step 0: 97.57398223876953\n",
      "Loss at step 100: 98.194091796875\n",
      "Loss at step 200: 100.55370330810547\n",
      "Loss at step 300: 100.33272552490234\n",
      "Loss at step 400: 99.02799987792969\n",
      "Loss at step 500: 101.51309967041016\n",
      "Loss at step 600: 97.76276397705078\n",
      "Loss at step 700: 99.5050277709961\n",
      "Loss at step 800: 99.36944580078125\n",
      "Loss at step 900: 97.64170837402344\n",
      "Loss at step 1000: 98.28885650634766\n",
      "Loss at step 1100: 98.23766326904297\n",
      "Loss at step 1200: 98.84506225585938\n",
      "Epoch 49/1000, Avg Train Loss: 99.44009870387204\n",
      "Loss at step 0: 99.27740478515625\n",
      "Loss at step 100: 100.0703353881836\n",
      "Loss at step 200: 98.02869415283203\n",
      "Loss at step 300: 98.0995864868164\n",
      "Loss at step 400: 99.7245101928711\n",
      "Loss at step 500: 98.02685546875\n",
      "Loss at step 600: 98.7535400390625\n",
      "Loss at step 700: 98.19772338867188\n",
      "Loss at step 800: 98.09851837158203\n",
      "Loss at step 900: 98.53304290771484\n",
      "Loss at step 1000: 98.137451171875\n",
      "Loss at step 1100: 99.3825454711914\n",
      "Loss at step 1200: 97.78775787353516\n",
      "Epoch 50/1000, Avg Train Loss: 99.46230198264507\n",
      "Loss at step 0: 97.73348999023438\n",
      "Loss at step 100: 99.0075454711914\n",
      "Loss at step 200: 97.74759674072266\n",
      "Loss at step 300: 97.9836196899414\n",
      "Loss at step 400: 99.75434112548828\n",
      "Loss at step 500: 97.8183822631836\n",
      "Loss at step 600: 97.87842559814453\n",
      "Loss at step 700: 98.88914489746094\n",
      "Loss at step 800: 99.17980194091797\n",
      "Loss at step 900: 104.00543212890625\n",
      "Loss at step 1000: 100.06494140625\n",
      "Loss at step 1100: 98.10294342041016\n",
      "Loss at step 1200: 100.63001251220703\n",
      "Epoch 51/1000, Avg Train Loss: 99.44914904227149\n",
      "Loss at step 0: 100.4892807006836\n",
      "Loss at step 100: 98.98336791992188\n",
      "Loss at step 200: 99.1624526977539\n",
      "Loss at step 300: 99.36771392822266\n",
      "Loss at step 400: 98.50675201416016\n",
      "Loss at step 500: 106.13587951660156\n",
      "Loss at step 600: 98.39366912841797\n",
      "Loss at step 700: 99.08538818359375\n",
      "Loss at step 800: 97.7205810546875\n",
      "Loss at step 900: 98.57138061523438\n",
      "Loss at step 1000: 100.92027282714844\n",
      "Loss at step 1100: 99.6214599609375\n",
      "Loss at step 1200: 99.10322570800781\n",
      "Epoch 52/1000, Avg Train Loss: 99.47545126881029\n",
      "Loss at step 0: 100.66930389404297\n",
      "Loss at step 100: 100.81670379638672\n",
      "Loss at step 200: 107.58322143554688\n",
      "Loss at step 300: 103.24639892578125\n",
      "Loss at step 400: 100.62296295166016\n",
      "Loss at step 500: 98.03826141357422\n",
      "Loss at step 600: 98.64543151855469\n",
      "Loss at step 700: 97.60105895996094\n",
      "Loss at step 800: 98.32669830322266\n",
      "Loss at step 900: 99.40608978271484\n",
      "Loss at step 1000: 99.68589782714844\n",
      "Loss at step 1100: 99.38969421386719\n",
      "Loss at step 1200: 97.97718811035156\n",
      "Epoch 53/1000, Avg Train Loss: 99.41638187297339\n",
      "Loss at step 0: 107.66014099121094\n",
      "Loss at step 100: 98.51178741455078\n",
      "Loss at step 200: 97.62940216064453\n",
      "Loss at step 300: 101.9118881225586\n",
      "Loss at step 400: 98.51993560791016\n",
      "Loss at step 500: 98.85765075683594\n",
      "Loss at step 600: 97.94084167480469\n",
      "Loss at step 700: 97.99211883544922\n",
      "Loss at step 800: 98.874267578125\n",
      "Loss at step 900: 110.64353942871094\n",
      "Loss at step 1000: 99.13665008544922\n",
      "Loss at step 1100: 101.55011749267578\n",
      "Loss at step 1200: 97.49334716796875\n",
      "Epoch 54/1000, Avg Train Loss: 99.43539313282396\n",
      "Loss at step 0: 99.3019790649414\n",
      "Loss at step 100: 99.64401245117188\n",
      "Loss at step 200: 99.62215423583984\n",
      "Loss at step 300: 99.10325622558594\n",
      "Loss at step 400: 97.93585205078125\n",
      "Loss at step 500: 98.4735107421875\n",
      "Loss at step 600: 100.17906188964844\n",
      "Loss at step 700: 98.9366683959961\n",
      "Loss at step 800: 99.13821411132812\n",
      "Loss at step 900: 99.71554565429688\n",
      "Loss at step 1000: 98.35212707519531\n",
      "Loss at step 1100: 98.50279235839844\n",
      "Loss at step 1200: 99.0174560546875\n",
      "Epoch 55/1000, Avg Train Loss: 99.46237546803496\n",
      "Loss at step 0: 101.3294906616211\n",
      "Loss at step 100: 98.93250274658203\n",
      "Loss at step 200: 98.11021423339844\n",
      "Loss at step 300: 97.90863800048828\n",
      "Loss at step 400: 98.32942199707031\n",
      "Loss at step 500: 99.02523040771484\n",
      "Loss at step 600: 98.15682983398438\n",
      "Loss at step 700: 97.18860626220703\n",
      "Loss at step 800: 99.71870422363281\n",
      "Loss at step 900: 98.43714141845703\n",
      "Loss at step 1000: 98.4264907836914\n",
      "Loss at step 1100: 99.59746551513672\n",
      "Loss at step 1200: 99.02873992919922\n",
      "Epoch 56/1000, Avg Train Loss: 99.45670779083153\n",
      "Loss at step 0: 98.5643081665039\n",
      "Loss at step 100: 100.79517364501953\n",
      "Loss at step 200: 100.71900177001953\n",
      "Loss at step 300: 106.54450988769531\n",
      "Loss at step 400: 99.89337921142578\n",
      "Loss at step 500: 100.74293518066406\n",
      "Loss at step 600: 98.04653930664062\n",
      "Loss at step 700: 99.26766204833984\n",
      "Loss at step 800: 100.77110290527344\n",
      "Loss at step 900: 97.94596099853516\n",
      "Loss at step 1000: 96.74168395996094\n",
      "Loss at step 1100: 99.66921997070312\n",
      "Loss at step 1200: 99.64181518554688\n",
      "Epoch 57/1000, Avg Train Loss: 99.4440732789271\n",
      "Loss at step 0: 98.70718383789062\n",
      "Loss at step 100: 98.64085388183594\n",
      "Loss at step 200: 98.6878890991211\n",
      "Loss at step 300: 99.82596588134766\n",
      "Loss at step 400: 99.0514907836914\n",
      "Loss at step 500: 97.16020965576172\n",
      "Loss at step 600: 101.30632019042969\n",
      "Loss at step 700: 99.17585754394531\n",
      "Loss at step 800: 113.15746307373047\n",
      "Loss at step 900: 101.80062866210938\n",
      "Loss at step 1000: 101.509521484375\n",
      "Loss at step 1100: 98.95843505859375\n",
      "Loss at step 1200: 99.11322784423828\n",
      "Epoch 58/1000, Avg Train Loss: 99.4170097672052\n",
      "Loss at step 0: 100.45968627929688\n",
      "Loss at step 100: 98.04994201660156\n",
      "Loss at step 200: 99.12348175048828\n",
      "Loss at step 300: 98.9128646850586\n",
      "Loss at step 400: 98.84819793701172\n",
      "Loss at step 500: 99.12712860107422\n",
      "Loss at step 600: 97.22896575927734\n",
      "Loss at step 700: 98.4595718383789\n",
      "Loss at step 800: 99.24711608886719\n",
      "Loss at step 900: 106.56897735595703\n",
      "Loss at step 1000: 97.61797332763672\n",
      "Loss at step 1100: 98.01626586914062\n",
      "Loss at step 1200: 99.81167602539062\n",
      "Epoch 59/1000, Avg Train Loss: 99.42930988817925\n",
      "Loss at step 0: 97.52667236328125\n",
      "Loss at step 100: 97.8297348022461\n",
      "Loss at step 200: 98.29971313476562\n",
      "Loss at step 300: 100.3984603881836\n",
      "Loss at step 400: 100.29608154296875\n",
      "Loss at step 500: 98.57195281982422\n",
      "Loss at step 600: 99.14669036865234\n",
      "Loss at step 700: 101.72979736328125\n",
      "Loss at step 800: 98.61697387695312\n",
      "Loss at step 900: 98.65692138671875\n",
      "Loss at step 1000: 98.99244689941406\n",
      "Loss at step 1100: 98.33063507080078\n",
      "Loss at step 1200: 100.71995544433594\n",
      "Epoch 60/1000, Avg Train Loss: 99.43740569426404\n",
      "Loss at step 0: 98.78730773925781\n",
      "Loss at step 100: 98.37648010253906\n",
      "Loss at step 200: 109.54058074951172\n",
      "Loss at step 300: 98.97640991210938\n",
      "Loss at step 400: 99.04342651367188\n",
      "Loss at step 500: 99.08331298828125\n",
      "Loss at step 600: 98.87004852294922\n",
      "Loss at step 700: 99.25273132324219\n",
      "Loss at step 800: 99.64693450927734\n",
      "Loss at step 900: 103.13658905029297\n",
      "Loss at step 1000: 98.65816497802734\n",
      "Loss at step 1100: 99.38893127441406\n",
      "Loss at step 1200: 97.7468490600586\n",
      "Epoch 61/1000, Avg Train Loss: 99.42751253615691\n",
      "Loss at step 0: 98.47123718261719\n",
      "Loss at step 100: 99.29669952392578\n",
      "Loss at step 200: 98.38116455078125\n",
      "Loss at step 300: 99.22476196289062\n",
      "Loss at step 400: 98.46267700195312\n",
      "Loss at step 500: 98.94880676269531\n",
      "Loss at step 600: 98.67288208007812\n",
      "Loss at step 700: 98.01404571533203\n",
      "Loss at step 800: 98.54190826416016\n",
      "Loss at step 900: 98.55596923828125\n",
      "Loss at step 1000: 97.90408325195312\n",
      "Loss at step 1100: 98.28266143798828\n",
      "Loss at step 1200: 104.20854187011719\n",
      "Epoch 62/1000, Avg Train Loss: 99.43679515134941\n",
      "Loss at step 0: 99.21049499511719\n",
      "Loss at step 100: 98.85072326660156\n",
      "Loss at step 200: 99.20178985595703\n",
      "Loss at step 300: 101.64790344238281\n",
      "Loss at step 400: 98.18375396728516\n",
      "Loss at step 500: 99.13944244384766\n",
      "Loss at step 600: 98.7238998413086\n",
      "Loss at step 700: 98.22604370117188\n",
      "Loss at step 800: 98.80028533935547\n",
      "Loss at step 900: 98.58857727050781\n",
      "Loss at step 1000: 98.73040771484375\n",
      "Loss at step 1100: 98.8393783569336\n",
      "Loss at step 1200: 98.40071105957031\n",
      "Epoch 63/1000, Avg Train Loss: 99.40649670844711\n",
      "Loss at step 0: 99.6985092163086\n",
      "Loss at step 100: 97.88488006591797\n",
      "Loss at step 200: 98.63440704345703\n",
      "Loss at step 300: 99.13846588134766\n",
      "Loss at step 400: 98.10250091552734\n",
      "Loss at step 500: 97.99559020996094\n",
      "Loss at step 600: 98.21595764160156\n",
      "Loss at step 700: 107.27114868164062\n",
      "Loss at step 800: 98.12382507324219\n",
      "Loss at step 900: 97.79143524169922\n",
      "Loss at step 1000: 98.0141372680664\n",
      "Loss at step 1100: 98.58341217041016\n",
      "Loss at step 1200: 102.86174774169922\n",
      "Epoch 64/1000, Avg Train Loss: 99.41156694109772\n",
      "Loss at step 0: 98.6075668334961\n",
      "Loss at step 100: 98.9782943725586\n",
      "Loss at step 200: 99.66950225830078\n",
      "Loss at step 300: 97.99374389648438\n",
      "Loss at step 400: 98.25167846679688\n",
      "Loss at step 500: 98.84135437011719\n",
      "Loss at step 600: 98.36784362792969\n",
      "Loss at step 700: 98.79557037353516\n",
      "Loss at step 800: 99.63021087646484\n",
      "Loss at step 900: 99.31880950927734\n",
      "Loss at step 1000: 101.17247009277344\n",
      "Loss at step 1100: 99.29360961914062\n",
      "Loss at step 1200: 100.53653717041016\n",
      "Epoch 65/1000, Avg Train Loss: 99.44287207520124\n",
      "Loss at step 0: 99.52576446533203\n",
      "Loss at step 100: 98.81497192382812\n",
      "Loss at step 200: 98.84320831298828\n",
      "Loss at step 300: 105.08384704589844\n",
      "Loss at step 400: 99.861083984375\n",
      "Loss at step 500: 98.16633605957031\n",
      "Loss at step 600: 98.2879638671875\n",
      "Loss at step 700: 103.33425903320312\n",
      "Loss at step 800: 98.84009552001953\n",
      "Loss at step 900: 100.19700622558594\n",
      "Loss at step 1000: 99.15862274169922\n",
      "Loss at step 1100: 98.2445068359375\n",
      "Loss at step 1200: 98.0272216796875\n",
      "Epoch 66/1000, Avg Train Loss: 99.4579867637659\n",
      "Loss at step 0: 97.8437271118164\n",
      "Loss at step 100: 98.48939514160156\n",
      "Loss at step 200: 100.86617279052734\n",
      "Loss at step 300: 98.74220275878906\n",
      "Loss at step 400: 99.16051483154297\n",
      "Loss at step 500: 98.88088989257812\n",
      "Loss at step 600: 100.57508087158203\n",
      "Loss at step 700: 104.82201385498047\n",
      "Loss at step 800: 99.09935760498047\n",
      "Loss at step 900: 98.28642272949219\n",
      "Loss at step 1000: 99.85662841796875\n",
      "Loss at step 1100: 96.77115631103516\n",
      "Loss at step 1200: 98.73966217041016\n",
      "Epoch 67/1000, Avg Train Loss: 99.43426224791888\n",
      "Loss at step 0: 98.48119354248047\n",
      "Loss at step 100: 98.65690612792969\n",
      "Loss at step 200: 98.41978454589844\n",
      "Loss at step 300: 99.09471130371094\n",
      "Loss at step 400: 98.5538101196289\n",
      "Loss at step 500: 99.51422119140625\n",
      "Loss at step 600: 99.79293060302734\n",
      "Loss at step 700: 98.98694610595703\n",
      "Loss at step 800: 98.59284973144531\n",
      "Loss at step 900: 98.61383819580078\n",
      "Loss at step 1000: 98.25523376464844\n",
      "Loss at step 1100: 97.8546142578125\n",
      "Loss at step 1200: 98.14083862304688\n",
      "Epoch 68/1000, Avg Train Loss: 99.44162310134246\n",
      "Loss at step 0: 98.8870620727539\n",
      "Loss at step 100: 98.67996215820312\n",
      "Loss at step 200: 101.4001235961914\n",
      "Loss at step 300: 97.38027954101562\n",
      "Loss at step 400: 98.60700225830078\n",
      "Loss at step 500: 98.7277603149414\n",
      "Loss at step 600: 98.19330596923828\n",
      "Loss at step 700: 98.58353424072266\n",
      "Loss at step 800: 98.30986022949219\n",
      "Loss at step 900: 98.57737731933594\n",
      "Loss at step 1000: 99.19538116455078\n",
      "Loss at step 1100: 99.06759643554688\n",
      "Loss at step 1200: 96.8539810180664\n",
      "Epoch 69/1000, Avg Train Loss: 99.43589572844768\n",
      "Loss at step 0: 103.8797607421875\n",
      "Loss at step 100: 98.69597625732422\n",
      "Loss at step 200: 98.60078430175781\n",
      "Loss at step 300: 98.96257019042969\n",
      "Loss at step 400: 98.71923828125\n",
      "Loss at step 500: 98.92364501953125\n",
      "Loss at step 600: 101.13008880615234\n",
      "Loss at step 700: 98.89549255371094\n",
      "Loss at step 800: 99.94295501708984\n",
      "Loss at step 900: 98.1689453125\n",
      "Loss at step 1000: 98.31969451904297\n",
      "Loss at step 1100: 99.15044403076172\n",
      "Loss at step 1200: 99.17000579833984\n",
      "Epoch 70/1000, Avg Train Loss: 99.4294973885743\n",
      "Loss at step 0: 98.7234115600586\n",
      "Loss at step 100: 100.60236358642578\n",
      "Loss at step 200: 98.28243255615234\n",
      "Loss at step 300: 98.18630981445312\n",
      "Loss at step 400: 99.9829330444336\n",
      "Loss at step 500: 100.12764739990234\n",
      "Loss at step 600: 99.19029235839844\n",
      "Loss at step 700: 98.08318328857422\n",
      "Loss at step 800: 99.52753448486328\n",
      "Loss at step 900: 98.45933532714844\n",
      "Loss at step 1000: 100.79930114746094\n",
      "Loss at step 1100: 101.84408569335938\n",
      "Loss at step 1200: 98.59561920166016\n",
      "Epoch 71/1000, Avg Train Loss: 99.4321880710935\n",
      "Loss at step 0: 97.55937194824219\n",
      "Loss at step 100: 98.5306625366211\n",
      "Loss at step 200: 97.75355529785156\n",
      "Loss at step 300: 98.6589584350586\n",
      "Loss at step 400: 98.38752746582031\n",
      "Loss at step 500: 98.42035675048828\n",
      "Loss at step 600: 99.66165924072266\n",
      "Loss at step 700: 97.84738159179688\n",
      "Loss at step 800: 116.8681640625\n",
      "Loss at step 900: 98.3364486694336\n",
      "Loss at step 1000: 98.17674255371094\n",
      "Loss at step 1100: 97.13715362548828\n",
      "Loss at step 1200: 98.61480712890625\n",
      "Epoch 72/1000, Avg Train Loss: 99.43992644992075\n",
      "Loss at step 0: 98.2231216430664\n",
      "Loss at step 100: 100.14313507080078\n",
      "Loss at step 200: 98.2308349609375\n",
      "Loss at step 300: 100.27993774414062\n",
      "Loss at step 400: 98.85165405273438\n",
      "Loss at step 500: 98.47093963623047\n",
      "Loss at step 600: 99.4357681274414\n",
      "Loss at step 700: 98.4188461303711\n",
      "Loss at step 800: 98.61775207519531\n",
      "Loss at step 900: 97.93067169189453\n",
      "Loss at step 1000: 98.78480529785156\n",
      "Loss at step 1100: 110.22193145751953\n",
      "Loss at step 1200: 97.87635803222656\n",
      "Epoch 73/1000, Avg Train Loss: 99.41308041297889\n",
      "Loss at step 0: 100.6755599975586\n",
      "Loss at step 100: 105.77770233154297\n",
      "Loss at step 200: 98.87620544433594\n",
      "Loss at step 300: 99.72442626953125\n",
      "Loss at step 400: 99.22628784179688\n",
      "Loss at step 500: 101.10076904296875\n",
      "Loss at step 600: 100.46393585205078\n",
      "Loss at step 700: 98.73639678955078\n",
      "Loss at step 800: 98.02604675292969\n",
      "Loss at step 900: 98.98796081542969\n",
      "Loss at step 1000: 112.4206314086914\n",
      "Loss at step 1100: 98.16779327392578\n",
      "Loss at step 1200: 98.88420867919922\n",
      "Epoch 74/1000, Avg Train Loss: 99.42486523501695\n",
      "Loss at step 0: 98.67688751220703\n",
      "Loss at step 100: 100.49031829833984\n",
      "Loss at step 200: 98.85292053222656\n",
      "Loss at step 300: 99.5367660522461\n",
      "Loss at step 400: 97.79436492919922\n",
      "Loss at step 500: 98.05934143066406\n",
      "Loss at step 600: 98.28865814208984\n",
      "Loss at step 700: 98.56121826171875\n",
      "Loss at step 800: 99.006103515625\n",
      "Loss at step 900: 100.02771759033203\n",
      "Loss at step 1000: 99.67981719970703\n",
      "Loss at step 1100: 100.42724609375\n",
      "Loss at step 1200: 98.03364562988281\n",
      "Epoch 75/1000, Avg Train Loss: 99.48790203329042\n",
      "Loss at step 0: 97.32176971435547\n",
      "Loss at step 100: 98.73907470703125\n",
      "Loss at step 200: 97.75594329833984\n",
      "Loss at step 300: 105.06129455566406\n",
      "Loss at step 400: 98.74032592773438\n",
      "Loss at step 500: 98.9732894897461\n",
      "Loss at step 600: 99.8351821899414\n",
      "Loss at step 700: 98.75724029541016\n",
      "Loss at step 800: 100.5594482421875\n",
      "Loss at step 900: 98.46807861328125\n",
      "Loss at step 1000: 99.25524139404297\n",
      "Loss at step 1100: 99.16704559326172\n",
      "Loss at step 1200: 98.96434783935547\n",
      "Epoch 76/1000, Avg Train Loss: 99.45824902728923\n",
      "Loss at step 0: 98.9895248413086\n",
      "Loss at step 100: 98.56157684326172\n",
      "Loss at step 200: 105.16969299316406\n",
      "Loss at step 300: 101.78337097167969\n",
      "Loss at step 400: 98.81460571289062\n",
      "Loss at step 500: 100.19947052001953\n",
      "Loss at step 600: 100.47982025146484\n",
      "Loss at step 700: 102.51153564453125\n",
      "Loss at step 800: 98.1823959350586\n",
      "Loss at step 900: 97.53411102294922\n",
      "Loss at step 1000: 97.95339965820312\n",
      "Loss at step 1100: 98.56544494628906\n",
      "Loss at step 1200: 99.75027465820312\n",
      "Epoch 77/1000, Avg Train Loss: 99.41628882028525\n",
      "Loss at step 0: 98.39795684814453\n",
      "Loss at step 100: 99.07670593261719\n",
      "Loss at step 200: 98.77902221679688\n",
      "Loss at step 300: 98.40713500976562\n",
      "Loss at step 400: 100.52392578125\n",
      "Loss at step 500: 98.71588897705078\n",
      "Loss at step 600: 99.38982391357422\n",
      "Loss at step 700: 97.8807601928711\n",
      "Loss at step 800: 101.98744201660156\n",
      "Loss at step 900: 100.5329818725586\n",
      "Loss at step 1000: 98.73070526123047\n",
      "Loss at step 1100: 100.1159896850586\n",
      "Loss at step 1200: 99.48816680908203\n",
      "Epoch 78/1000, Avg Train Loss: 99.43186795132831\n",
      "Loss at step 0: 98.0963134765625\n",
      "Loss at step 100: 98.96920013427734\n",
      "Loss at step 200: 100.55787658691406\n",
      "Loss at step 300: 98.81082916259766\n",
      "Loss at step 400: 98.9135971069336\n",
      "Loss at step 500: 99.23900604248047\n",
      "Loss at step 600: 98.3319320678711\n",
      "Loss at step 700: 98.50398254394531\n",
      "Loss at step 800: 98.80925750732422\n",
      "Loss at step 900: 105.73954772949219\n",
      "Loss at step 1000: 100.83699035644531\n",
      "Loss at step 1100: 101.8339614868164\n",
      "Loss at step 1200: 98.65959167480469\n",
      "Epoch 79/1000, Avg Train Loss: 99.48651277363108\n",
      "Loss at step 0: 98.51629638671875\n",
      "Loss at step 100: 98.354248046875\n",
      "Loss at step 200: 98.360107421875\n",
      "Loss at step 300: 98.79512786865234\n",
      "Loss at step 400: 98.36304473876953\n",
      "Loss at step 500: 97.5051040649414\n",
      "Loss at step 600: 97.1113052368164\n",
      "Loss at step 700: 98.11669921875\n",
      "Loss at step 800: 98.83967590332031\n",
      "Loss at step 900: 99.62957763671875\n",
      "Loss at step 1000: 101.38130187988281\n",
      "Loss at step 1100: 99.21442413330078\n",
      "Loss at step 1200: 97.94268798828125\n",
      "Epoch 80/1000, Avg Train Loss: 99.44717452286902\n",
      "Loss at step 0: 99.33528900146484\n",
      "Loss at step 100: 98.76969909667969\n",
      "Loss at step 200: 98.90849304199219\n",
      "Loss at step 300: 98.97425842285156\n",
      "Loss at step 400: 99.98025512695312\n",
      "Loss at step 500: 98.8577651977539\n",
      "Loss at step 600: 98.38522338867188\n",
      "Loss at step 700: 100.7818832397461\n",
      "Loss at step 800: 98.30245208740234\n",
      "Loss at step 900: 99.23521423339844\n",
      "Loss at step 1000: 100.87805938720703\n",
      "Loss at step 1100: 99.09082794189453\n",
      "Loss at step 1200: 98.05953216552734\n",
      "Epoch 81/1000, Avg Train Loss: 99.42770832641996\n",
      "Loss at step 0: 98.31246948242188\n",
      "Loss at step 100: 98.43489837646484\n",
      "Loss at step 200: 99.69950866699219\n",
      "Loss at step 300: 99.35357666015625\n",
      "Loss at step 400: 97.49585723876953\n",
      "Loss at step 500: 101.07447814941406\n",
      "Loss at step 600: 99.22078704833984\n",
      "Loss at step 700: 99.77906036376953\n",
      "Loss at step 800: 98.17528533935547\n",
      "Loss at step 900: 98.03836059570312\n",
      "Loss at step 1000: 97.93122863769531\n",
      "Loss at step 1100: 97.32240295410156\n",
      "Loss at step 1200: 98.90135192871094\n",
      "Epoch 82/1000, Avg Train Loss: 99.44832160480586\n",
      "Loss at step 0: 99.40995788574219\n",
      "Loss at step 100: 101.36146545410156\n",
      "Loss at step 200: 98.04304504394531\n",
      "Loss at step 300: 98.65473175048828\n",
      "Loss at step 400: 98.51368713378906\n",
      "Loss at step 500: 100.2283935546875\n",
      "Loss at step 600: 99.2270736694336\n",
      "Loss at step 700: 98.5230941772461\n",
      "Loss at step 800: 101.16036224365234\n",
      "Loss at step 900: 97.6231689453125\n",
      "Loss at step 1000: 100.40694427490234\n",
      "Loss at step 1100: 98.76478576660156\n",
      "Loss at step 1200: 97.92386627197266\n",
      "Epoch 83/1000, Avg Train Loss: 99.420399261524\n",
      "Loss at step 0: 98.51710510253906\n",
      "Loss at step 100: 99.74163818359375\n",
      "Loss at step 200: 99.7619857788086\n",
      "Loss at step 300: 99.05982971191406\n",
      "Loss at step 400: 98.56298065185547\n",
      "Loss at step 500: 97.8265380859375\n",
      "Loss at step 600: 99.15679931640625\n",
      "Loss at step 700: 99.10433197021484\n",
      "Loss at step 800: 103.155517578125\n",
      "Loss at step 900: 99.02200317382812\n",
      "Loss at step 1000: 99.45140075683594\n",
      "Loss at step 1100: 98.57527923583984\n",
      "Loss at step 1200: 99.00029754638672\n",
      "Epoch 84/1000, Avg Train Loss: 99.44634149915578\n",
      "Loss at step 0: 98.89800262451172\n",
      "Loss at step 100: 98.37109375\n",
      "Loss at step 200: 99.29183959960938\n",
      "Loss at step 300: 98.40464782714844\n",
      "Loss at step 400: 99.41321563720703\n",
      "Loss at step 500: 98.07551574707031\n",
      "Loss at step 600: 97.56978607177734\n",
      "Loss at step 700: 97.91152954101562\n",
      "Loss at step 800: 103.13555145263672\n",
      "Loss at step 900: 98.00968170166016\n",
      "Loss at step 1000: 98.40180969238281\n",
      "Loss at step 1100: 101.72595977783203\n",
      "Loss at step 1200: 98.89308166503906\n",
      "Epoch 85/1000, Avg Train Loss: 99.42337177486482\n",
      "Loss at step 0: 98.56743621826172\n",
      "Loss at step 100: 98.53289031982422\n",
      "Loss at step 200: 108.4468002319336\n",
      "Loss at step 300: 98.41444396972656\n",
      "Loss at step 400: 99.31521606445312\n",
      "Loss at step 500: 99.45679473876953\n",
      "Loss at step 600: 97.4544906616211\n",
      "Loss at step 700: 97.40583801269531\n",
      "Loss at step 800: 98.20537567138672\n",
      "Loss at step 900: 99.12967681884766\n",
      "Loss at step 1000: 97.76914978027344\n",
      "Loss at step 1100: 98.87068176269531\n",
      "Loss at step 1200: 99.17575073242188\n",
      "Epoch 86/1000, Avg Train Loss: 99.42139340372918\n",
      "Loss at step 0: 100.26316833496094\n",
      "Loss at step 100: 102.4579849243164\n",
      "Loss at step 200: 101.3654556274414\n",
      "Loss at step 300: 99.18196868896484\n",
      "Loss at step 400: 98.67993927001953\n",
      "Loss at step 500: 101.67317199707031\n",
      "Loss at step 600: 97.23290252685547\n",
      "Loss at step 700: 98.49604034423828\n",
      "Loss at step 800: 104.537109375\n",
      "Loss at step 900: 99.1526870727539\n",
      "Loss at step 1000: 97.73446655273438\n",
      "Loss at step 1100: 99.75959777832031\n",
      "Loss at step 1200: 99.17259979248047\n",
      "Epoch 87/1000, Avg Train Loss: 99.44368882163829\n",
      "Loss at step 0: 99.98634338378906\n",
      "Loss at step 100: 100.68416595458984\n",
      "Loss at step 200: 99.84613800048828\n",
      "Loss at step 300: 99.22505187988281\n",
      "Loss at step 400: 98.82473754882812\n",
      "Loss at step 500: 100.62853240966797\n",
      "Loss at step 600: 104.8823013305664\n",
      "Loss at step 700: 99.66084289550781\n",
      "Loss at step 800: 98.28980255126953\n",
      "Loss at step 900: 98.421875\n",
      "Loss at step 1000: 98.80838012695312\n",
      "Loss at step 1100: 98.87738037109375\n",
      "Loss at step 1200: 99.07171630859375\n",
      "Epoch 88/1000, Avg Train Loss: 99.43356734340631\n",
      "Loss at step 0: 97.78307342529297\n",
      "Loss at step 100: 98.45367431640625\n",
      "Loss at step 200: 98.7829360961914\n",
      "Loss at step 300: 98.6952133178711\n",
      "Loss at step 400: 99.65802001953125\n",
      "Loss at step 500: 98.20732879638672\n",
      "Loss at step 600: 99.1479263305664\n",
      "Loss at step 700: 98.74971008300781\n",
      "Loss at step 800: 105.35227966308594\n",
      "Loss at step 900: 101.12862396240234\n",
      "Loss at step 1000: 99.42611694335938\n",
      "Loss at step 1100: 97.81427001953125\n",
      "Loss at step 1200: 97.91032409667969\n",
      "Epoch 89/1000, Avg Train Loss: 99.41035283653481\n",
      "Loss at step 0: 99.01563262939453\n",
      "Loss at step 100: 98.56075286865234\n",
      "Loss at step 200: 98.37834930419922\n",
      "Loss at step 300: 97.53681182861328\n",
      "Loss at step 400: 98.23351287841797\n",
      "Loss at step 500: 97.28994750976562\n",
      "Loss at step 600: 144.67405700683594\n",
      "Loss at step 700: 99.98611450195312\n",
      "Loss at step 800: 98.93827819824219\n",
      "Loss at step 900: 98.81068420410156\n",
      "Loss at step 1000: 98.11772155761719\n",
      "Loss at step 1100: 100.60069274902344\n",
      "Loss at step 1200: 98.01260375976562\n",
      "Epoch 90/1000, Avg Train Loss: 99.44186563707864\n",
      "Loss at step 0: 98.94425964355469\n",
      "Loss at step 100: 97.65181732177734\n",
      "Loss at step 200: 100.97482299804688\n",
      "Loss at step 300: 98.206787109375\n",
      "Loss at step 400: 98.92607879638672\n",
      "Loss at step 500: 98.12635040283203\n",
      "Loss at step 600: 99.64324951171875\n",
      "Loss at step 700: 98.958984375\n",
      "Loss at step 800: 97.77244567871094\n",
      "Loss at step 900: 106.66499328613281\n",
      "Loss at step 1000: 98.22728729248047\n",
      "Loss at step 1100: 99.0723648071289\n",
      "Loss at step 1200: 98.76113891601562\n",
      "Epoch 91/1000, Avg Train Loss: 99.41262059998743\n",
      "Loss at step 0: 98.14127349853516\n",
      "Loss at step 100: 98.58724212646484\n",
      "Loss at step 200: 99.83554077148438\n",
      "Loss at step 300: 97.57637023925781\n",
      "Loss at step 400: 99.12259674072266\n",
      "Loss at step 500: 98.75469207763672\n",
      "Loss at step 600: 97.40081787109375\n",
      "Loss at step 700: 98.69779205322266\n",
      "Loss at step 800: 97.99124145507812\n",
      "Loss at step 900: 98.6467514038086\n",
      "Loss at step 1000: 103.02100372314453\n",
      "Loss at step 1100: 97.4049301147461\n",
      "Loss at step 1200: 98.54592895507812\n",
      "Epoch 92/1000, Avg Train Loss: 99.39114021251888\n",
      "Loss at step 0: 97.29534149169922\n",
      "Loss at step 100: 99.09593200683594\n",
      "Loss at step 200: 98.28681182861328\n",
      "Loss at step 300: 98.13077545166016\n",
      "Loss at step 400: 98.86886596679688\n",
      "Loss at step 500: 98.99427032470703\n",
      "Loss at step 600: 97.86011505126953\n",
      "Loss at step 700: 105.17158508300781\n",
      "Loss at step 800: 100.05240631103516\n",
      "Loss at step 900: 99.10856628417969\n",
      "Loss at step 1000: 98.16373443603516\n",
      "Loss at step 1100: 99.17486572265625\n",
      "Loss at step 1200: 99.66087341308594\n",
      "Epoch 93/1000, Avg Train Loss: 99.3966854058423\n",
      "Loss at step 0: 98.83277130126953\n",
      "Loss at step 100: 100.50553894042969\n",
      "Loss at step 200: 103.97322082519531\n",
      "Loss at step 300: 97.9315185546875\n",
      "Loss at step 400: 97.6526107788086\n",
      "Loss at step 500: 98.8875732421875\n",
      "Loss at step 600: 98.63109588623047\n",
      "Loss at step 700: 99.198486328125\n",
      "Loss at step 800: 101.98273468017578\n",
      "Loss at step 900: 98.63005065917969\n",
      "Loss at step 1000: 98.75891876220703\n",
      "Loss at step 1100: 98.1483154296875\n",
      "Loss at step 1200: 99.14456939697266\n",
      "Epoch 94/1000, Avg Train Loss: 99.42426692629323\n",
      "Loss at step 0: 99.02479553222656\n",
      "Loss at step 100: 100.8051528930664\n",
      "Loss at step 200: 100.06101989746094\n",
      "Loss at step 300: 98.20024108886719\n",
      "Loss at step 400: 97.77880859375\n",
      "Loss at step 500: 100.18783569335938\n",
      "Loss at step 600: 98.65290069580078\n",
      "Loss at step 700: 102.53636932373047\n",
      "Loss at step 800: 102.2743911743164\n",
      "Loss at step 900: 99.58464813232422\n",
      "Loss at step 1000: 98.65788269042969\n",
      "Loss at step 1100: 98.14039611816406\n",
      "Loss at step 1200: 98.84474182128906\n",
      "Epoch 95/1000, Avg Train Loss: 99.41297639149292\n",
      "Loss at step 0: 99.65960693359375\n",
      "Loss at step 100: 98.23516082763672\n",
      "Loss at step 200: 98.56016540527344\n",
      "Loss at step 300: 97.38015747070312\n",
      "Loss at step 400: 98.17048645019531\n",
      "Loss at step 500: 98.18327331542969\n",
      "Loss at step 600: 100.23194885253906\n",
      "Loss at step 700: 97.70147705078125\n",
      "Loss at step 800: 99.13382720947266\n",
      "Loss at step 900: 98.8449935913086\n",
      "Loss at step 1000: 97.5284194946289\n",
      "Loss at step 1100: 96.72454833984375\n",
      "Loss at step 1200: 105.29391479492188\n",
      "Epoch 96/1000, Avg Train Loss: 99.39942603743964\n",
      "Loss at step 0: 98.462158203125\n",
      "Loss at step 100: 98.85884857177734\n",
      "Loss at step 200: 97.76483154296875\n",
      "Loss at step 300: 101.32183074951172\n",
      "Loss at step 400: 97.93840789794922\n",
      "Loss at step 500: 98.3419189453125\n",
      "Loss at step 600: 100.81927490234375\n",
      "Loss at step 700: 98.84253692626953\n",
      "Loss at step 800: 99.11912536621094\n",
      "Loss at step 900: 98.78404235839844\n",
      "Loss at step 1000: 98.2622299194336\n",
      "Loss at step 1100: 97.25767517089844\n",
      "Loss at step 1200: 109.88201904296875\n",
      "Epoch 97/1000, Avg Train Loss: 99.41905646339589\n",
      "Loss at step 0: 97.92769622802734\n",
      "Loss at step 100: 98.33268737792969\n",
      "Loss at step 200: 99.31363677978516\n",
      "Loss at step 300: 98.0753173828125\n",
      "Loss at step 400: 98.79016876220703\n",
      "Loss at step 500: 100.48451232910156\n",
      "Loss at step 600: 98.81616973876953\n",
      "Loss at step 700: 99.09542083740234\n",
      "Loss at step 800: 99.13007354736328\n",
      "Loss at step 900: 98.15253448486328\n",
      "Loss at step 1000: 100.07807922363281\n",
      "Loss at step 1100: 98.92396545410156\n",
      "Loss at step 1200: 100.62776947021484\n",
      "Epoch 98/1000, Avg Train Loss: 99.42567114845448\n",
      "Loss at step 0: 99.1402587890625\n",
      "Loss at step 100: 98.57392883300781\n",
      "Loss at step 200: 110.42601776123047\n",
      "Loss at step 300: 107.7625503540039\n",
      "Loss at step 400: 98.7890396118164\n",
      "Loss at step 500: 97.28209686279297\n",
      "Loss at step 600: 98.23121643066406\n",
      "Loss at step 700: 99.11793518066406\n",
      "Loss at step 800: 100.69457244873047\n",
      "Loss at step 900: 97.80675506591797\n",
      "Loss at step 1000: 97.8544692993164\n",
      "Loss at step 1100: 98.80247497558594\n",
      "Loss at step 1200: 99.79702758789062\n",
      "Epoch 99/1000, Avg Train Loss: 99.42241073040515\n",
      "Loss at step 0: 100.41116333007812\n",
      "Loss at step 100: 102.49097442626953\n",
      "Loss at step 200: 97.8735580444336\n",
      "Loss at step 300: 101.48308563232422\n",
      "Loss at step 400: 99.01860046386719\n",
      "Loss at step 500: 98.1439437866211\n",
      "Loss at step 600: 99.44956970214844\n",
      "Loss at step 700: 99.6633529663086\n",
      "Loss at step 800: 98.75601959228516\n",
      "Loss at step 900: 98.24612426757812\n",
      "Loss at step 1000: 99.46631622314453\n",
      "Loss at step 1100: 98.5885238647461\n",
      "Loss at step 1200: 98.86731719970703\n",
      "Epoch 100/1000, Avg Train Loss: 99.41820131690757\n",
      "Loss at step 0: 97.7797622680664\n",
      "Loss at step 100: 100.35932159423828\n",
      "Loss at step 200: 98.29460144042969\n",
      "Loss at step 300: 99.10276794433594\n",
      "Loss at step 400: 97.9287338256836\n",
      "Loss at step 500: 99.72962188720703\n",
      "Loss at step 600: 99.20935821533203\n",
      "Loss at step 700: 99.69047546386719\n",
      "Loss at step 800: 99.33244323730469\n",
      "Loss at step 900: 97.61801147460938\n",
      "Loss at step 1000: 103.31761932373047\n",
      "Loss at step 1100: 98.18146514892578\n",
      "Loss at step 1200: 99.09843444824219\n",
      "Epoch 101/1000, Avg Train Loss: 99.41469417806582\n",
      "Loss at step 0: 99.64875793457031\n",
      "Loss at step 100: 98.95538330078125\n",
      "Loss at step 200: 98.42219543457031\n",
      "Loss at step 300: 105.68917083740234\n",
      "Loss at step 400: 99.21625518798828\n",
      "Loss at step 500: 97.80167388916016\n",
      "Loss at step 600: 98.82886505126953\n",
      "Loss at step 700: 98.2432632446289\n",
      "Loss at step 800: 99.04621887207031\n",
      "Loss at step 900: 98.27510070800781\n",
      "Loss at step 1000: 98.17726135253906\n",
      "Loss at step 1100: 99.61685180664062\n",
      "Loss at step 1200: 98.8527603149414\n",
      "Epoch 102/1000, Avg Train Loss: 99.41242350729539\n",
      "Loss at step 0: 99.54618072509766\n",
      "Loss at step 100: 99.1638412475586\n",
      "Loss at step 200: 97.6837387084961\n",
      "Loss at step 300: 97.83808898925781\n",
      "Loss at step 400: 97.89603424072266\n",
      "Loss at step 500: 100.90965270996094\n",
      "Loss at step 600: 98.70885467529297\n",
      "Loss at step 700: 99.10157012939453\n",
      "Loss at step 800: 97.95777130126953\n",
      "Loss at step 900: 98.77344512939453\n",
      "Loss at step 1000: 100.33738708496094\n",
      "Loss at step 1100: 98.80842590332031\n",
      "Loss at step 1200: 98.88230895996094\n",
      "Epoch 103/1000, Avg Train Loss: 99.39215063200028\n",
      "Loss at step 0: 97.92353057861328\n",
      "Loss at step 100: 98.8907699584961\n",
      "Loss at step 200: 99.1782455444336\n",
      "Loss at step 300: 98.1422348022461\n",
      "Loss at step 400: 99.25731658935547\n",
      "Loss at step 500: 99.96910858154297\n",
      "Loss at step 600: 100.44701385498047\n",
      "Loss at step 700: 98.46846008300781\n",
      "Loss at step 800: 99.15071868896484\n",
      "Loss at step 900: 100.04484558105469\n",
      "Loss at step 1000: 98.50947570800781\n",
      "Loss at step 1100: 98.02775573730469\n",
      "Loss at step 1200: 98.4852523803711\n",
      "Epoch 104/1000, Avg Train Loss: 99.41286962935068\n",
      "Loss at step 0: 98.8061294555664\n",
      "Loss at step 100: 99.06617736816406\n",
      "Loss at step 200: 98.02459716796875\n",
      "Loss at step 300: 98.92251586914062\n",
      "Loss at step 400: 98.8129653930664\n",
      "Loss at step 500: 98.06358337402344\n",
      "Loss at step 600: 101.73077392578125\n",
      "Loss at step 700: 98.74288940429688\n",
      "Loss at step 800: 99.65371704101562\n",
      "Loss at step 900: 97.20635986328125\n",
      "Loss at step 1000: 99.388671875\n",
      "Loss at step 1100: 98.2457046508789\n",
      "Loss at step 1200: 99.52484130859375\n",
      "Epoch 105/1000, Avg Train Loss: 99.44595870230962\n",
      "Loss at step 0: 99.15377044677734\n",
      "Loss at step 100: 98.52643585205078\n",
      "Loss at step 200: 99.58118438720703\n",
      "Loss at step 300: 98.45532989501953\n",
      "Loss at step 400: 108.03368377685547\n",
      "Loss at step 500: 98.88195037841797\n",
      "Loss at step 600: 97.380859375\n",
      "Loss at step 700: 98.21683502197266\n",
      "Loss at step 800: 102.742431640625\n",
      "Loss at step 900: 98.75326538085938\n",
      "Loss at step 1000: 99.844482421875\n",
      "Loss at step 1100: 98.66292572021484\n",
      "Loss at step 1200: 98.36080169677734\n",
      "Epoch 106/1000, Avg Train Loss: 99.42565126017846\n",
      "Loss at step 0: 98.33731842041016\n",
      "Loss at step 100: 98.318603515625\n",
      "Loss at step 200: 99.90814971923828\n",
      "Loss at step 300: 98.57699584960938\n",
      "Loss at step 400: 98.58023834228516\n",
      "Loss at step 500: 97.64844512939453\n",
      "Loss at step 600: 99.38794708251953\n",
      "Loss at step 700: 99.51881408691406\n",
      "Loss at step 800: 98.6750717163086\n",
      "Loss at step 900: 99.46009826660156\n",
      "Loss at step 1000: 98.71588897705078\n",
      "Loss at step 1100: 97.92549133300781\n",
      "Loss at step 1200: 99.21342468261719\n",
      "Epoch 107/1000, Avg Train Loss: 99.4096697588183\n",
      "Loss at step 0: 101.60537719726562\n",
      "Loss at step 100: 98.7362060546875\n",
      "Loss at step 200: 98.45018005371094\n",
      "Loss at step 300: 98.33819580078125\n",
      "Loss at step 400: 98.87550354003906\n",
      "Loss at step 500: 99.60202026367188\n",
      "Loss at step 600: 98.54444122314453\n",
      "Loss at step 700: 108.93643188476562\n",
      "Loss at step 800: 97.55583953857422\n",
      "Loss at step 900: 96.8573226928711\n",
      "Loss at step 1000: 99.2791976928711\n",
      "Loss at step 1100: 100.64090728759766\n",
      "Loss at step 1200: 98.84004211425781\n",
      "Epoch 108/1000, Avg Train Loss: 99.44437293018724\n",
      "Loss at step 0: 99.40818786621094\n",
      "Loss at step 100: 99.180419921875\n",
      "Loss at step 200: 99.21179962158203\n",
      "Loss at step 300: 98.77649688720703\n",
      "Loss at step 400: 98.6247787475586\n",
      "Loss at step 500: 104.75839233398438\n",
      "Loss at step 600: 99.92298889160156\n",
      "Loss at step 700: 99.89863586425781\n",
      "Loss at step 800: 97.92310333251953\n",
      "Loss at step 900: 109.69145202636719\n",
      "Loss at step 1000: 98.72782135009766\n",
      "Loss at step 1100: 100.14612579345703\n",
      "Loss at step 1200: 97.65841674804688\n",
      "Epoch 109/1000, Avg Train Loss: 99.46294373447455\n",
      "Loss at step 0: 97.67537689208984\n",
      "Loss at step 100: 97.45960235595703\n",
      "Loss at step 200: 99.87945556640625\n",
      "Loss at step 300: 103.18280792236328\n",
      "Loss at step 400: 98.64219665527344\n",
      "Loss at step 500: 98.39649200439453\n",
      "Loss at step 600: 98.42426300048828\n",
      "Loss at step 700: 99.03146362304688\n",
      "Loss at step 800: 99.56369018554688\n",
      "Loss at step 900: 99.45166778564453\n",
      "Loss at step 1000: 100.87873077392578\n",
      "Loss at step 1100: 100.04975891113281\n",
      "Loss at step 1200: 97.2364273071289\n",
      "Epoch 110/1000, Avg Train Loss: 99.41905314868322\n",
      "Loss at step 0: 102.30591583251953\n",
      "Loss at step 100: 98.40979766845703\n",
      "Loss at step 200: 98.605224609375\n",
      "Loss at step 300: 100.00798797607422\n",
      "Loss at step 400: 98.60282135009766\n",
      "Loss at step 500: 100.11317443847656\n",
      "Loss at step 600: 97.79180908203125\n",
      "Loss at step 700: 98.74436950683594\n",
      "Loss at step 800: 99.71562957763672\n",
      "Loss at step 900: 98.41018676757812\n",
      "Loss at step 1000: 98.6612319946289\n",
      "Loss at step 1100: 99.61200714111328\n",
      "Loss at step 1200: 99.35508728027344\n",
      "Epoch 111/1000, Avg Train Loss: 99.42735512742719\n",
      "Loss at step 0: 101.52218627929688\n",
      "Loss at step 100: 100.12576293945312\n",
      "Loss at step 200: 98.77111053466797\n",
      "Loss at step 300: 97.8195571899414\n",
      "Loss at step 400: 104.33784484863281\n",
      "Loss at step 500: 99.62494659423828\n",
      "Loss at step 600: 99.67633819580078\n",
      "Loss at step 700: 100.45405578613281\n",
      "Loss at step 800: 101.36141967773438\n",
      "Loss at step 900: 99.01971435546875\n",
      "Loss at step 1000: 98.3555908203125\n",
      "Loss at step 1100: 99.95536041259766\n",
      "Loss at step 1200: 99.31184387207031\n",
      "Epoch 112/1000, Avg Train Loss: 99.40112889854653\n",
      "Loss at step 0: 98.88699340820312\n",
      "Loss at step 100: 98.51012420654297\n",
      "Loss at step 200: 99.0301284790039\n",
      "Loss at step 300: 98.0047607421875\n",
      "Loss at step 400: 98.55459594726562\n",
      "Loss at step 500: 98.51835632324219\n",
      "Loss at step 600: 100.5950698852539\n",
      "Loss at step 700: 98.85458374023438\n",
      "Loss at step 800: 99.51325988769531\n",
      "Loss at step 900: 98.10813903808594\n",
      "Loss at step 1000: 98.6939697265625\n",
      "Loss at step 1100: 98.43424224853516\n",
      "Loss at step 1200: 98.71351623535156\n",
      "Epoch 113/1000, Avg Train Loss: 99.41428589435071\n",
      "Loss at step 0: 99.34903717041016\n",
      "Loss at step 100: 99.74747467041016\n",
      "Loss at step 200: 100.61006164550781\n",
      "Loss at step 300: 99.67381286621094\n",
      "Loss at step 400: 98.52522277832031\n",
      "Loss at step 500: 98.77690887451172\n",
      "Loss at step 600: 98.60395050048828\n",
      "Loss at step 700: 97.99683380126953\n",
      "Loss at step 800: 98.50337219238281\n",
      "Loss at step 900: 98.92909240722656\n",
      "Loss at step 1000: 100.83673858642578\n",
      "Loss at step 1100: 97.48931884765625\n",
      "Loss at step 1200: 99.14842224121094\n",
      "Epoch 114/1000, Avg Train Loss: 99.41258303324382\n",
      "Loss at step 0: 100.09307861328125\n",
      "Loss at step 100: 99.44217681884766\n",
      "Loss at step 200: 97.87055206298828\n",
      "Loss at step 300: 97.84048461914062\n",
      "Loss at step 400: 99.33607482910156\n",
      "Loss at step 500: 98.84050750732422\n",
      "Loss at step 600: 97.04937744140625\n",
      "Loss at step 700: 137.0098419189453\n",
      "Loss at step 800: 98.83528137207031\n",
      "Loss at step 900: 98.4121322631836\n",
      "Loss at step 1000: 97.25912475585938\n",
      "Loss at step 1100: 99.09040069580078\n",
      "Loss at step 1200: 98.89424133300781\n",
      "Epoch 115/1000, Avg Train Loss: 99.42357507724206\n",
      "Loss at step 0: 97.62249755859375\n",
      "Loss at step 100: 98.62702941894531\n",
      "Loss at step 200: 99.24502563476562\n",
      "Loss at step 300: 97.41724395751953\n",
      "Loss at step 400: 99.6837387084961\n",
      "Loss at step 500: 99.13060760498047\n",
      "Loss at step 600: 100.90253448486328\n",
      "Loss at step 700: 98.76806640625\n",
      "Loss at step 800: 98.32299041748047\n",
      "Loss at step 900: 97.97684478759766\n",
      "Loss at step 1000: 102.7766342163086\n",
      "Loss at step 1100: 99.16200256347656\n",
      "Loss at step 1200: 98.6450424194336\n",
      "Epoch 116/1000, Avg Train Loss: 99.45930433427631\n",
      "Loss at step 0: 98.06877136230469\n",
      "Loss at step 100: 98.33323669433594\n",
      "Loss at step 200: 98.44277954101562\n",
      "Loss at step 300: 98.23931121826172\n",
      "Loss at step 400: 97.8031997680664\n",
      "Loss at step 500: 99.124755859375\n",
      "Loss at step 600: 97.61742401123047\n",
      "Loss at step 700: 99.38825988769531\n",
      "Loss at step 800: 98.28901672363281\n",
      "Loss at step 900: 99.34469604492188\n",
      "Loss at step 1000: 102.50171661376953\n",
      "Loss at step 1100: 99.12767028808594\n",
      "Loss at step 1200: 98.87106323242188\n",
      "Epoch 117/1000, Avg Train Loss: 99.37889632437994\n",
      "Loss at step 0: 99.87733459472656\n",
      "Loss at step 100: 98.73164367675781\n",
      "Loss at step 200: 99.0948715209961\n",
      "Loss at step 300: 98.4993896484375\n",
      "Loss at step 400: 98.83975219726562\n",
      "Loss at step 500: 98.83355712890625\n",
      "Loss at step 600: 97.78376007080078\n",
      "Loss at step 700: 105.47432708740234\n",
      "Loss at step 800: 99.02986145019531\n",
      "Loss at step 900: 98.91300201416016\n",
      "Loss at step 1000: 97.42681121826172\n",
      "Loss at step 1100: 98.96710205078125\n",
      "Loss at step 1200: 98.75333404541016\n",
      "Epoch 118/1000, Avg Train Loss: 99.43435215255589\n",
      "Loss at step 0: 97.59608459472656\n",
      "Loss at step 100: 101.06076049804688\n",
      "Loss at step 200: 98.0951919555664\n",
      "Loss at step 300: 97.97879791259766\n",
      "Loss at step 400: 101.32161712646484\n",
      "Loss at step 500: 98.05427551269531\n",
      "Loss at step 600: 98.56202697753906\n",
      "Loss at step 700: 100.01763153076172\n",
      "Loss at step 800: 105.985107421875\n",
      "Loss at step 900: 98.45519256591797\n",
      "Loss at step 1000: 101.29356384277344\n",
      "Loss at step 1100: 101.041748046875\n",
      "Loss at step 1200: 97.81774139404297\n",
      "Epoch 119/1000, Avg Train Loss: 99.4160169521196\n",
      "Loss at step 0: 99.16238403320312\n",
      "Loss at step 100: 98.99588012695312\n",
      "Loss at step 200: 98.63579559326172\n",
      "Loss at step 300: 97.52141571044922\n",
      "Loss at step 400: 98.41802978515625\n",
      "Loss at step 500: 97.91475677490234\n",
      "Loss at step 600: 99.22833251953125\n",
      "Loss at step 700: 100.24855041503906\n",
      "Loss at step 800: 97.8786392211914\n",
      "Loss at step 900: 98.9863052368164\n",
      "Loss at step 1000: 98.04151153564453\n",
      "Loss at step 1100: 99.20513916015625\n",
      "Loss at step 1200: 98.86997985839844\n",
      "Epoch 120/1000, Avg Train Loss: 99.43665895184267\n",
      "Loss at step 0: 100.7973403930664\n",
      "Loss at step 100: 97.47984313964844\n",
      "Loss at step 200: 99.50192260742188\n",
      "Loss at step 300: 98.93987274169922\n",
      "Loss at step 400: 101.3482437133789\n",
      "Loss at step 500: 98.55306243896484\n",
      "Loss at step 600: 99.51541137695312\n",
      "Loss at step 700: 98.6053466796875\n",
      "Loss at step 800: 98.67053985595703\n",
      "Loss at step 900: 98.09162902832031\n",
      "Loss at step 1000: 98.8974838256836\n",
      "Loss at step 1100: 98.50687408447266\n",
      "Loss at step 1200: 98.4166488647461\n",
      "Epoch 121/1000, Avg Train Loss: 99.43513215321167\n",
      "Loss at step 0: 98.80175018310547\n",
      "Loss at step 100: 98.82746887207031\n",
      "Loss at step 200: 97.70599365234375\n",
      "Loss at step 300: 99.03848266601562\n",
      "Loss at step 400: 100.25562286376953\n",
      "Loss at step 500: 100.2710189819336\n",
      "Loss at step 600: 105.30401611328125\n",
      "Loss at step 700: 98.6146469116211\n",
      "Loss at step 800: 108.64210510253906\n",
      "Loss at step 900: 111.93500518798828\n",
      "Loss at step 1000: 100.21224975585938\n",
      "Loss at step 1100: 102.23596954345703\n",
      "Loss at step 1200: 98.89864349365234\n",
      "Epoch 122/1000, Avg Train Loss: 99.43821826799017\n",
      "Loss at step 0: 98.35289001464844\n",
      "Loss at step 100: 98.3927993774414\n",
      "Loss at step 200: 98.44447326660156\n",
      "Loss at step 300: 98.93568420410156\n",
      "Loss at step 400: 98.50376892089844\n",
      "Loss at step 500: 99.24362182617188\n",
      "Loss at step 600: 98.35438537597656\n",
      "Loss at step 700: 97.98931884765625\n",
      "Loss at step 800: 98.98380279541016\n",
      "Loss at step 900: 99.60330200195312\n",
      "Loss at step 1000: 99.2691650390625\n",
      "Loss at step 1100: 97.84932708740234\n",
      "Loss at step 1200: 113.50686645507812\n",
      "Epoch 123/1000, Avg Train Loss: 99.40939807583213\n",
      "Loss at step 0: 98.08246612548828\n",
      "Loss at step 100: 98.72830200195312\n",
      "Loss at step 200: 98.03681945800781\n",
      "Loss at step 300: 99.0794448852539\n",
      "Loss at step 400: 99.38385772705078\n",
      "Loss at step 500: 97.65241241455078\n",
      "Loss at step 600: 99.29150390625\n",
      "Loss at step 700: 98.9045639038086\n",
      "Loss at step 800: 98.78541564941406\n",
      "Loss at step 900: 110.10623931884766\n",
      "Loss at step 1000: 96.64881134033203\n",
      "Loss at step 1100: 99.06279754638672\n",
      "Loss at step 1200: 98.70601654052734\n",
      "Epoch 124/1000, Avg Train Loss: 99.42110556691982\n",
      "Loss at step 0: 97.73263549804688\n",
      "Loss at step 100: 98.51121520996094\n",
      "Loss at step 200: 97.50555419921875\n",
      "Loss at step 300: 98.48558044433594\n",
      "Loss at step 400: 99.87137603759766\n",
      "Loss at step 500: 98.58735656738281\n",
      "Loss at step 600: 99.0041732788086\n",
      "Loss at step 700: 98.68486785888672\n",
      "Loss at step 800: 99.4059829711914\n",
      "Loss at step 900: 98.68312072753906\n",
      "Loss at step 1000: 99.20541381835938\n",
      "Loss at step 1100: 98.7533950805664\n",
      "Loss at step 1200: 98.09075927734375\n",
      "Epoch 125/1000, Avg Train Loss: 99.4178493215814\n",
      "Loss at step 0: 98.15074157714844\n",
      "Loss at step 100: 99.66110229492188\n",
      "Loss at step 200: 98.75493621826172\n",
      "Loss at step 300: 99.52336883544922\n",
      "Loss at step 400: 99.87140655517578\n",
      "Loss at step 500: 100.22099304199219\n",
      "Loss at step 600: 98.56416320800781\n",
      "Loss at step 700: 99.1394271850586\n",
      "Loss at step 800: 99.16563415527344\n",
      "Loss at step 900: 98.50192260742188\n",
      "Loss at step 1000: 98.05479431152344\n",
      "Loss at step 1100: 97.47877502441406\n",
      "Loss at step 1200: 97.71012878417969\n",
      "Epoch 126/1000, Avg Train Loss: 99.43307302530529\n",
      "Loss at step 0: 98.29908752441406\n",
      "Loss at step 100: 99.33085632324219\n",
      "Loss at step 200: 98.61482238769531\n",
      "Loss at step 300: 97.4586410522461\n",
      "Loss at step 400: 98.6185531616211\n",
      "Loss at step 500: 98.09507751464844\n",
      "Loss at step 600: 98.35549926757812\n",
      "Loss at step 700: 98.59173583984375\n",
      "Loss at step 800: 102.98605346679688\n",
      "Loss at step 900: 98.73269653320312\n",
      "Loss at step 1000: 98.46328735351562\n",
      "Loss at step 1100: 98.87482452392578\n",
      "Loss at step 1200: 98.54051208496094\n",
      "Epoch 127/1000, Avg Train Loss: 99.41710694939573\n",
      "Loss at step 0: 100.1050796508789\n",
      "Loss at step 100: 97.1619873046875\n",
      "Loss at step 200: 99.217041015625\n",
      "Loss at step 300: 97.9894790649414\n",
      "Loss at step 400: 108.68025207519531\n",
      "Loss at step 500: 98.18620300292969\n",
      "Loss at step 600: 99.37258911132812\n",
      "Loss at step 700: 101.90916442871094\n",
      "Loss at step 800: 114.52738952636719\n",
      "Loss at step 900: 98.3287124633789\n",
      "Loss at step 1000: 98.87850952148438\n",
      "Loss at step 1100: 101.61177062988281\n",
      "Loss at step 1200: 98.04158020019531\n",
      "Epoch 128/1000, Avg Train Loss: 99.42071214077157\n",
      "Loss at step 0: 98.13653564453125\n",
      "Loss at step 100: 98.56707763671875\n",
      "Loss at step 200: 98.00985717773438\n",
      "Loss at step 300: 98.94037628173828\n",
      "Loss at step 400: 100.32793426513672\n",
      "Loss at step 500: 98.43955993652344\n",
      "Loss at step 600: 98.59046936035156\n",
      "Loss at step 700: 99.4165267944336\n",
      "Loss at step 800: 98.66476440429688\n",
      "Loss at step 900: 99.56118774414062\n",
      "Loss at step 1000: 99.73712158203125\n",
      "Loss at step 1100: 97.91929626464844\n",
      "Loss at step 1200: 101.31780242919922\n",
      "Epoch 129/1000, Avg Train Loss: 99.45653351225128\n",
      "Loss at step 0: 100.61566925048828\n",
      "Loss at step 100: 98.41667175292969\n",
      "Loss at step 200: 104.38675689697266\n",
      "Loss at step 300: 98.05823516845703\n",
      "Loss at step 400: 98.78910827636719\n",
      "Loss at step 500: 98.73310089111328\n",
      "Loss at step 600: 99.85360717773438\n",
      "Loss at step 700: 99.8243408203125\n",
      "Loss at step 800: 97.7467269897461\n",
      "Loss at step 900: 98.00641632080078\n",
      "Loss at step 1000: 98.5578384399414\n",
      "Loss at step 1100: 101.73200225830078\n",
      "Loss at step 1200: 98.51776885986328\n",
      "Epoch 130/1000, Avg Train Loss: 99.38819185892741\n",
      "Loss at step 0: 99.7903823852539\n",
      "Loss at step 100: 98.95677947998047\n",
      "Loss at step 200: 98.35913848876953\n",
      "Loss at step 300: 98.30062866210938\n",
      "Loss at step 400: 99.05057525634766\n",
      "Loss at step 500: 98.93101501464844\n",
      "Loss at step 600: 97.812255859375\n",
      "Loss at step 700: 98.88809204101562\n",
      "Loss at step 800: 98.5320816040039\n",
      "Loss at step 900: 97.89791107177734\n",
      "Loss at step 1000: 97.86402893066406\n",
      "Loss at step 1100: 99.6201400756836\n",
      "Loss at step 1200: 99.32379913330078\n",
      "Epoch 131/1000, Avg Train Loss: 99.46989655726165\n",
      "Loss at step 0: 97.50770568847656\n",
      "Loss at step 100: 100.50934600830078\n",
      "Loss at step 200: 98.19303131103516\n",
      "Loss at step 300: 98.02978515625\n",
      "Loss at step 400: 106.86331176757812\n",
      "Loss at step 500: 99.0023193359375\n",
      "Loss at step 600: 98.99092102050781\n",
      "Loss at step 700: 98.32901000976562\n",
      "Loss at step 800: 98.90260314941406\n",
      "Loss at step 900: 98.0112533569336\n",
      "Loss at step 1000: 97.9206314086914\n",
      "Loss at step 1100: 99.31535339355469\n",
      "Loss at step 1200: 99.19081115722656\n",
      "Epoch 132/1000, Avg Train Loss: 99.3862515051388\n",
      "Loss at step 0: 100.88445281982422\n",
      "Loss at step 100: 97.8799819946289\n",
      "Loss at step 200: 98.11882781982422\n",
      "Loss at step 300: 98.67399597167969\n",
      "Loss at step 400: 98.3357925415039\n",
      "Loss at step 500: 97.83097076416016\n",
      "Loss at step 600: 98.02027893066406\n",
      "Loss at step 700: 100.04988861083984\n",
      "Loss at step 800: 98.294677734375\n",
      "Loss at step 900: 98.63056945800781\n",
      "Loss at step 1000: 99.93289947509766\n",
      "Loss at step 1100: 97.6765365600586\n",
      "Loss at step 1200: 97.54801177978516\n",
      "Epoch 133/1000, Avg Train Loss: 99.40110347957673\n",
      "Loss at step 0: 98.65799713134766\n",
      "Loss at step 100: 99.46495056152344\n",
      "Loss at step 200: 98.68146514892578\n",
      "Loss at step 300: 99.17955017089844\n",
      "Loss at step 400: 99.6843032836914\n",
      "Loss at step 500: 102.4291763305664\n",
      "Loss at step 600: 100.28466796875\n",
      "Loss at step 700: 100.25556182861328\n",
      "Loss at step 800: 97.60416412353516\n",
      "Loss at step 900: 98.98827362060547\n",
      "Loss at step 1000: 99.00658416748047\n",
      "Loss at step 1100: 99.51302337646484\n",
      "Loss at step 1200: 103.28742218017578\n",
      "Epoch 134/1000, Avg Train Loss: 99.40143311756714\n",
      "Loss at step 0: 99.17463684082031\n",
      "Loss at step 100: 98.39490509033203\n",
      "Loss at step 200: 98.72602081298828\n",
      "Loss at step 300: 98.44587707519531\n",
      "Loss at step 400: 98.13227081298828\n",
      "Loss at step 500: 97.69442749023438\n",
      "Loss at step 600: 98.12993621826172\n",
      "Loss at step 700: 98.3531494140625\n",
      "Loss at step 800: 97.6491928100586\n",
      "Loss at step 900: 98.16055297851562\n",
      "Loss at step 1000: 100.18199920654297\n",
      "Loss at step 1100: 98.90294647216797\n",
      "Loss at step 1200: 99.7620620727539\n",
      "Epoch 135/1000, Avg Train Loss: 99.42206583362567\n",
      "Loss at step 0: 100.09716796875\n",
      "Loss at step 100: 97.97035217285156\n",
      "Loss at step 200: 99.3158950805664\n",
      "Loss at step 300: 98.42867279052734\n",
      "Loss at step 400: 98.52659606933594\n",
      "Loss at step 500: 98.08818817138672\n",
      "Loss at step 600: 97.44441223144531\n",
      "Loss at step 700: 98.98450469970703\n",
      "Loss at step 800: 97.95625305175781\n",
      "Loss at step 900: 97.61664581298828\n",
      "Loss at step 1000: 99.08729553222656\n",
      "Loss at step 1100: 98.49225616455078\n",
      "Loss at step 1200: 98.80816650390625\n",
      "Epoch 136/1000, Avg Train Loss: 99.39801461796931\n",
      "Loss at step 0: 98.3363037109375\n",
      "Loss at step 100: 97.7509765625\n",
      "Loss at step 200: 98.52642822265625\n",
      "Loss at step 300: 97.57190704345703\n",
      "Loss at step 400: 98.19120788574219\n",
      "Loss at step 500: 98.16993713378906\n",
      "Loss at step 600: 103.12124633789062\n",
      "Loss at step 700: 98.58592987060547\n",
      "Loss at step 800: 98.52296447753906\n",
      "Loss at step 900: 99.99181365966797\n",
      "Loss at step 1000: 98.24022674560547\n",
      "Loss at step 1100: 98.04737854003906\n",
      "Loss at step 1200: 98.56310272216797\n",
      "Epoch 137/1000, Avg Train Loss: 99.39079085908661\n",
      "Loss at step 0: 98.34777069091797\n",
      "Loss at step 100: 97.75895690917969\n",
      "Loss at step 200: 102.57032775878906\n",
      "Loss at step 300: 99.432861328125\n",
      "Loss at step 400: 99.18833923339844\n",
      "Loss at step 500: 104.66302490234375\n",
      "Loss at step 600: 103.37994384765625\n",
      "Loss at step 700: 100.69834899902344\n",
      "Loss at step 800: 99.25534057617188\n",
      "Loss at step 900: 98.5399398803711\n",
      "Loss at step 1000: 98.73651123046875\n",
      "Loss at step 1100: 98.31534576416016\n",
      "Loss at step 1200: 99.60197448730469\n",
      "Epoch 138/1000, Avg Train Loss: 99.41671561577559\n",
      "Loss at step 0: 98.66143798828125\n",
      "Loss at step 100: 102.61908721923828\n",
      "Loss at step 200: 101.62084197998047\n",
      "Loss at step 300: 98.55256652832031\n",
      "Loss at step 400: 98.47113037109375\n",
      "Loss at step 500: 98.71775817871094\n",
      "Loss at step 600: 97.93994140625\n",
      "Loss at step 700: 100.2578125\n",
      "Loss at step 800: 101.10598754882812\n",
      "Loss at step 900: 97.91738891601562\n",
      "Loss at step 1000: 101.3487548828125\n",
      "Loss at step 1100: 99.21692657470703\n",
      "Loss at step 1200: 98.41915130615234\n",
      "Epoch 139/1000, Avg Train Loss: 99.40987670923128\n",
      "Loss at step 0: 100.81218719482422\n",
      "Loss at step 100: 98.13975524902344\n",
      "Loss at step 200: 97.72696685791016\n",
      "Loss at step 300: 98.80097198486328\n",
      "Loss at step 400: 106.7841796875\n",
      "Loss at step 500: 98.84017944335938\n",
      "Loss at step 600: 101.07343292236328\n",
      "Loss at step 700: 98.96977233886719\n",
      "Loss at step 800: 97.66295623779297\n",
      "Loss at step 900: 98.2920913696289\n",
      "Loss at step 1000: 100.30998992919922\n",
      "Loss at step 1100: 98.35233306884766\n",
      "Loss at step 1200: 98.61932373046875\n",
      "Epoch 140/1000, Avg Train Loss: 99.45857896773947\n",
      "Loss at step 0: 100.24070739746094\n",
      "Loss at step 100: 97.52752685546875\n",
      "Loss at step 200: 105.35897827148438\n",
      "Loss at step 300: 97.84162139892578\n",
      "Loss at step 400: 97.40158081054688\n",
      "Loss at step 500: 98.40885162353516\n",
      "Loss at step 600: 98.71372985839844\n",
      "Loss at step 700: 98.30180358886719\n",
      "Loss at step 800: 100.26240539550781\n",
      "Loss at step 900: 100.97261810302734\n",
      "Loss at step 1000: 97.86536407470703\n",
      "Loss at step 1100: 99.2931900024414\n",
      "Loss at step 1200: 102.30494689941406\n",
      "Epoch 141/1000, Avg Train Loss: 99.39294987280392\n",
      "Loss at step 0: 99.68495178222656\n",
      "Loss at step 100: 98.55313110351562\n",
      "Loss at step 200: 97.8680419921875\n",
      "Loss at step 300: 99.2166976928711\n",
      "Loss at step 400: 99.46451568603516\n",
      "Loss at step 500: 98.13811492919922\n",
      "Loss at step 600: 98.3798599243164\n",
      "Loss at step 700: 97.61793518066406\n",
      "Loss at step 800: 97.56202697753906\n",
      "Loss at step 900: 97.99874114990234\n",
      "Loss at step 1000: 98.41522216796875\n",
      "Loss at step 1100: 98.11650848388672\n",
      "Loss at step 1200: 98.24748992919922\n",
      "Epoch 142/1000, Avg Train Loss: 99.42774211550221\n",
      "Loss at step 0: 98.15607452392578\n",
      "Loss at step 100: 98.73873138427734\n",
      "Loss at step 200: 99.5768814086914\n",
      "Loss at step 300: 102.08566284179688\n",
      "Loss at step 400: 98.88863372802734\n",
      "Loss at step 500: 99.16691589355469\n",
      "Loss at step 600: 99.20624542236328\n",
      "Loss at step 700: 97.70935821533203\n",
      "Loss at step 800: 99.22563171386719\n",
      "Loss at step 900: 99.5641860961914\n",
      "Loss at step 1000: 102.05233001708984\n",
      "Loss at step 1100: 97.66161346435547\n",
      "Loss at step 1200: 99.05001831054688\n",
      "Epoch 143/1000, Avg Train Loss: 99.42692349875243\n",
      "Loss at step 0: 99.33723449707031\n",
      "Loss at step 100: 98.249267578125\n",
      "Loss at step 200: 98.63522338867188\n",
      "Loss at step 300: 98.39199829101562\n",
      "Loss at step 400: 98.00003051757812\n",
      "Loss at step 500: 98.38763427734375\n",
      "Loss at step 600: 98.60968017578125\n",
      "Loss at step 700: 100.44182586669922\n",
      "Loss at step 800: 99.07679748535156\n",
      "Loss at step 900: 99.07505798339844\n",
      "Loss at step 1000: 98.43942260742188\n",
      "Loss at step 1100: 98.88890075683594\n",
      "Loss at step 1200: 98.34416198730469\n",
      "Epoch 144/1000, Avg Train Loss: 99.4221507445894\n",
      "Loss at step 0: 98.52569580078125\n",
      "Loss at step 100: 100.53022003173828\n",
      "Loss at step 200: 98.36573791503906\n",
      "Loss at step 300: 98.35718536376953\n",
      "Loss at step 400: 99.68946838378906\n",
      "Loss at step 500: 97.94466400146484\n",
      "Loss at step 600: 99.017822265625\n",
      "Loss at step 700: 99.29766845703125\n",
      "Loss at step 800: 97.60074615478516\n",
      "Loss at step 900: 99.22301483154297\n",
      "Loss at step 1000: 110.79264831542969\n",
      "Loss at step 1100: 100.35336303710938\n",
      "Loss at step 1200: 98.3280029296875\n",
      "Epoch 145/1000, Avg Train Loss: 99.42378496583612\n",
      "Loss at step 0: 100.98918914794922\n",
      "Loss at step 100: 98.00585174560547\n",
      "Loss at step 200: 99.604248046875\n",
      "Loss at step 300: 98.62613677978516\n",
      "Loss at step 400: 98.22364807128906\n",
      "Loss at step 500: 100.66706848144531\n",
      "Loss at step 600: 99.7496566772461\n",
      "Loss at step 700: 103.20439147949219\n",
      "Loss at step 800: 98.23877716064453\n",
      "Loss at step 900: 98.13966369628906\n",
      "Loss at step 1000: 98.13931274414062\n",
      "Loss at step 1100: 99.06222534179688\n",
      "Loss at step 1200: 99.75841522216797\n",
      "Epoch 146/1000, Avg Train Loss: 99.42155520738521\n",
      "Loss at step 0: 98.2728271484375\n",
      "Loss at step 100: 98.46571350097656\n",
      "Loss at step 200: 100.8844985961914\n",
      "Loss at step 300: 99.47136688232422\n",
      "Loss at step 400: 98.07177734375\n",
      "Loss at step 500: 99.20957946777344\n",
      "Loss at step 600: 97.87720489501953\n",
      "Loss at step 700: 101.42681884765625\n",
      "Loss at step 800: 98.83079528808594\n",
      "Loss at step 900: 97.25531005859375\n",
      "Loss at step 1000: 97.6715087890625\n",
      "Loss at step 1100: 97.11998748779297\n",
      "Loss at step 1200: 100.06451416015625\n",
      "Epoch 147/1000, Avg Train Loss: 99.3793896239938\n",
      "Loss at step 0: 97.53690338134766\n",
      "Loss at step 100: 98.7317123413086\n",
      "Loss at step 200: 99.0434799194336\n",
      "Loss at step 300: 98.88795471191406\n",
      "Loss at step 400: 98.25740814208984\n",
      "Loss at step 500: 98.97151184082031\n",
      "Loss at step 600: 97.79312896728516\n",
      "Loss at step 700: 99.60759735107422\n",
      "Loss at step 800: 98.9141845703125\n",
      "Loss at step 900: 100.16751098632812\n",
      "Loss at step 1000: 101.08515167236328\n",
      "Loss at step 1100: 98.71576690673828\n",
      "Loss at step 1200: 97.71183776855469\n",
      "Epoch 148/1000, Avg Train Loss: 99.4038713881113\n",
      "Loss at step 0: 97.89549255371094\n",
      "Loss at step 100: 105.5953598022461\n",
      "Loss at step 200: 99.4113998413086\n",
      "Loss at step 300: 101.6795425415039\n",
      "Loss at step 400: 99.32715606689453\n",
      "Loss at step 500: 99.59977722167969\n",
      "Loss at step 600: 99.6052474975586\n",
      "Loss at step 700: 98.12399291992188\n",
      "Loss at step 800: 99.83134460449219\n",
      "Loss at step 900: 98.94002532958984\n",
      "Loss at step 1000: 98.23078918457031\n",
      "Loss at step 1100: 98.23565673828125\n",
      "Loss at step 1200: 98.89627838134766\n",
      "Epoch 149/1000, Avg Train Loss: 99.40059981608468\n",
      "Loss at step 0: 98.58473205566406\n",
      "Loss at step 100: 99.28767395019531\n",
      "Loss at step 200: 98.10579681396484\n",
      "Loss at step 300: 100.80115509033203\n",
      "Loss at step 400: 98.58406829833984\n",
      "Loss at step 500: 97.89531707763672\n",
      "Loss at step 600: 102.29377746582031\n",
      "Loss at step 700: 100.2787857055664\n",
      "Loss at step 800: 98.71045684814453\n",
      "Loss at step 900: 98.81633758544922\n",
      "Loss at step 1000: 98.4577865600586\n",
      "Loss at step 1100: 98.26322174072266\n",
      "Loss at step 1200: 98.07001495361328\n",
      "Epoch 150/1000, Avg Train Loss: 99.41210558499333\n",
      "Loss at step 0: 97.75789642333984\n",
      "Loss at step 100: 98.53926849365234\n",
      "Loss at step 200: 100.76582336425781\n",
      "Loss at step 300: 99.64094543457031\n",
      "Loss at step 400: 100.1175308227539\n",
      "Loss at step 500: 99.20137786865234\n",
      "Loss at step 600: 99.35382080078125\n",
      "Loss at step 700: 98.32823944091797\n",
      "Loss at step 800: 98.4767837524414\n",
      "Loss at step 900: 97.57237243652344\n",
      "Loss at step 1000: 98.7789077758789\n",
      "Loss at step 1100: 99.07464599609375\n",
      "Loss at step 1200: 97.39533233642578\n",
      "Epoch 151/1000, Avg Train Loss: 99.42228412319541\n",
      "Loss at step 0: 98.07647705078125\n",
      "Loss at step 100: 97.05619049072266\n",
      "Loss at step 200: 99.47811126708984\n",
      "Loss at step 300: 99.20703125\n",
      "Loss at step 400: 100.00843811035156\n",
      "Loss at step 500: 98.52945709228516\n",
      "Loss at step 600: 112.87055206298828\n",
      "Loss at step 700: 99.04887390136719\n",
      "Loss at step 800: 99.26647186279297\n",
      "Loss at step 900: 97.24012756347656\n",
      "Loss at step 1000: 98.4921875\n",
      "Loss at step 1100: 99.85047912597656\n",
      "Loss at step 1200: 98.7643814086914\n",
      "Epoch 152/1000, Avg Train Loss: 99.41661449543481\n",
      "Loss at step 0: 98.53185272216797\n",
      "Loss at step 100: 98.793701171875\n",
      "Loss at step 200: 99.5838851928711\n",
      "Loss at step 300: 98.0404281616211\n",
      "Loss at step 400: 98.56253051757812\n",
      "Loss at step 500: 99.11051177978516\n",
      "Loss at step 600: 111.5565414428711\n",
      "Loss at step 700: 98.84661102294922\n",
      "Loss at step 800: 99.2354965209961\n",
      "Loss at step 900: 99.520751953125\n",
      "Loss at step 1000: 99.49039459228516\n",
      "Loss at step 1100: 98.13508605957031\n",
      "Loss at step 1200: 98.65511322021484\n",
      "Epoch 153/1000, Avg Train Loss: 99.38223966579993\n",
      "Loss at step 0: 98.1593017578125\n",
      "Loss at step 100: 99.01410675048828\n",
      "Loss at step 200: 97.82991027832031\n",
      "Loss at step 300: 99.41963958740234\n",
      "Loss at step 400: 105.61976623535156\n",
      "Loss at step 500: 100.60437774658203\n",
      "Loss at step 600: 104.25469970703125\n",
      "Loss at step 700: 99.08992004394531\n",
      "Loss at step 800: 112.98575592041016\n",
      "Loss at step 900: 99.26683807373047\n",
      "Loss at step 1000: 100.99063873291016\n",
      "Loss at step 1100: 99.04975891113281\n",
      "Loss at step 1200: 98.49331665039062\n",
      "Epoch 154/1000, Avg Train Loss: 99.40728948031429\n",
      "Loss at step 0: 98.56192779541016\n",
      "Loss at step 100: 98.42979431152344\n",
      "Loss at step 200: 98.79019927978516\n",
      "Loss at step 300: 99.42940521240234\n",
      "Loss at step 400: 99.67372131347656\n",
      "Loss at step 500: 102.43267822265625\n",
      "Loss at step 600: 98.56929779052734\n",
      "Loss at step 700: 99.01443481445312\n",
      "Loss at step 800: 100.08702087402344\n",
      "Loss at step 900: 98.47581481933594\n",
      "Loss at step 1000: 96.67942810058594\n",
      "Loss at step 1100: 99.24758911132812\n",
      "Loss at step 1200: 98.9379653930664\n",
      "Epoch 155/1000, Avg Train Loss: 99.39453857693472\n",
      "Loss at step 0: 102.6000747680664\n",
      "Loss at step 100: 99.17425537109375\n",
      "Loss at step 200: 98.5639419555664\n",
      "Loss at step 300: 100.75577545166016\n",
      "Loss at step 400: 106.29448699951172\n",
      "Loss at step 500: 99.2509994506836\n",
      "Loss at step 600: 97.70201110839844\n",
      "Loss at step 700: 98.27427673339844\n",
      "Loss at step 800: 97.5223388671875\n",
      "Loss at step 900: 98.453857421875\n",
      "Loss at step 1000: 98.709228515625\n",
      "Loss at step 1100: 97.8601303100586\n",
      "Loss at step 1200: 98.43162536621094\n",
      "Epoch 156/1000, Avg Train Loss: 99.4404766922244\n",
      "Loss at step 0: 99.7879409790039\n",
      "Loss at step 100: 98.93599700927734\n",
      "Loss at step 200: 98.59391784667969\n",
      "Loss at step 300: 98.97044372558594\n",
      "Loss at step 400: 99.36422729492188\n",
      "Loss at step 500: 99.19441223144531\n",
      "Loss at step 600: 97.74018859863281\n",
      "Loss at step 700: 98.91197967529297\n",
      "Loss at step 800: 97.40607452392578\n",
      "Loss at step 900: 98.17549896240234\n",
      "Loss at step 1000: 98.29139709472656\n",
      "Loss at step 1100: 99.58417510986328\n",
      "Loss at step 1200: 99.51058959960938\n",
      "Epoch 157/1000, Avg Train Loss: 99.41884167989095\n",
      "Loss at step 0: 97.9437026977539\n",
      "Loss at step 100: 97.57438659667969\n",
      "Loss at step 200: 99.60320281982422\n",
      "Loss at step 300: 99.0814208984375\n",
      "Loss at step 400: 98.92112731933594\n",
      "Loss at step 500: 99.66262817382812\n",
      "Loss at step 600: 99.01237487792969\n",
      "Loss at step 700: 99.7707748413086\n",
      "Loss at step 800: 99.36393737792969\n",
      "Loss at step 900: 98.14251708984375\n",
      "Loss at step 1000: 98.7756118774414\n",
      "Loss at step 1100: 97.5721435546875\n",
      "Loss at step 1200: 98.05580139160156\n",
      "Epoch 158/1000, Avg Train Loss: 99.4048619007987\n",
      "Loss at step 0: 98.73540496826172\n",
      "Loss at step 100: 98.94002532958984\n",
      "Loss at step 200: 97.03010559082031\n",
      "Loss at step 300: 97.27284240722656\n",
      "Loss at step 400: 98.46672821044922\n",
      "Loss at step 500: 97.78553771972656\n",
      "Loss at step 600: 98.49451446533203\n",
      "Loss at step 700: 98.37133026123047\n",
      "Loss at step 800: 97.90502166748047\n",
      "Loss at step 900: 98.44091796875\n",
      "Loss at step 1000: 102.57258605957031\n",
      "Loss at step 1100: 98.90702056884766\n",
      "Loss at step 1200: 98.62103271484375\n",
      "Epoch 159/1000, Avg Train Loss: 99.41074918851884\n",
      "Loss at step 0: 98.54600524902344\n",
      "Loss at step 100: 103.39817810058594\n",
      "Loss at step 200: 97.92012023925781\n",
      "Loss at step 300: 100.05577850341797\n",
      "Loss at step 400: 98.33219909667969\n",
      "Loss at step 500: 98.52777862548828\n",
      "Loss at step 600: 98.4499282836914\n",
      "Loss at step 700: 103.34371185302734\n",
      "Loss at step 800: 98.23068237304688\n",
      "Loss at step 900: 98.78157043457031\n",
      "Loss at step 1000: 99.51387023925781\n",
      "Loss at step 1100: 98.57054138183594\n",
      "Loss at step 1200: 97.99027252197266\n",
      "Epoch 160/1000, Avg Train Loss: 99.47764741101311\n",
      "Loss at step 0: 100.2480239868164\n",
      "Loss at step 100: 98.18206787109375\n",
      "Loss at step 200: 98.83903503417969\n",
      "Loss at step 300: 98.74744415283203\n",
      "Loss at step 400: 98.8989486694336\n",
      "Loss at step 500: 97.68751525878906\n",
      "Loss at step 600: 98.00972747802734\n",
      "Loss at step 700: 99.76805114746094\n",
      "Loss at step 800: 99.10516357421875\n",
      "Loss at step 900: 98.30209350585938\n",
      "Loss at step 1000: 97.7076416015625\n",
      "Loss at step 1100: 98.37506866455078\n",
      "Loss at step 1200: 100.95529174804688\n",
      "Epoch 161/1000, Avg Train Loss: 99.42635042381904\n",
      "Loss at step 0: 98.90138244628906\n",
      "Loss at step 100: 99.17349243164062\n",
      "Loss at step 200: 97.38967895507812\n",
      "Loss at step 300: 97.72201538085938\n",
      "Loss at step 400: 99.70326232910156\n",
      "Loss at step 500: 98.66555786132812\n",
      "Loss at step 600: 100.48101806640625\n",
      "Loss at step 700: 98.95040130615234\n",
      "Loss at step 800: 99.2167739868164\n",
      "Loss at step 900: 97.99919891357422\n",
      "Loss at step 1000: 98.20986938476562\n",
      "Loss at step 1100: 98.50399780273438\n",
      "Loss at step 1200: 98.885498046875\n",
      "Epoch 162/1000, Avg Train Loss: 99.39731746043974\n",
      "Loss at step 0: 99.32960510253906\n",
      "Loss at step 100: 99.9857177734375\n",
      "Loss at step 200: 101.84480285644531\n",
      "Loss at step 300: 98.19132995605469\n",
      "Loss at step 400: 99.77456665039062\n",
      "Loss at step 500: 100.67374420166016\n",
      "Loss at step 600: 98.23699951171875\n",
      "Loss at step 700: 99.27552795410156\n",
      "Loss at step 800: 99.33287811279297\n",
      "Loss at step 900: 99.50152587890625\n",
      "Loss at step 1000: 99.90740203857422\n",
      "Loss at step 1100: 102.14131164550781\n",
      "Loss at step 1200: 97.48944091796875\n",
      "Epoch 163/1000, Avg Train Loss: 99.4264091812677\n",
      "Loss at step 0: 99.91287231445312\n",
      "Loss at step 100: 100.07294464111328\n",
      "Loss at step 200: 98.9134521484375\n",
      "Loss at step 300: 99.03271484375\n",
      "Loss at step 400: 99.35596466064453\n",
      "Loss at step 500: 99.29219055175781\n",
      "Loss at step 600: 111.56494903564453\n",
      "Loss at step 700: 98.69789123535156\n",
      "Loss at step 800: 103.64015197753906\n",
      "Loss at step 900: 100.96348571777344\n",
      "Loss at step 1000: 98.33454895019531\n",
      "Loss at step 1100: 98.5965576171875\n",
      "Loss at step 1200: 97.96974182128906\n",
      "Epoch 164/1000, Avg Train Loss: 99.41541246457393\n",
      "Loss at step 0: 99.54541015625\n",
      "Loss at step 100: 97.53568267822266\n",
      "Loss at step 200: 99.63533020019531\n",
      "Loss at step 300: 99.8488540649414\n",
      "Loss at step 400: 98.4527359008789\n",
      "Loss at step 500: 97.22969055175781\n",
      "Loss at step 600: 99.99758911132812\n",
      "Loss at step 700: 97.93949890136719\n",
      "Loss at step 800: 100.2022705078125\n",
      "Loss at step 900: 97.9722900390625\n",
      "Loss at step 1000: 97.892333984375\n",
      "Loss at step 1100: 97.77252960205078\n",
      "Loss at step 1200: 97.51116943359375\n",
      "Epoch 165/1000, Avg Train Loss: 99.3849745938693\n",
      "Loss at step 0: 99.84027099609375\n",
      "Loss at step 100: 105.81671905517578\n",
      "Loss at step 200: 99.90763092041016\n",
      "Loss at step 300: 100.24066925048828\n",
      "Loss at step 400: 99.87440490722656\n",
      "Loss at step 500: 109.6008071899414\n",
      "Loss at step 600: 99.94144439697266\n",
      "Loss at step 700: 101.60751342773438\n",
      "Loss at step 800: 99.43317413330078\n",
      "Loss at step 900: 98.26189422607422\n",
      "Loss at step 1000: 97.98829650878906\n",
      "Loss at step 1100: 99.13774871826172\n",
      "Loss at step 1200: 97.98566436767578\n",
      "Epoch 166/1000, Avg Train Loss: 99.39742136464535\n",
      "Loss at step 0: 98.7016372680664\n",
      "Loss at step 100: 100.20883178710938\n",
      "Loss at step 200: 98.5498275756836\n",
      "Loss at step 300: 99.58733367919922\n",
      "Loss at step 400: 98.15754699707031\n",
      "Loss at step 500: 99.91638946533203\n",
      "Loss at step 600: 97.37443542480469\n",
      "Loss at step 700: 100.16229248046875\n",
      "Loss at step 800: 97.5099868774414\n",
      "Loss at step 900: 98.87244415283203\n",
      "Loss at step 1000: 98.73682403564453\n",
      "Loss at step 1100: 98.83893585205078\n",
      "Loss at step 1200: 98.2117919921875\n",
      "Epoch 167/1000, Avg Train Loss: 99.40815408947398\n",
      "Loss at step 0: 98.83454132080078\n",
      "Loss at step 100: 99.24911499023438\n",
      "Loss at step 200: 99.49111938476562\n",
      "Loss at step 300: 100.84188079833984\n",
      "Loss at step 400: 98.64860534667969\n",
      "Loss at step 500: 98.24060821533203\n",
      "Loss at step 600: 97.96070861816406\n",
      "Loss at step 700: 97.90580749511719\n",
      "Loss at step 800: 101.94871520996094\n",
      "Loss at step 900: 101.54694366455078\n",
      "Loss at step 1000: 99.52110290527344\n",
      "Loss at step 1100: 99.63053131103516\n",
      "Loss at step 1200: 98.25238037109375\n",
      "Epoch 168/1000, Avg Train Loss: 99.41467130222753\n",
      "Loss at step 0: 98.2085952758789\n",
      "Loss at step 100: 98.59312438964844\n",
      "Loss at step 200: 98.71133422851562\n",
      "Loss at step 300: 98.50666809082031\n",
      "Loss at step 400: 98.50143432617188\n",
      "Loss at step 500: 98.62509155273438\n",
      "Loss at step 600: 98.0890884399414\n",
      "Loss at step 700: 98.1473159790039\n",
      "Loss at step 800: 98.97481536865234\n",
      "Loss at step 900: 98.23099517822266\n",
      "Loss at step 1000: 97.97380828857422\n",
      "Loss at step 1100: 100.56597137451172\n",
      "Loss at step 1200: 99.09114074707031\n",
      "Epoch 169/1000, Avg Train Loss: 99.42065545733307\n",
      "Loss at step 0: 99.2555160522461\n",
      "Loss at step 100: 97.87007904052734\n",
      "Loss at step 200: 98.55276489257812\n",
      "Loss at step 300: 99.13406372070312\n",
      "Loss at step 400: 99.73099517822266\n",
      "Loss at step 500: 117.56230926513672\n",
      "Loss at step 600: 99.48870086669922\n",
      "Loss at step 700: 97.9056625366211\n",
      "Loss at step 800: 98.44775390625\n",
      "Loss at step 900: 99.33196258544922\n",
      "Loss at step 1000: 98.31098937988281\n",
      "Loss at step 1100: 97.19536590576172\n",
      "Loss at step 1200: 99.35234832763672\n",
      "Epoch 170/1000, Avg Train Loss: 99.37706319568227\n",
      "Loss at step 0: 98.43057250976562\n",
      "Loss at step 100: 98.83451080322266\n",
      "Loss at step 200: 100.13845825195312\n",
      "Loss at step 300: 99.19737243652344\n",
      "Loss at step 400: 98.51461791992188\n",
      "Loss at step 500: 98.7954330444336\n",
      "Loss at step 600: 97.70246124267578\n",
      "Loss at step 700: 98.024658203125\n",
      "Loss at step 800: 98.64008331298828\n",
      "Loss at step 900: 98.0656509399414\n",
      "Loss at step 1000: 97.97577667236328\n",
      "Loss at step 1100: 98.75454711914062\n",
      "Loss at step 1200: 98.9383544921875\n",
      "Epoch 171/1000, Avg Train Loss: 99.43784238451121\n",
      "Loss at step 0: 97.538330078125\n",
      "Loss at step 100: 98.35317993164062\n",
      "Loss at step 200: 99.46128845214844\n",
      "Loss at step 300: 98.10763549804688\n",
      "Loss at step 400: 97.96238708496094\n",
      "Loss at step 500: 98.4434814453125\n",
      "Loss at step 600: 100.21150970458984\n",
      "Loss at step 700: 99.20907592773438\n",
      "Loss at step 800: 99.35719299316406\n",
      "Loss at step 900: 101.87921142578125\n",
      "Loss at step 1000: 97.76289367675781\n",
      "Loss at step 1100: 99.59004211425781\n",
      "Loss at step 1200: 97.44700622558594\n",
      "Epoch 172/1000, Avg Train Loss: 99.40749315922314\n",
      "Loss at step 0: 97.8332290649414\n",
      "Loss at step 100: 98.6728515625\n",
      "Loss at step 200: 97.40272521972656\n",
      "Loss at step 300: 97.69282531738281\n",
      "Loss at step 400: 99.42369079589844\n",
      "Loss at step 500: 98.57682037353516\n",
      "Loss at step 600: 98.48167419433594\n",
      "Loss at step 700: 102.05471801757812\n",
      "Loss at step 800: 98.61402893066406\n",
      "Loss at step 900: 101.04737091064453\n",
      "Loss at step 1000: 98.42021179199219\n",
      "Loss at step 1100: 99.07765197753906\n",
      "Loss at step 1200: 97.47775268554688\n",
      "Epoch 173/1000, Avg Train Loss: 99.42582755104237\n",
      "Loss at step 0: 104.59201049804688\n",
      "Loss at step 100: 99.41732025146484\n",
      "Loss at step 200: 97.39806365966797\n",
      "Loss at step 300: 98.72183227539062\n",
      "Loss at step 400: 98.59123229980469\n",
      "Loss at step 500: 97.58037567138672\n",
      "Loss at step 600: 97.80160522460938\n",
      "Loss at step 700: 98.22307586669922\n",
      "Loss at step 800: 99.62300109863281\n",
      "Loss at step 900: 98.45985412597656\n",
      "Loss at step 1000: 98.87686157226562\n",
      "Loss at step 1100: 98.23292541503906\n",
      "Loss at step 1200: 98.19522094726562\n",
      "Epoch 174/1000, Avg Train Loss: 99.40974889835493\n",
      "Loss at step 0: 98.76470947265625\n",
      "Loss at step 100: 98.286376953125\n",
      "Loss at step 200: 99.40225219726562\n",
      "Loss at step 300: 97.66590881347656\n",
      "Loss at step 400: 99.04483795166016\n",
      "Loss at step 500: 99.36761474609375\n",
      "Loss at step 600: 98.20419311523438\n",
      "Loss at step 700: 98.37435150146484\n",
      "Loss at step 800: 98.40672302246094\n",
      "Loss at step 900: 99.09814453125\n",
      "Loss at step 1000: 98.45655822753906\n",
      "Loss at step 1100: 99.87602996826172\n",
      "Loss at step 1200: 101.22671508789062\n",
      "Epoch 175/1000, Avg Train Loss: 99.38545421489233\n",
      "Loss at step 0: 97.36304473876953\n",
      "Loss at step 100: 99.509033203125\n",
      "Loss at step 200: 100.50021362304688\n",
      "Loss at step 300: 97.59732818603516\n",
      "Loss at step 400: 98.58779907226562\n",
      "Loss at step 500: 98.88372039794922\n",
      "Loss at step 600: 99.2834243774414\n",
      "Loss at step 700: 98.84320831298828\n",
      "Loss at step 800: 98.65975189208984\n",
      "Loss at step 900: 98.63189697265625\n",
      "Loss at step 1000: 98.3040771484375\n",
      "Loss at step 1100: 98.0638656616211\n",
      "Loss at step 1200: 99.83452606201172\n",
      "Epoch 176/1000, Avg Train Loss: 99.38032023729244\n",
      "Loss at step 0: 99.5822525024414\n",
      "Loss at step 100: 98.055419921875\n",
      "Loss at step 200: 98.10359191894531\n",
      "Loss at step 300: 99.75773620605469\n",
      "Loss at step 400: 99.12588500976562\n",
      "Loss at step 500: 102.00755310058594\n",
      "Loss at step 600: 98.49047088623047\n",
      "Loss at step 700: 98.32030487060547\n",
      "Loss at step 800: 103.22986602783203\n",
      "Loss at step 900: 97.92190551757812\n",
      "Loss at step 1000: 99.48046112060547\n",
      "Loss at step 1100: 98.44834899902344\n",
      "Loss at step 1200: 98.93132781982422\n",
      "Epoch 177/1000, Avg Train Loss: 99.41597659533849\n",
      "Loss at step 0: 97.87255859375\n",
      "Loss at step 100: 98.2392807006836\n",
      "Loss at step 200: 98.17488098144531\n",
      "Loss at step 300: 100.86089324951172\n",
      "Loss at step 400: 104.65852355957031\n",
      "Loss at step 500: 102.50814819335938\n",
      "Loss at step 600: 102.16214752197266\n",
      "Loss at step 700: 98.89556121826172\n",
      "Loss at step 800: 98.57606506347656\n",
      "Loss at step 900: 99.19961547851562\n",
      "Loss at step 1000: 99.8663101196289\n",
      "Loss at step 1100: 98.58268737792969\n",
      "Loss at step 1200: 97.70613098144531\n",
      "Epoch 178/1000, Avg Train Loss: 99.39060494506244\n",
      "Loss at step 0: 96.86683654785156\n",
      "Loss at step 100: 99.92300415039062\n",
      "Loss at step 200: 100.18012237548828\n",
      "Loss at step 300: 97.43646240234375\n",
      "Loss at step 400: 99.28071594238281\n",
      "Loss at step 500: 97.61648559570312\n",
      "Loss at step 600: 98.25572204589844\n",
      "Loss at step 700: 100.98355865478516\n",
      "Loss at step 800: 98.52898406982422\n",
      "Loss at step 900: 100.15269470214844\n",
      "Loss at step 1000: 98.03593444824219\n",
      "Loss at step 1100: 97.59925079345703\n",
      "Loss at step 1200: 99.91426086425781\n",
      "Epoch 179/1000, Avg Train Loss: 99.38752200919834\n",
      "Loss at step 0: 99.20538330078125\n",
      "Loss at step 100: 98.64803314208984\n",
      "Loss at step 200: 112.74003601074219\n",
      "Loss at step 300: 100.8503646850586\n",
      "Loss at step 400: 97.41278076171875\n",
      "Loss at step 500: 97.91613006591797\n",
      "Loss at step 600: 97.93067169189453\n",
      "Loss at step 700: 99.32917022705078\n",
      "Loss at step 800: 99.92896270751953\n",
      "Loss at step 900: 97.64291381835938\n",
      "Loss at step 1000: 100.78028106689453\n",
      "Loss at step 1100: 98.13041687011719\n",
      "Loss at step 1200: 100.0456314086914\n",
      "Epoch 180/1000, Avg Train Loss: 99.43003417450248\n",
      "Loss at step 0: 104.4594497680664\n",
      "Loss at step 100: 98.55274200439453\n",
      "Loss at step 200: 100.30581665039062\n",
      "Loss at step 300: 99.25066375732422\n",
      "Loss at step 400: 101.97652435302734\n",
      "Loss at step 500: 100.88729858398438\n",
      "Loss at step 600: 98.28492736816406\n",
      "Loss at step 700: 101.55538177490234\n",
      "Loss at step 800: 99.48118591308594\n",
      "Loss at step 900: 99.95164489746094\n",
      "Loss at step 1000: 98.48301696777344\n",
      "Loss at step 1100: 101.43556213378906\n",
      "Loss at step 1200: 99.75399780273438\n",
      "Epoch 181/1000, Avg Train Loss: 99.39724351210116\n",
      "Loss at step 0: 105.35002899169922\n",
      "Loss at step 100: 98.26042938232422\n",
      "Loss at step 200: 98.40707397460938\n",
      "Loss at step 300: 98.66162872314453\n",
      "Loss at step 400: 97.67375946044922\n",
      "Loss at step 500: 102.29545593261719\n",
      "Loss at step 600: 98.39492797851562\n",
      "Loss at step 700: 98.40869903564453\n",
      "Loss at step 800: 98.55802917480469\n",
      "Loss at step 900: 98.58478546142578\n",
      "Loss at step 1000: 99.56352996826172\n",
      "Loss at step 1100: 98.79015350341797\n",
      "Loss at step 1200: 98.3301773071289\n",
      "Epoch 182/1000, Avg Train Loss: 99.36756412419686\n",
      "Loss at step 0: 98.60630798339844\n",
      "Loss at step 100: 97.8673324584961\n",
      "Loss at step 200: 102.89846801757812\n",
      "Loss at step 300: 101.25340270996094\n",
      "Loss at step 400: 99.24710083007812\n",
      "Loss at step 500: 98.944580078125\n",
      "Loss at step 600: 98.10747528076172\n",
      "Loss at step 700: 98.96729278564453\n",
      "Loss at step 800: 98.112548828125\n",
      "Loss at step 900: 99.74178314208984\n",
      "Loss at step 1000: 98.5455093383789\n",
      "Loss at step 1100: 102.55113983154297\n",
      "Loss at step 1200: 100.08589172363281\n",
      "Epoch 183/1000, Avg Train Loss: 99.3740040677265\n",
      "Loss at step 0: 99.75511169433594\n",
      "Loss at step 100: 98.50286865234375\n",
      "Loss at step 200: 99.07913970947266\n",
      "Loss at step 300: 110.01313781738281\n",
      "Loss at step 400: 98.62785339355469\n",
      "Loss at step 500: 100.67109680175781\n",
      "Loss at step 600: 98.71460723876953\n",
      "Loss at step 700: 99.6637191772461\n",
      "Loss at step 800: 99.42802429199219\n",
      "Loss at step 900: 98.3280029296875\n",
      "Loss at step 1000: 98.89879608154297\n",
      "Loss at step 1100: 100.06478118896484\n",
      "Loss at step 1200: 98.67007446289062\n",
      "Epoch 184/1000, Avg Train Loss: 99.42502755717553\n",
      "Loss at step 0: 98.947998046875\n",
      "Loss at step 100: 100.54476165771484\n",
      "Loss at step 200: 97.63190460205078\n",
      "Loss at step 300: 99.97579193115234\n",
      "Loss at step 400: 98.89930725097656\n",
      "Loss at step 500: 99.60424041748047\n",
      "Loss at step 600: 98.67257690429688\n",
      "Loss at step 700: 101.65380096435547\n",
      "Loss at step 800: 98.33953094482422\n",
      "Loss at step 900: 97.69070434570312\n",
      "Loss at step 1000: 98.970947265625\n",
      "Loss at step 1100: 98.64325714111328\n",
      "Loss at step 1200: 99.18341064453125\n",
      "Epoch 185/1000, Avg Train Loss: 99.38846229503841\n",
      "Loss at step 0: 108.01190185546875\n",
      "Loss at step 100: 99.48031616210938\n",
      "Loss at step 200: 99.2923583984375\n",
      "Loss at step 300: 97.83235931396484\n",
      "Loss at step 400: 98.2127685546875\n",
      "Loss at step 500: 117.26907348632812\n",
      "Loss at step 600: 97.70323181152344\n",
      "Loss at step 700: 99.64309692382812\n",
      "Loss at step 800: 98.01326751708984\n",
      "Loss at step 900: 99.60990142822266\n",
      "Loss at step 1000: 98.7422103881836\n",
      "Loss at step 1100: 98.68217468261719\n",
      "Loss at step 1200: 99.36659240722656\n",
      "Epoch 186/1000, Avg Train Loss: 99.39375945896778\n",
      "Loss at step 0: 98.30899047851562\n",
      "Loss at step 100: 97.80963897705078\n",
      "Loss at step 200: 99.75507354736328\n",
      "Loss at step 300: 99.21233367919922\n",
      "Loss at step 400: 98.59101104736328\n",
      "Loss at step 500: 99.0617904663086\n",
      "Loss at step 600: 99.28382873535156\n",
      "Loss at step 700: 100.05235290527344\n",
      "Loss at step 800: 98.93618774414062\n",
      "Loss at step 900: 117.93870544433594\n",
      "Loss at step 1000: 99.03999328613281\n",
      "Loss at step 1100: 100.02641296386719\n",
      "Loss at step 1200: 97.9298324584961\n",
      "Epoch 187/1000, Avg Train Loss: 99.43454545524128\n",
      "Loss at step 0: 98.93814086914062\n",
      "Loss at step 100: 98.57718658447266\n",
      "Loss at step 200: 99.58061981201172\n",
      "Loss at step 300: 105.76822662353516\n",
      "Loss at step 400: 99.36578369140625\n",
      "Loss at step 500: 101.62731170654297\n",
      "Loss at step 600: 98.51691436767578\n",
      "Loss at step 700: 98.22872161865234\n",
      "Loss at step 800: 99.05669403076172\n",
      "Loss at step 900: 98.36015319824219\n",
      "Loss at step 1000: 99.17842864990234\n",
      "Loss at step 1100: 98.51673889160156\n",
      "Loss at step 1200: 101.2737045288086\n",
      "Epoch 188/1000, Avg Train Loss: 99.37103430121462\n",
      "Loss at step 0: 109.45317840576172\n",
      "Loss at step 100: 98.59632110595703\n",
      "Loss at step 200: 101.22479248046875\n",
      "Loss at step 300: 97.61847686767578\n",
      "Loss at step 400: 97.47577667236328\n",
      "Loss at step 500: 101.82999420166016\n",
      "Loss at step 600: 98.69549560546875\n",
      "Loss at step 700: 97.28511810302734\n",
      "Loss at step 800: 97.94435119628906\n",
      "Loss at step 900: 98.02157592773438\n",
      "Loss at step 1000: 99.62083435058594\n",
      "Loss at step 1100: 98.36412811279297\n",
      "Loss at step 1200: 98.08081817626953\n",
      "Epoch 189/1000, Avg Train Loss: 99.39701159949442\n",
      "Loss at step 0: 97.69268035888672\n",
      "Loss at step 100: 99.56813049316406\n",
      "Loss at step 200: 99.68610382080078\n",
      "Loss at step 300: 98.09464263916016\n",
      "Loss at step 400: 98.42799377441406\n",
      "Loss at step 500: 98.56403350830078\n",
      "Loss at step 600: 98.77666473388672\n",
      "Loss at step 700: 98.27525329589844\n",
      "Loss at step 800: 102.7525405883789\n",
      "Loss at step 900: 98.33800506591797\n",
      "Loss at step 1000: 97.68367767333984\n",
      "Loss at step 1100: 99.20427703857422\n",
      "Loss at step 1200: 98.81771087646484\n",
      "Epoch 190/1000, Avg Train Loss: 99.37633018123293\n",
      "Loss at step 0: 98.67366790771484\n",
      "Loss at step 100: 99.74003601074219\n",
      "Loss at step 200: 98.14239501953125\n",
      "Loss at step 300: 98.13584899902344\n",
      "Loss at step 400: 98.85108184814453\n",
      "Loss at step 500: 100.72820281982422\n",
      "Loss at step 600: 99.52708435058594\n",
      "Loss at step 700: 103.3969497680664\n",
      "Loss at step 800: 98.2587890625\n",
      "Loss at step 900: 101.16722869873047\n",
      "Loss at step 1000: 104.2138442993164\n",
      "Loss at step 1100: 98.70054626464844\n",
      "Loss at step 1200: 98.615478515625\n",
      "Epoch 191/1000, Avg Train Loss: 99.36390964421639\n",
      "Loss at step 0: 98.08338928222656\n",
      "Loss at step 100: 98.5472640991211\n",
      "Loss at step 200: 98.9415054321289\n",
      "Loss at step 300: 100.79398345947266\n",
      "Loss at step 400: 99.56409454345703\n",
      "Loss at step 500: 98.49333190917969\n",
      "Loss at step 600: 102.90226745605469\n",
      "Loss at step 700: 99.07573699951172\n",
      "Loss at step 800: 101.64322662353516\n",
      "Loss at step 900: 98.63371276855469\n",
      "Loss at step 1000: 100.7457275390625\n",
      "Loss at step 1100: 99.85152435302734\n",
      "Loss at step 1200: 99.02005004882812\n",
      "Epoch 192/1000, Avg Train Loss: 99.39970791764244\n",
      "Loss at step 0: 98.88617706298828\n",
      "Loss at step 100: 98.35292053222656\n",
      "Loss at step 200: 98.80541229248047\n",
      "Loss at step 300: 99.9566650390625\n",
      "Loss at step 400: 97.3255844116211\n",
      "Loss at step 500: 98.54531860351562\n",
      "Loss at step 600: 99.32609558105469\n",
      "Loss at step 700: 98.59420776367188\n",
      "Loss at step 800: 98.39035034179688\n",
      "Loss at step 900: 98.91664123535156\n",
      "Loss at step 1000: 97.66368103027344\n",
      "Loss at step 1100: 98.24969482421875\n",
      "Loss at step 1200: 99.39229583740234\n",
      "Epoch 193/1000, Avg Train Loss: 99.38039336466866\n",
      "Loss at step 0: 99.50162506103516\n",
      "Loss at step 100: 99.08061981201172\n",
      "Loss at step 200: 98.0235824584961\n",
      "Loss at step 300: 99.17447662353516\n",
      "Loss at step 400: 101.87971496582031\n",
      "Loss at step 500: 100.10301208496094\n",
      "Loss at step 600: 109.36449432373047\n",
      "Loss at step 700: 108.25834655761719\n",
      "Loss at step 800: 99.17157745361328\n",
      "Loss at step 900: 98.70711517333984\n",
      "Loss at step 1000: 98.08653259277344\n",
      "Loss at step 1100: 97.84963989257812\n",
      "Loss at step 1200: 97.9505386352539\n",
      "Epoch 194/1000, Avg Train Loss: 99.40275556447051\n",
      "Loss at step 0: 99.404541015625\n",
      "Loss at step 100: 99.63825225830078\n",
      "Loss at step 200: 99.9426040649414\n",
      "Loss at step 300: 100.33399200439453\n",
      "Loss at step 400: 107.20185852050781\n",
      "Loss at step 500: 97.3072738647461\n",
      "Loss at step 600: 98.74827575683594\n",
      "Loss at step 700: 98.28805541992188\n",
      "Loss at step 800: 97.73016357421875\n",
      "Loss at step 900: 98.97699737548828\n",
      "Loss at step 1000: 98.1830825805664\n",
      "Loss at step 1100: 97.97632598876953\n",
      "Loss at step 1200: 99.13103485107422\n",
      "Epoch 195/1000, Avg Train Loss: 99.37524345546093\n",
      "Loss at step 0: 98.57898712158203\n",
      "Loss at step 100: 100.2027359008789\n",
      "Loss at step 200: 99.3623275756836\n",
      "Loss at step 300: 103.77163696289062\n",
      "Loss at step 400: 97.68933868408203\n",
      "Loss at step 500: 98.23018646240234\n",
      "Loss at step 600: 100.84941101074219\n",
      "Loss at step 700: 98.8995132446289\n",
      "Loss at step 800: 99.26840209960938\n",
      "Loss at step 900: 98.20813751220703\n",
      "Loss at step 1000: 103.63186645507812\n",
      "Loss at step 1100: 99.3031005859375\n",
      "Loss at step 1200: 98.6355972290039\n",
      "Epoch 196/1000, Avg Train Loss: 99.43279745355008\n",
      "Loss at step 0: 101.54798126220703\n",
      "Loss at step 100: 98.13321685791016\n",
      "Loss at step 200: 100.46051788330078\n",
      "Loss at step 300: 98.91963958740234\n",
      "Loss at step 400: 98.20592498779297\n",
      "Loss at step 500: 99.29781341552734\n",
      "Loss at step 600: 98.91590118408203\n",
      "Loss at step 700: 98.92304992675781\n",
      "Loss at step 800: 98.52865600585938\n",
      "Loss at step 900: 99.41542053222656\n",
      "Loss at step 1000: 98.25157165527344\n",
      "Loss at step 1100: 98.64812469482422\n",
      "Loss at step 1200: 98.47966766357422\n",
      "Epoch 197/1000, Avg Train Loss: 99.39355460725555\n",
      "Loss at step 0: 98.3351821899414\n",
      "Loss at step 100: 99.4998550415039\n",
      "Loss at step 200: 97.5904541015625\n",
      "Loss at step 300: 99.15955352783203\n",
      "Loss at step 400: 98.7597885131836\n",
      "Loss at step 500: 99.11290740966797\n",
      "Loss at step 600: 98.54808807373047\n",
      "Loss at step 700: 98.2662353515625\n",
      "Loss at step 800: 98.7719497680664\n",
      "Loss at step 900: 101.83899688720703\n",
      "Loss at step 1000: 99.39369201660156\n",
      "Loss at step 1100: 106.21993255615234\n",
      "Loss at step 1200: 97.22572326660156\n",
      "Epoch 198/1000, Avg Train Loss: 99.36245783092906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 42, in <module>\n",
      "    glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/aoden/PycharmProjects/PyTorch Art Project/scratch.py\", line 32, in load_glove_embeddings\n",
      "    vector = np.asarray(values[1:], dtype='float32')\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m average \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, criterion, optimizer)\n\u001b[1;32m      8\u001b[0m     loss_plot\u001b[38;5;241m.\u001b[39mextend(batch_losses)  \u001b[38;5;66;03m# Store losses for each batch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Print average loss for the epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m step_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# loops over all mini-batches in the dataloader \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, price \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     price_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(price_pred, price)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Training Loop \n",
    "num_epochs = 1000\n",
    "loss_plot = []  # keep a vector to plot the loss as we go \n",
    "average = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    loss_plot.extend(batch_losses)  # Store losses for each batch\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    average.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Avg Train Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    # Update the learning rate \n",
    "    scheduler.step()\n",
    "\n",
    "    # if the average epoch loss goes below human error significantly, stop training \n",
    "    if avg_epoch_loss < 30: \n",
    "        break\n",
    "\n",
    "    if (epoch % 100) == 0: \n",
    "        torch.save(model.state_dict(), 'model_weights.txt')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(loss_plot, label=\"Batch Loss\")\n",
    "plt.plot(average, label=\"Average Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b4213067-3809-46fa-91c3-442678fdcbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average, label=\"Average Batch Loss\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31cbe535-ac17-4ad2-b76f-6e0b7fec80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: Circle of A. de Simone\n",
      "Title: The Motor Yacht Dolphin , Royal Thames Yacht Club, in the Bay of Naples\n",
      "Real Price: $549.16\n",
      "Model Prediction: $2830.93\n",
      "5428083.0\n"
     ]
    }
   ],
   "source": [
    "def prediction_to_real_price(price_tensor): \n",
    "    return (price_tensor * dataset.price_std) + dataset.price_median\n",
    "\n",
    "# Sample the Training Set \n",
    "index = 16\n",
    "x_test, price_test = dataset.__getitem__(index)\n",
    "artist_str, title_str = dataset.__getstring__(index)\n",
    "print(f\"Artist: {artist_str}\")\n",
    "print(f\"Title: {title_str}\")\n",
    "print(f\"Real Price: ${prediction_to_real_price(price_test).item():.2f}\")\n",
    "prediction = model(x_test.view(1, -1))\n",
    "print(f\"Model Prediction: ${prediction_to_real_price(prediction).item():.2f}\")\n",
    "with torch.no_grad(): \n",
    "    print(criterion(price_test, prediction).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cc8aeab-5ef6-4f0f-9fc8-67710cd44e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: A pink lustre 'Freemason's' jug\n",
      "Title: nan\n",
      "Real Price: $219.52\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'artist_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal Price: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_to_real_price(price_test)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model(artist_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), title_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), numerics_test\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Prediction: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_to_real_price(prediction)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'artist_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Artist: {artist_str}\")\n",
    "print(f\"Title: {title_str}\")\n",
    "print(f\"Real Price: ${prediction_to_real_price(price_test).item():.2f}\")\n",
    "prediction = model(artist_test.view(1, -1), title_test.view(1, -1), numerics_test.view(1, -1))\n",
    "print(f\"Model Prediction: ${prediction_to_real_price(prediction).item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "35729b1f-12eb-4630-8629-bed3cb9e1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54.4121)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    print(criterion(price_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8ab652e-d671-456d-9a3e-364fb098e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Price Normalized:  tensor([-0.0092])\n",
      "Model Prediction Normalized:  tensor([[-2.1695e-08]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Price Normalized: \", price_test)\n",
    "print(\"Model Prediction Normalized: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5423a23-3678-4b67-9e99-aee0cb76d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight:\n",
      "tensor([[-0.2612, -0.3165, -0.0925, -0.1503, -0.1302,  0.0803,  0.1983, -0.0735,\n",
      "          0.0091,  0.3834, -0.1505,  0.0686,  0.0193],\n",
      "        [ 0.2101,  0.3574,  0.4466, -0.3402,  0.1273,  0.0058, -0.3784,  0.0053,\n",
      "         -0.5275,  0.3315, -0.1071, -0.1518, -0.0346],\n",
      "        [-0.0795, -0.2030,  0.3063, -0.0636, -0.1748,  0.0216,  0.0373, -0.1911,\n",
      "         -0.2639,  0.0804,  0.1417, -0.0013, -0.0812],\n",
      "        [ 0.1090,  0.1107, -0.2536,  0.3599, -0.1825,  0.2160,  0.1724, -0.4350,\n",
      "          0.2164, -0.4358,  0.1931,  0.3384,  0.0540],\n",
      "        [-0.3601, -0.1574, -0.1143, -0.0907, -0.2680,  0.1386,  0.2232, -0.1045,\n",
      "          0.1425, -0.0342,  0.0586, -0.0394, -0.0060],\n",
      "        [-0.1171, -0.5389,  0.1044,  0.1699, -0.1923, -0.3639, -0.0117, -0.0824,\n",
      "         -0.1775, -0.2600, -0.2471,  0.2647, -0.0533],\n",
      "        [ 0.1294,  0.0744, -0.0263,  0.4745, -0.2291,  0.1951,  0.3845,  0.1570,\n",
      "          0.2338, -0.2118,  0.5391,  0.0463,  0.4172],\n",
      "        [-0.2072, -0.0140, -0.0899,  0.0605, -0.2144, -0.1092,  0.0264, -0.0385,\n",
      "         -0.0209, -0.0822,  0.0858, -0.2819, -0.0158]])\n",
      "\n",
      "fc1.bias:\n",
      "tensor([-0.1312,  0.3242, -0.0517, -0.1531,  0.1452, -0.1008,  0.2891, -0.0679])\n",
      "\n",
      "fc2.weight:\n",
      "tensor([[ 0.1471, -0.3638, -0.0070, -0.0702, -0.3050, -0.1307, -0.2299, -0.0376],\n",
      "        [-0.0179, -0.2813, -0.0876,  0.0059, -0.2308,  0.0426, -0.0613, -0.3399],\n",
      "        [-0.5157, -0.1543,  0.0142, -0.4407,  0.4583,  0.2321, -0.2439, -0.0939],\n",
      "        [ 0.4911, -0.4961,  0.2836, -0.0364, -0.2880, -0.0405, -0.0236, -0.1612]])\n",
      "\n",
      "fc2.bias:\n",
      "tensor([ 0.2232, -0.0777, -0.2261, -0.2100])\n",
      "\n",
      "fc3.weight:\n",
      "tensor([[-0.2992, -0.0023, -0.0525,  0.0641]])\n",
      "\n",
      "fc3.bias:\n",
      "tensor([-2.1695e-08])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all weights and biases\n",
    "for name, param in model.state_dict().items():\n",
    "    print(f\"{name}:\\n{param}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dae16-82c1-4b11-92eb-23a6dbdaa020",
   "metadata": {},
   "source": [
    "Compute human error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2039366-5817-4d25-aafd-fb8294cc6dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (MAPE): 39.08%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'clean_art.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the human predicted price (midpoint)\n",
    "data['Human Predicted Price'] = (data['Real LB Estimate USD'] + data['Real UB Estimate USD']) / 2\n",
    "\n",
    "# Calculate the absolute percentage error for each row\n",
    "data['Absolute Percentage Error'] = abs(data['Human Predicted Price'] - data['Real Price USD']) / data['Real Price USD']\n",
    "\n",
    "# Calculate the mean of the absolute percentage error (MAPE)\n",
    "mape = data['Absolute Percentage Error'].mean() * 100\n",
    "\n",
    "# Output the result\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93666450-0d6e-4efa-b20a-9172a66a829d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
